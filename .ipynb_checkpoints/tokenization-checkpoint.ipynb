{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d955a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv, os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff0e67",
   "metadata": {},
   "source": [
    "## Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "021b720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file to use\n",
    "data_dir    = \"datasets\"\n",
    "filename    = \"raw_arxiv_10.csv\"\n",
    "\n",
    "# read data as panda dataframe\n",
    "raw_data = pd.read_csv(data_dir + os.sep + filename)\n",
    "\n",
    "# remove newlines\n",
    "remove_lst = ['\\r\\n']\n",
    "remove = lambda x: ' '.join([item for item in x.split() if item not in remove_lst])\n",
    "raw_data['summary'] = raw_data['summary'].apply(remove)\n",
    "\n",
    "# 1 abstract example\n",
    "ex_abstract = raw_data['summary'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed1665",
   "metadata": {},
   "source": [
    "## SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a319ab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Electron', 'temperature', 'anisotropies', 'and', 'electron', 'beams', 'are', 'nonthermal', 'features', 'of', 'the', 'observed', 'nonequilibrium', 'electron', 'velocity', 'distributions', 'in', 'the', 'solar', 'wind.', 'In', 'collision', '-', 'poor', 'plasmas', 'these', 'nonequilibrium', 'distributions', 'are', 'expected', 'to', 'be', 'regulated', 'by', 'kinetic', 'instabilities', 'through', 'wave', '-', 'particle', 'interactions.', 'This', 'study', 'considers', 'electron', 'instabilities', 'driven', 'by', 'the', 'interplay', 'of', 'core', 'electron', 'temperature', 'anisotropies', 'and', 'the', 'electron', 'beam,', 'and', 'firstly', 'gives', 'a', 'comprehensive', 'analysis', 'of', 'instabilities', 'in', 'arbitrary', 'directions', 'to', 'the', 'background', 'magnetic', 'field.', 'It', 'clarifies', 'the', 'dominant', 'parameter', 'regime', '(', 'e.g.,', 'parallel', 'core', 'electron', 'plasma', 'beta', '$', '\\\\beta', '_', '{', '\\\\', 'mathrm', '{', 'ec', '\\\\', 'parallel', '}', '}', '$,', 'core', 'electron', 'temperature', 'anisotropy', '$', 'A', '_', '{', '\\\\', 'mathrm', '{', 'ec', '}', '}', '\\\\', 'equiv', 'T', '_', '{', '\\\\', 'mathrm', '{', 'ec', '\\\\', 'perp', '}', '}', '/', 'T', '_', '{', '\\\\', 'mathrm', '{', 'ec', '\\\\', 'parallel', '}', '}', '$,', 'and', 'electron', 'beam', 'velocity', '$', 'V', '_', '{', '\\\\', 'mathrm', '{', 'eb', '}', '}', '$', ')', 'for', 'each', 'kind', 'of', 'electron', 'instability', '(', 'e.g.,', 'the', 'electron', 'beam', '-', 'driven', 'electron', 'acoustic', '/', 'magnetoacoustic', 'instability,', 'the', 'electron', 'beam', '-', 'driven', 'whistler', 'instability,', 'the', 'electromagnetic', 'electron', 'cyclotron', 'instability,', 'the', 'electron', 'mirror', 'instability,', 'the', 'electron', 'firehose', 'instability,', 'and', 'the', 'ordinary', '-', 'mode', 'instability).', 'It', 'finds', 'that', 'the', 'electron', 'beam', 'can', 'destabilize', 'electron', 'acoustic', '/', 'magnetoacoustic', 'waves', 'in', 'the', 'low', '-', '$', '\\\\', 'beta', '_', '{', '\\\\', 'mathrm', '{', 'ec', '\\\\', 'parallel', '}', '}', '$', 'regime,', 'and', 'whistler', 'waves', 'in', 'the', 'medium', '-', 'and', 'large', '-', '$', '\\\\', 'beta', '_', '{', '\\\\', 'mathrm', '{', 'ec', '\\\\', 'parallel', '}', '}', '$', 'regime.', 'It', 'also', 'finds', 'that', 'a', 'new', 'oblique', 'fast', '-', 'magnetosonic', '/', 'whistler', 'instability', 'is', 'driven', 'by', 'the', 'electron', 'beam', 'with', '$', 'V', '_', '{', '\\\\', 'mathrm', '{', 'eb', '}', '}', '\\\\', 'gtrsim7V', '_', '{', '\\\\', 'mathrm', '{', 'A', '}', '}', '$', 'in', 'a', 'regime', 'where', '$', '\\\\beta', '_', '{', '\\\\', 'mathrm', '{', 'ec', '\\\\', 'parallel', '}', '}', '\\\\', 'sim0.1', '-', '2', '$', 'and', '$', 'A', '_', '{', '\\\\', 'mathrm', '{', 'ec', '}', '}', '<1$.', 'Moreover,', 'this', 'study', 'presents', 'electromagnetic', 'responses', 'of', 'each', 'kind', 'of', 'electron', 'instability.', 'These', 'results', 'provide', 'a', 'comprehensive', 'overview', 'for', 'electron', 'instability', 'constraints', 'on', 'core', 'electron', 'temperature', 'anisotropies', 'and', 'electron', 'beams', 'in', 'the', 'solar', 'wind.']\n",
      "Electron temperature anisotropies and electron beams are nonthermal features of the observed nonequilibrium electron velocity distributions in the solar wind. In collision-poor plasmas these nonequilibrium distributions are expected to be regulated by kinetic instabilities through wave-particle interactions. This study considers electron instabilities driven by the interplay of core electron temperature anisotropies and the electron beam, and firstly gives a comprehensive analysis of instabilities in arbitrary directions to the background magnetic field. It clarifies the dominant parameter regime (e.g., parallel core electron plasma beta $\\beta_{\\mathrm{ec\\parallel}}$, core electron temperature anisotropy $A_{\\mathrm{ec}}\\equiv T_{\\mathrm{ec\\perp}}/T_{\\mathrm{ec\\parallel}}$, and electron beam velocity $V_{\\mathrm{eb}}$) for each kind of electron instability (e.g., the electron beam-driven electron acoustic/magnetoacoustic instability, the electron beam-driven whistler instability, the electromagnetic electron cyclotron instability, the electron mirror instability, the electron firehose instability, and the ordinary-mode instability). It finds that the electron beam can destabilize electron acoustic/magnetoacoustic waves in the low-$\\beta_{\\mathrm{ec\\parallel}}$ regime, and whistler waves in the medium- and large-$\\beta_{\\mathrm{ec\\parallel}}$ regime. It also finds that a new oblique fast-magnetosonic/whistler instability is driven by the electron beam with $V_{\\mathrm{eb}}\\gtrsim7V_{\\mathrm{A}}$ in a regime where $\\beta_{\\mathrm{ec\\parallel}}\\sim0.1-2$ and $A_{\\mathrm{ec}}<1$. Moreover, this study presents electromagnetic responses of each kind of electron instability. These results provide a comprehensive overview for electron instability constraints on core electron temperature anisotropies and electron beams in the solar wind.\n",
      "electron electron\n",
      "[ 1.6359476   1.2197661   1.0791715   0.22061068 -0.18464139 -0.62149626\n",
      " -0.85533226  1.1039672  -0.22038344  0.61783147  0.05886897 -0.18137953\n",
      " -0.0267646  -0.25434923 -0.34519613  1.1546748  -0.69344926  0.8576056\n",
      " -0.10625421 -0.5252139  -0.12560001  0.66130257 -0.15873623 -0.65267134\n",
      " -0.36719483  0.34119546 -0.16332695 -0.29758245  0.5986947   0.57589793\n",
      " -1.447281   -0.67508805  1.0717443   0.66420263  0.76023304 -0.61803675\n",
      "  0.14616334 -0.22135796  0.45021382  1.762789    0.01802069 -0.7867534\n",
      "  0.25562203 -0.00248158 -0.31264883 -0.500069   -1.0548601   0.2054412\n",
      "  0.7278371  -0.03351544  0.8509996  -0.29540485  0.53208077  0.35618272\n",
      " -0.46070662 -0.60723644  0.06455062 -0.92349976  1.0978587   0.25548095\n",
      " -1.0649534   0.11434057  0.880379   -0.5298178  -0.31597018 -1.0172052\n",
      " -1.0687377  -0.29594228 -1.1326357  -0.41508007  0.4148717  -0.5590596\n",
      "  1.0389106  -0.55146056  0.76399124 -0.50455785  0.20212033 -1.0382758\n",
      " -0.8134848  -0.18122752  0.14224909  0.89626884  0.9855679   0.3110178\n",
      " -0.5731282  -0.5101104  -1.3693875  -0.16373107 -1.2182004  -0.55963075\n",
      " -0.41879785  0.23132777  0.40513396 -0.33848667 -0.38383394 -0.35923204] [ 2.7703328   1.5390176   0.21056214 -0.01488352 -0.08313948 -0.09491304\n",
      " -0.8471716   2.618311   -1.4331765   0.24742734 -0.42597148 -1.3123201\n",
      " -0.14127341  0.09188929  0.0938527   0.43867904  0.12501198  0.8718308\n",
      "  0.8588989   0.0626421  -0.28229123  0.72030056 -0.70382816  0.0171567\n",
      " -0.37416056 -1.0292716   0.0307129   0.52970225  0.8113474   1.1854209\n",
      " -0.02541561 -1.2147251   0.74049044 -0.2964272   0.54199684 -0.81558937\n",
      " -0.15558788  0.31275377 -0.08123937  1.5349     -0.2783133  -0.40094742\n",
      "  0.30241776  0.7604102  -1.3692173  -1.9603758  -0.7999906  -0.59068114\n",
      "  0.36609805  0.25954527  1.4171934  -0.40117082 -0.4586087   0.95187294\n",
      "  0.48136365 -1.1496458  -1.1772215  -0.7615105   2.0626495   0.92029583\n",
      " -0.9727121  -0.04132877  2.091956   -0.66845226 -0.698822   -0.7950918\n",
      " -0.72305983 -0.08675072 -0.09036727 -0.05655383  0.5899816  -0.09198999\n",
      "  0.93528974  0.8922069   0.8931733  -0.47233358  0.6042206  -1.6542598\n",
      " -0.50003594 -0.09838156 -0.7332028   0.3233594  -0.5017176   1.1924632\n",
      "  0.49267215 -0.00931436 -1.2725142  -0.5744523  -0.4608453  -0.46871045\n",
      " -0.7724104  -0.8344298   0.6487079  -0.2805196  -0.17872076 -0.64508796]\n"
     ]
    }
   ],
   "source": [
    "special_cases = {\"\\\\\": [{\"ORTH\": \"\\\\\"}]}\n",
    "prefix_re = re.compile(r'''^[\\$\"'\\{\\(\\[]''')\n",
    "suffix_re = re.compile(r'''[\\$\"'\\}\\)\\]]$''')\n",
    "infix_re = re.compile(r'''[-~_/\\{\\}\\[\\]\\\\]''')\n",
    "simple_url_re = re.compile(r'''^https?://''')\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, rules=special_cases,\n",
    "                                prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                url_match=simple_url_re.match)\n",
    "\n",
    "nlp = spacy.load( \"en_core_web_sm\" )\n",
    "nlp.tokenizer = custom_tokenizer( nlp )\n",
    "doc = nlp( ex_abstract )\n",
    "\n",
    "# get the tokenized version of the sentence\n",
    "print([token.text for token in doc])\n",
    "print(''.join(token.text_with_ws for token in doc))\n",
    "\n",
    "# TODO : Is there a way to give it a list of tokens and have it recreate the original input?\n",
    "\n",
    "# problems : why are 2 identical words different vectors?\n",
    "print(doc[13].text, doc[4].text)\n",
    "print(doc[13].vector, doc[4].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1e830",
   "metadata": {},
   "source": [
    "## Custom\n",
    "Doesn't really work right now, taken from SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "37fe6793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Electron', 'temperature', 'anisotropies', 'and', 'electron', 'beams', 'are', 'nonthermal', 'features', 'of', 'the', 'observed', 'nonequilibrium', 'electron', 'velocity', 'distributions', 'in', 'the', 'solar', 'wind.', 'In', 'collision-poor', 'plasmas', 'these', 'nonequilibrium', 'distributions', 'are', 'expected', 'to', 'be', 'regulated', 'by', 'kinetic', 'instabilities', 'through', 'wave-particle', 'interactions.', 'This', 'study', 'considers', 'electron', 'instabilities', 'driven', 'by', 'the', 'interplay', 'of', 'core', 'electron', 'temperature', 'anisotropies', 'and', 'the', 'electron', 'beam,', 'and', 'firstly', 'gives', 'a', 'comprehensive', 'analysis', 'of', 'instabilities', 'in', 'arbitrary', 'directions', 'to', 'the', 'background', 'magnetic', 'field.', 'It', 'clarifies', 'the', 'dominant', 'parameter', 'regime', '(e.g.,', 'parallel', 'core', 'electron', 'plasma', 'beta', '$\\\\beta_{\\\\mathrm{ec\\\\parallel}}$,', 'core', 'electron', 'temperature', 'anisotropy', '$A_{\\\\mathrm{ec}}\\\\equiv', 'T_{\\\\mathrm{ec\\\\perp}}/T_{\\\\mathrm{ec\\\\parallel}}$,', 'and', 'electron', 'beam', 'velocity', '$V_{\\\\mathrm{eb}}$)', 'for', 'each', 'kind', 'of', 'electron', 'instability', '(e.g.,', 'the', 'electron', 'beam-driven', 'electron', 'acoustic/magnetoacoustic', 'instability,', 'the', 'electron', 'beam-driven', 'whistler', 'instability,', 'the', 'electromagnetic', 'electron', 'cyclotron', 'instability,', 'the', 'electron', 'mirror', 'instability,', 'the', 'electron', 'firehose', 'instability,', 'and', 'the', 'ordinary-mode', 'instability).', 'It', 'finds', 'that', 'the', 'electron', 'beam', 'can', 'destabilize', 'electron', 'acoustic/magnetoacoustic', 'waves', 'in', 'the', 'low-$\\\\beta_{\\\\mathrm{ec\\\\parallel}}$', 'regime,', 'and', 'whistler', 'waves', 'in', 'the', 'medium-', 'and', 'large-$\\\\beta_{\\\\mathrm{ec\\\\parallel}}$', 'regime.', 'It', 'also', 'finds', 'that', 'a', 'new', 'oblique', 'fast-magnetosonic/whistler', 'instability', 'is', 'driven', 'by', 'the', 'electron', 'beam', 'with', '$V_{\\\\mathrm{eb}}\\\\gtrsim7V_{\\\\mathrm{A}}$', 'in', 'a', 'regime', 'where', '$\\\\beta_{\\\\mathrm{ec\\\\parallel}}\\\\sim0.1-2$', 'and', '$A_{\\\\mathrm{ec}}<1$.', 'Moreover,', 'this', 'study', 'presents', 'electromagnetic', 'responses', 'of', 'each', 'kind', 'of', 'electron', 'instability.', 'These', 'results', 'provide', 'a', 'comprehensive', 'overview', 'for', 'electron', 'instability', 'constraints', 'on', 'core', 'electron', 'temperature', 'anisotropies', 'and', 'electron', 'beams', 'in', 'the', 'solar', 'wind.']\n"
     ]
    }
   ],
   "source": [
    "class WhitespaceTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(\" \")\n",
    "        return Doc(self.vocab, words=words)\n",
    "\n",
    "class BertTokenizer:\n",
    "    def __init__(self, vocab, vocab_file, lowercase=True):\n",
    "        self.vocab = vocab\n",
    "        self._tokenizer = BertWordPieceTokenizer(vocab_file, lowercase=lowercase)\n",
    "\n",
    "    def __call__(self, text):\n",
    "        tokens = self._tokenizer.encode(text)\n",
    "        words = []\n",
    "        spaces = []\n",
    "        for i, (text, (start, end)) in enumerate(zip(tokens.tokens, tokens.offsets)):\n",
    "            words.append(text)\n",
    "            if i < len(tokens.tokens) - 1:\n",
    "                # If next start != current end we assume a space in between\n",
    "                next_start, next_end = tokens.offsets[i + 1]\n",
    "                spaces.append(next_start > end)\n",
    "            else:\n",
    "                spaces.append(True)\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "\n",
    "txt_file = \"\"\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab, )\n",
    "doc = nlp( ex_abstract )\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc553cb",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b608bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, normalizers, pre_tokenizers, decoders, processors\n",
    "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
    "from tokenizers.normalizers import NFD, NFKD, NFC, NFKC, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import ByteLevel, Whitespace, WhitespaceSplit, Punctuation, Metaspace,\\\n",
    "                                        CharDelimiterSplit\n",
    "from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordPieceTrainer, WordLevelTrainer\n",
    "from pathlib import Path\n",
    "\n",
    "#from transformers import PreTrainedTokenizerFast, PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d6ec172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_token = \"<s>\"\n",
    "pad_token = \"<pad>\"\n",
    "eos_token = \"</s>\"\n",
    "unk_token = \"<unk>\"\n",
    "mask_token = \"<mask>\"\n",
    "\n",
    "special_token_list = [bos_token, pad_token, eos_token, unk_token, mask_token]\n",
    "\n",
    "class BPE_token(object):\n",
    "    def __init__(self):\n",
    "        # instantiate\n",
    "        self.tokenizer = Tokenizer(BPE())\n",
    "        \n",
    "        # normalization\n",
    "        self.tokenizer.normalizer = Sequence([\n",
    "            NFKC()\n",
    "        ])\n",
    "        \n",
    "        # pre-tokenizer\n",
    "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
    "        \n",
    "        # decoder\n",
    "        self.tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "    def bpe_train(self, iterator):\n",
    "        trainer = BpeTrainer(vocab_size=50000, show_progress=True, inital_alphabet=ByteLevel.alphabet()\n",
    "                                             , special_tokens=special_token_list)\n",
    "        self.tokenizer.train_from_iterator(trainer=trainer, iterator=iterator) # paths is iterator\n",
    "\n",
    "    def save_tokenizer(self, location, prefix=None):\n",
    "        if not os.path.exists(location):\n",
    "            os.makedirs(location)\n",
    "        self.tokenizer.model.save(location, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8f40d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_tokenizer(token_model, data_iterator, token_dir, token_filename, vocab_size=30000, vocab=None\n",
    "                          , max_input_chars_per_word=None):\n",
    "    \"\"\"\n",
    "    Building a Tokenizer using HuggingFace library. The pipeline seems to be:\n",
    "    \n",
    "        - Model           : algorithm that tokenizes, it is a mandatory component. There are\n",
    "                            only 4 models implemented (BPE, Unigram, WordLevel, WordPiece)\n",
    "        - Normalizer      : some preprocessing that could happen before, but doesn't necessarily\n",
    "        - Pre-Tokenizer   : splitting the input according to some rules\n",
    "        - Post-Processing : needing to add some tokens/input after (mostly seems to be eos\n",
    "                            , bos tokens)\n",
    "        - Decoder         : certain previous pipeline steps need to be reversed for proper\n",
    "                            decoding\n",
    "        - Trainer         : The corresponding training algorithm for the model\n",
    "    \n",
    "    Note : Some pre-processing might need to happen beforehand in previous functions (might\n",
    "            be easier using pandas)\n",
    "    \n",
    "    Input\n",
    "        token_model              : algorithm to use for tokenization\n",
    "        data_iterator            : a python iterator that goes through the data to be used for \n",
    "                                    training\n",
    "        token_dir                : directory with tokenizers\n",
    "        vocab_size               : size of the vocabulary to use\n",
    "        token_filename           : filename of particular token we want to train. Will overwrite\n",
    "                                    previously save files.\n",
    "        vocab                    : models other than BPE can use non-mandatory vocab as input\n",
    "        max_input_chars_per_word : used for WordPiece\n",
    "        \n",
    "    Output\n",
    "        tokenizer                : huggingFace Tokenizer object, our fully trainer tokenizer\n",
    "            \n",
    "    \"\"\"\n",
    "    special_token_lst = [unk_token, bos_token, eos_token, pad_token, mask_token]\n",
    "    \n",
    "    normalizer_lst = [NFKC()]; pre_tokenizer_lst = [ByteLevel()]; decoder_lst = []\n",
    "    \n",
    "    bos_idx = special_token_list.index(bos_token); eos_idx = special_token_list.index(eos_token)\n",
    "    \n",
    "    if token_model == 'BPE':\n",
    "        model   = BPE(unk_token=unk_token) \n",
    "        Trainer = BpeTrainer\n",
    "    elif token_model == 'Unigram':\n",
    "        model   = Unigram(vocab=vocab) \n",
    "        Trainer = UnigramTrainer\n",
    "    elif token_model == 'WordLevel':\n",
    "        model   = WordLevel(unk_token=unk_token,vocab=vocab)\n",
    "        Trainer = WordLevelTrainer\n",
    "    elif token_model == 'WordPiece':\n",
    "        model   = WordPiece(unk_token=unk_token,vocab=vocab, max_input_chars_per_word=max_input_chars_per_word)\n",
    "        Trainer = WordPieceTrainer\n",
    "        decoder_lst.append( decoders.WordPiece())\n",
    "    else:\n",
    "        error_msg = f'Error: token_model ({token_model}) not an algorithm in [BPE, Unigram, WordLevel, WordPiece]'\n",
    "        raise SystemExit(error_msg)       \n",
    "    \n",
    "    # instantiation\n",
    "    tokenizer = Tokenizer(model)\n",
    "    \n",
    "    # trainer \n",
    "    trainer = Trainer(vocab_size=vocab_size, show_progress=True, special_tokens=special_tokens_lst)\n",
    "    \n",
    "    # normalizer\n",
    "    tokenizer.normalizer = normalizers.Sequence( normalizer_lst )\n",
    "    \n",
    "    # pre-tokenizer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Sequence( pre_tokenizer_lst )\n",
    "    \n",
    "    # post-processing\n",
    "    tokenizer.post_processor = processors.TemplateProcessing( single=bos_token+\" $A \"+eos_token\n",
    "                                                    #, pair=bos_token+\" $A \"+eos_token\" $B:1 \"+eos_token+\":1\"\n",
    "                                                    , special_tokens=[(bos_token, bos_idx),(eos_token, eos_idx)]\n",
    "                                                    )\n",
    "    \n",
    "    # decoder\n",
    "    if ByteLevel() in pre_tokenizer_lst: decoder_lst.append( decoders.ByteLevel() )\n",
    "    if Metaspace() in pre_tokenizer_lst: decoder_lst.append( decoders.Metaspace() ) \n",
    "    tokenizer.decoder = decoders.Sequence( decoder_lst )\n",
    "\n",
    "    tokenizer.train_from_iterator(trainer=trainer, iterator=data_iterator)\n",
    "    \n",
    "    if not os.path.exists( token_dir ):\n",
    "        os.makedirs( token_dir )\n",
    "    if os.path.exists( token_dir + os.sep + token_filename ):\n",
    "        print(f\"Warning : overwriting previously save tokenizer with same filename ( {token_filename} ).\")\n",
    "    tokenizer.save( token_dir + os.sep + token_filename )\n",
    "        \n",
    "    # TODO : Should I add PreTrained and Fast Tokenizer here? Seems like it might be appropriate.\n",
    "    transformer = False; fast = False\n",
    "    function_from_transformer_todo = None\n",
    "    if transformer:\n",
    "        raise SystemExit(\"HuggingFace transformers library not yet implemented here!\")\n",
    "        if fast: tokenizer = function_from_transformer_todo\n",
    "        else: tokenizer = function_from_transformer_todo\n",
    "                  \n",
    "    return tokenizer\n",
    "    \n",
    "    \n",
    "def load_custom_tokenizer(token_dir, token_filename, transformer=False, fast=False):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        token_dir      : directory with tokenizers saved\n",
    "        token_filename : trained tokenizer that we want to load\n",
    "        transformer    : (bool) whether to use HuggingFace transformers library implementation\n",
    "        fast           : (bool) whether to use HuggingFace transformers fast implementation\n",
    "    Output\n",
    "        tokenizer      : tokenizer from Tokenizer class to be passed to rest of algorithm\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer.from_file(token_dir + os.sep + token_filename)\n",
    "    \n",
    "    function_from_transformer_todo = None\n",
    "    if function_from_transformer != None:\n",
    "        if transformer:\n",
    "            raise SystemExit(\"HuggingFace transformers library not yet implemented here!\")\n",
    "            if fast: tokenizer = function_from_transformer_todo\n",
    "            else: tokenizer = function_from_transformer_todo\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b8335eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1536, 244, 594, 115, 94, 745, 201, 1529, 1470, 108, 93, 589, 832, 94, 830, 834, 101, 93, 523, 701, 11, 441, 1460, 10, 947, 1276, 516, 832, 834, 201, 1380, 143, 159, 1211, 203, 827, 564, 809, 383, 10, 735, 333, 11, 730, 822, 1351, 94, 564, 697, 203, 93, 820, 108, 419, 94, 244, 594, 115, 93, 94, 240, 9, 115, 1467, 1200, 88, 826, 1336, 108, 564, 101, 1447, 1401, 143, 93, 815, 1106, 343, 11, 442, 1539, 93, 1399, 1528, 342, 209, 56, 11, 58, 608, 781, 419, 94, 586, 1076, 438, 395, 215, 214, 78, 246, 49, 382, 765, 419, 94, 244, 1304, 188, 26, 215, 214, 78, 246, 569, 1305, 156, 215, 214, 78, 246, 49, 1162, 1188, 43, 215, 214, 78, 246, 49, 382, 765, 115, 94, 240, 830, 188, 45, 215, 214, 78, 623, 1257, 139, 542, 754, 108, 94, 224, 209, 56, 11, 58, 608, 93, 94, 240, 10, 622, 94, 670, 12, 784, 224, 9, 93, 94, 240, 10, 622, 749, 224, 9, 93, 806, 94, 1463, 224, 9, 93, 94, 1479, 224, 9, 93, 94, 1514, 224, 9, 115, 93, 1513, 10, 1425, 224, 839, 442, 772, 189, 93, 94, 240, 323, 1507, 94, 670, 12, 784, 758, 101, 93, 432, 607, 395, 215, 214, 78, 246, 49, 382, 340, 342, 9, 115, 749, 758, 101, 93, 1111, 10, 115, 550, 607, 395, 215, 214, 78, 246, 49, 382, 340, 342, 11, 442, 1218, 772, 189, 88, 1255, 1509, 1073, 10, 1530, 12, 978, 224, 145, 697, 203, 93, 94, 240, 181, 188, 45, 215, 214, 78, 623, 569, 1420, 20, 45, 215, 214, 78, 26, 340, 101, 88, 342, 748, 438, 395, 215, 214, 78, 246, 49, 382, 569, 497, 13, 11, 14, 10, 15, 5, 115, 188, 26, 215, 214, 78, 246, 1189, 14, 837, 1243, 9, 269, 822, 1274, 806, 1349, 108, 542, 754, 108, 94, 224, 11, 1171, 1390, 1516, 88, 826, 1320, 139, 94, 224, 1493, 249, 419, 94, 244, 594, 115, 94, 745, 101, 93, 523, 701, 11, 2]\n",
      "['<s>', 'ĠElectron', 'Ġtemperature', 'Ġanisotropies', 'Ġand', 'Ġelectron', 'Ġbeams', 'Ġare', 'Ġnonthermal', 'Ġfeatures', 'Ġof', 'Ġthe', 'Ġobserved', 'Ġnonequilibrium', 'Ġelectron', 'Ġvelocity', 'Ġdistributions', 'Ġin', 'Ġthe', 'Ġsolar', 'Ġwind', '.', 'ĠIn', 'Ġcollision', '-', 'poor', 'Ġplasmas', 'Ġthese', 'Ġnonequilibrium', 'Ġdistributions', 'Ġare', 'Ġexpected', 'Ġto', 'Ġbe', 'Ġregulated', 'Ġby', 'Ġkinetic', 'Ġinstabilities', 'Ġthrough', 'Ġwave', '-', 'particle', 'Ġinteractions', '.', 'ĠThis', 'Ġstudy', 'Ġconsiders', 'Ġelectron', 'Ġinstabilities', 'Ġdriven', 'Ġby', 'Ġthe', 'Ġinterplay', 'Ġof', 'Ġcore', 'Ġelectron', 'Ġtemperature', 'Ġanisotropies', 'Ġand', 'Ġthe', 'Ġelectron', 'Ġbeam', ',', 'Ġand', 'Ġfirstly', 'Ġgives', 'Ġa', 'Ġcomprehensive', 'Ġanalysis', 'Ġof', 'Ġinstabilities', 'Ġin', 'Ġarbitrary', 'Ġdirections', 'Ġto', 'Ġthe', 'Ġbackground', 'Ġmagnetic', 'Ġfield', '.', 'ĠIt', 'Ġclarifies', 'Ġthe', 'Ġdominant', 'Ġparameter', 'Ġregime', 'Ġ(', 'e', '.', 'g', '.,', 'Ġparallel', 'Ġcore', 'Ġelectron', 'Ġplasma', 'Ġbeta', 'Ġ$\\\\', 'beta', '_{\\\\', 'mathrm', '{', 'ec', '\\\\', 'parallel', '}}$,', 'Ġcore', 'Ġelectron', 'Ġtemperature', 'Ġanisotropy', 'Ġ$', 'A', '_{\\\\', 'mathrm', '{', 'ec', '}}\\\\', 'equiv', 'ĠT', '_{\\\\', 'mathrm', '{', 'ec', '\\\\', 'perp', '}}/', 'T', '_{\\\\', 'mathrm', '{', 'ec', '\\\\', 'parallel', '}}$,', 'Ġand', 'Ġelectron', 'Ġbeam', 'Ġvelocity', 'Ġ$', 'V', '_{\\\\', 'mathrm', '{', 'eb', '}}$)', 'Ġfor', 'Ġeach', 'Ġkind', 'Ġof', 'Ġelectron', 'Ġinstability', 'Ġ(', 'e', '.', 'g', '.,', 'Ġthe', 'Ġelectron', 'Ġbeam', '-', 'driven', 'Ġelectron', 'Ġacoustic', '/', 'magnetoacoustic', 'Ġinstability', ',', 'Ġthe', 'Ġelectron', 'Ġbeam', '-', 'driven', 'Ġwhistler', 'Ġinstability', ',', 'Ġthe', 'Ġelectromagnetic', 'Ġelectron', 'Ġcyclotron', 'Ġinstability', ',', 'Ġthe', 'Ġelectron', 'Ġmirror', 'Ġinstability', ',', 'Ġthe', 'Ġelectron', 'Ġfirehose', 'Ġinstability', ',', 'Ġand', 'Ġthe', 'Ġordinary', '-', 'mode', 'Ġinstability', ').', 'ĠIt', 'Ġfinds', 'Ġthat', 'Ġthe', 'Ġelectron', 'Ġbeam', 'Ġcan', 'Ġdestabilize', 'Ġelectron', 'Ġacoustic', '/', 'magnetoacoustic', 'Ġwaves', 'Ġin', 'Ġthe', 'Ġlow', '-$\\\\', 'beta', '_{\\\\', 'mathrm', '{', 'ec', '\\\\', 'parallel', '}}$', 'Ġregime', ',', 'Ġand', 'Ġwhistler', 'Ġwaves', 'Ġin', 'Ġthe', 'Ġmedium', '-', 'Ġand', 'Ġlarge', '-$\\\\', 'beta', '_{\\\\', 'mathrm', '{', 'ec', '\\\\', 'parallel', '}}$', 'Ġregime', '.', 'ĠIt', 'Ġalso', 'Ġfinds', 'Ġthat', 'Ġa', 'Ġnew', 'Ġoblique', 'Ġfast', '-', 'magnetosonic', '/', 'whistler', 'Ġinstability', 'Ġis', 'Ġdriven', 'Ġby', 'Ġthe', 'Ġelectron', 'Ġbeam', 'Ġwith', 'Ġ$', 'V', '_{\\\\', 'mathrm', '{', 'eb', '}}\\\\', 'gtrsim', '7', 'V', '_{\\\\', 'mathrm', '{', 'A', '}}$', 'Ġin', 'Ġa', 'Ġregime', 'Ġwhere', 'Ġ$\\\\', 'beta', '_{\\\\', 'mathrm', '{', 'ec', '\\\\', 'parallel', '}}\\\\', 'sim', '0', '.', '1', '-', '2', '$', 'Ġand', 'Ġ$', 'A', '_{\\\\', 'mathrm', '{', 'ec', '}}<', '1', '$.', 'ĠMoreover', ',', 'Ġthis', 'Ġstudy', 'Ġpresents', 'Ġelectromagnetic', 'Ġresponses', 'Ġof', 'Ġeach', 'Ġkind', 'Ġof', 'Ġelectron', 'Ġinstability', '.', 'ĠThese', 'Ġresults', 'Ġprovide', 'Ġa', 'Ġcomprehensive', 'Ġoverview', 'Ġfor', 'Ġelectron', 'Ġinstability', 'Ġconstraints', 'Ġon', 'Ġcore', 'Ġelectron', 'Ġtemperature', 'Ġanisotropies', 'Ġand', 'Ġelectron', 'Ġbeams', 'Ġin', 'Ġthe', 'Ġsolar', 'Ġwind', '.', '</s>']\n",
      " Electron temperature anisotropies and electron beams are nonthermal features of the observed nonequilibrium electron velocity distributions in the solar wind. In collision-poor plasmas these nonequilibrium distributions are expected to be regulated by kinetic instabilities through wave-particle interactions. This study considers electron instabilities driven by the interplay of core electron temperature anisotropies and the electron beam, and firstly gives a comprehensive analysis of instabilities in arbitrary directions to the background magnetic field. It clarifies the dominant parameter regime (e.g., parallel core electron plasma beta $\\beta_{\\mathrm{ec\\parallel}}$, core electron temperature anisotropy $A_{\\mathrm{ec}}\\equiv T_{\\mathrm{ec\\perp}}/T_{\\mathrm{ec\\parallel}}$, and electron beam velocity $V_{\\mathrm{eb}}$) for each kind of electron instability (e.g., the electron beam-driven electron acoustic/magnetoacoustic instability, the electron beam-driven whistler instability, the electromagnetic electron cyclotron instability, the electron mirror instability, the electron firehose instability, and the ordinary-mode instability). It finds that the electron beam can destabilize electron acoustic/magnetoacoustic waves in the low-$\\beta_{\\mathrm{ec\\parallel}}$ regime, and whistler waves in the medium- and large-$\\beta_{\\mathrm{ec\\parallel}}$ regime. It also finds that a new oblique fast-magnetosonic/whistler instability is driven by the electron beam with $V_{\\mathrm{eb}}\\gtrsim7V_{\\mathrm{A}}$ in a regime where $\\beta_{\\mathrm{ec\\parallel}}\\sim0.1-2$ and $A_{\\mathrm{ec}}<1$. Moreover, this study presents electromagnetic responses of each kind of electron instability. These results provide a comprehensive overview for electron instability constraints on core electron temperature anisotropies and electron beams in the solar wind.\n"
     ]
    }
   ],
   "source": [
    "# the folder 'text' contains all the files\n",
    "data_iter = iter(raw_data.summary.tolist())\n",
    "\n",
    "tokenizer = BPE_token()\n",
    "\n",
    "# train the tokenizer model\n",
    "tokenizer.bpe_train(data_iter)\n",
    "\n",
    "# saving the tokenized data in our specified folder \n",
    "save_path = 'tokenized_data'\n",
    "tokenizer.save_tokenizer(save_path)\n",
    "\n",
    "string_tokenized = tokenizer.tokenizer.encode(bos_token + ex_abstract + eos_token )\n",
    "decoded = tokenizer.tokenizer.decode(string_tokenized.ids)\n",
    "print(string_tokenized.ids)\n",
    "print(string_tokenized.tokens)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0486e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
