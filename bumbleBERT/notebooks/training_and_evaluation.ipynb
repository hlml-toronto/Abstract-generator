{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "MZmtW9scbJ9Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1632422718135,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "MZmtW9scbJ9Y",
    "outputId": "af1efac7-2891-49b0-ca37-afde5eb8d45a"
   },
   "outputs": [],
   "source": [
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    %cd /content/drive/MyDrive/Github/Abstract-generator/bumbleBERT/notebooks\n",
    "    \n",
    "    %%capture\n",
    "    !pip install feedparser tokenizers transformers;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04f6ae9e",
   "metadata": {
    "executionInfo": {
     "elapsed": 3839,
     "status": "ok",
     "timestamp": 1632422319956,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "04f6ae9e"
   },
   "outputs": [],
   "source": [
    "import os, torch, time, math, sys, re, csv\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('..' + os.sep )\n",
    "from src import default\n",
    "\n",
    "from src.data import download as dl, tokenization as tkn, custom_dataset as cd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from src.model.transformer_hf import TransformerModel\n",
    "from src.model.batching import CustomBatch\n",
    "from src.model.generate_text import gen_some_text\n",
    "from src.model.train_evaluate import train, evaluate\n",
    "#from src.model.transformer import make_gpt_model # imports don't work\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904013cd",
   "metadata": {
    "id": "904013cd"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da73442b",
   "metadata": {
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1632422325957,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "da73442b"
   },
   "outputs": [],
   "source": [
    "# ARCHITECTURE\n",
    "maxLen     = 35 # maximum sentence length\n",
    "vocabSize  = None # None if you want to let tokenizer do its thing\n",
    "emsize     = 512 # embedding dimension\n",
    "nhid       = 2048 # the dimension of the feedforward network model in torch.nn.TransformerEncoder\n",
    "nlayers    = 12 # the number of torch.nn.TransformerEncoderLayer in torch.nn.TransformerEncoder\n",
    "nhead      = 8 # the number of heads in the multiheadattention models\n",
    "dropout    = 0.2 # the dropout value\n",
    "batch_size = 10 #32\n",
    "val_batch_size = 10 #32\n",
    "epochs     = 10  # The number of epochs\n",
    "\n",
    "# TOKENIZER\n",
    "tknzerType = 'BPE' # type of tokenizing algorithm\n",
    "trainTokenizer = True # whether to train a new tokenizer or use one already trained\n",
    "\n",
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf83d8",
   "metadata": {
    "id": "33bf83d8"
   },
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67302c43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 645,
     "status": "ok",
     "timestamp": 1632422331293,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "67302c43",
    "outputId": "7f7754ad-6e1b-4d0d-d80c-20686ed70e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Using arxiv_10000.csv for training <<\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "nbrResults = 10**4 # number of data samples to download\n",
    "filename = f'arxiv_{nbrResults}'\n",
    "extension = '.csv'\n",
    "filename += extension\n",
    "\n",
    "filepath = default.RAW_DATA_DIR + os.sep + filename\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    dl.arxiv_api( default.RAW_DATA_DIR, filename, max_results=nbrResults ) # TODO : CHANGE SO THAT NOT CONSTANTLY LOADING DATA\n",
    "print(f'>> Using {filename} for training <<')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21983a34",
   "metadata": {
    "id": "21983a34"
   },
   "source": [
    "### Format Dataset\n",
    "\n",
    "Uses a custom dataset class, which is an iterable and callable structure that returns a sample from our dataset. Within this custom dataset, can determine all preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "015ae225",
   "metadata": {
    "executionInfo": {
     "elapsed": 2574,
     "status": "ok",
     "timestamp": 1632422336336,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "015ae225"
   },
   "outputs": [],
   "source": [
    "# create dataset\n",
    "dataset = cd.ArxivDataset(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11976e8e",
   "metadata": {
    "id": "11976e8e"
   },
   "source": [
    "### Training Tokenizer\n",
    "\n",
    "Training of a custom tokenizer. Many options possible here, check the tokenizer training functions to try out various strategies. If he tokenizer for the dataset has already been trained, no need to run this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3a132c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4557,
     "status": "ok",
     "timestamp": 1632422343490,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "d3a132c4",
    "outputId": "00864287-887d-41d6-c0c1-172eefa595b5"
   },
   "outputs": [],
   "source": [
    "trainTokenizer: _ = tkn.train_custom_tokenizer(tknzerType, dataset, filename\n",
    "                                            , default.TOK_DIR\n",
    "                                            , vocabSize\n",
    "                                            , **default.special_token_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3089723",
   "metadata": {
    "id": "f3089723"
   },
   "source": [
    "### Loading Tokenizer and Splitting Datasets\n",
    "\n",
    "For some reason, torch tokenizers are not callable as trained. This is confusing, but c'est la vie! Instead, need to load it from file it was saved in using the PreTrainedTokenizerFast class (__call__) implemented in here. Once that's done, you can add this tokenizer as a transform to your dataset! Useful.\n",
    "\n",
    "We also split the dataset here into training, testing and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe434fb7",
   "metadata": {
    "executionInfo": {
     "elapsed": 214,
     "status": "ok",
     "timestamp": 1632422348108,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "fe434fb7"
   },
   "outputs": [],
   "source": [
    "tknzrFile = default.TOK_DIR + os.sep + filename + '_' + tknzerType + '.json'\n",
    "\n",
    "# load PreTrainedTokenizerFast, for __call__. __call__ not implemented in\n",
    "# the base Tokenizer class... that sounds silly, but it is what it is\n",
    "tknzr = tkn.load_tokenizer(tknzrFile, **default.special_token_lst)\n",
    "\n",
    "if vocabSize is None: vocabSize = tknzr.vocab_size\n",
    "\n",
    "# set tknzr as the transform\n",
    "dataset.set_transform( tknzr )\n",
    "\n",
    "# separate dataset into train, test valid TODO : make into a function\n",
    "fracTrain, fracTest, fracVal = ( 0.7, 0.2, 0.1)\n",
    "trainTestVal = [ np.floor(fracTrain*len(dataset))\\\n",
    "                    , np.floor(fracTest*len(dataset))\\\n",
    "                    , len(dataset) - ( np.floor( fracTrain*len(dataset) ) +\n",
    "                    np.floor( fracTest*len(dataset) ) )\n",
    "                    ]\n",
    "\n",
    "trainDataset, testDataset, valDataset =\\\n",
    "        torch.utils.data.random_split(dataset, [int(x) for x in trainTestVal]\n",
    "                                , generator=torch.Generator().manual_seed(42) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bba4cb9",
   "metadata": {
    "id": "4bba4cb9"
   },
   "source": [
    "### Creating DataLoaders\n",
    "\n",
    "Training is done on batches, so we need a way to extract groupings of the data in the appropriate format for our transformer model.\n",
    "Note that for transformers which we are training, dataloaders outputs both src (x[:-1] and tgt ([1:]).\n",
    "The collation of batches for different transformer models we have vary. For HuggingFace it's ( maxLen x batch_size ) whereas I think that the Annotated Transformer has ( batch_size x maxLen ).\n",
    "\n",
    "NOTE : Do not use the tokenizer before the training if you use num_workers>0!\n",
    "FastTokenizer does not play nicely with forking if you use it before the forking of your data:\n",
    "https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c11048fc",
   "metadata": {
    "executionInfo": {
     "elapsed": 502,
     "status": "ok",
     "timestamp": 1632422351212,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "c11048fc"
   },
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "# uses collate function to transform batch to correct dimensions\n",
    "def collate_wrapper(batch):\n",
    "    return CustomBatch(batch, dim=0, maxLenModel=maxLen, padValue=tknzr.get_vocab()[\"<pad>\"])\n",
    "\n",
    "# dataloader for training\n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=batch_size, shuffle=True\n",
    "                                        , num_workers=2\n",
    "                                        , collate_fn=collate_wrapper\n",
    "                                        , pin_memory=True\n",
    "                                        )\n",
    "# dataloader for validation\n",
    "valDataLoader = DataLoader(valDataset, batch_size=val_batch_size, shuffle=True\n",
    "                                        , num_workers=2\n",
    "                                        , collate_fn=collate_wrapper\n",
    "                                        , pin_memory=True\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3955854",
   "metadata": {
    "id": "e3955854"
   },
   "source": [
    "### Selecting model\n",
    "\n",
    "Here we choose which model we shall use for training. For now, I've selected the black box Transformer from HuggingFace because the collate_fn I've written gives the correct input size force it... however this can easily be changed! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6de7d252",
   "metadata": {
    "executionInfo": {
     "elapsed": 969,
     "status": "ok",
     "timestamp": 1632422760770,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "6de7d252"
   },
   "outputs": [],
   "source": [
    "# transformer from huggingface\n",
    "# TODO : Change to the Annotated Transformer if I want\n",
    "model = TransformerModel(vocabSize, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "\n",
    "# criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "# learning rate Matt used with Adam is 0.5\n",
    "paramsAdam  = [{'params' : model.parameters(), 'lr' : 1e-3, 'betas' : (0.9, 0.999), 'eps' : 1e-08, 'weight_decay' : 0.0}]\n",
    "paramsAdamW = [{'params' : model.parameters(), 'lr' : 5e-5, 'betas' : (0.9, 0.999), 'eps' : 1e-08, 'weight_decay' : 0.0}]\n",
    "paramsSGD   = [{'params' : model.parameters(), 'lr' : 0.5, 'momentum' : 0.0, 'dampening' : 0.0, 'weight_decay' : 0.0}]\n",
    "\n",
    "#optimizer = torch.optim.SGD( paramsSGD )\n",
    "#optimizer = torch.optim.Adam( paramsAdam )\n",
    "optimizer = torch.optim.AdamW( paramsAdamW )\n",
    "\n",
    "# scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780d86f",
   "metadata": {
    "id": "9780d86f"
   },
   "source": [
    "### Training\n",
    "\n",
    "Training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75775f0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 745071,
     "status": "ok",
     "timestamp": 1632423535568,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "75775f0e",
    "outputId": "00a9192c-d755-4c4b-a3f3-cf15547131fe"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "view(): argument 'size' must be tuple of ints, but found element of type NoneType at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3c6b99668c17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxLen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxLen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/HLML/Abstract-generator/bumbleBERT/src/model/train_evaluate.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, maxLen, dataLoader, device, vocabSize, epoch, optimizer_, scheduler_, criterion_)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: view(): argument 'size' must be tuple of ints, but found element of type NoneType at pos 2"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train( model, maxLen, trainDataLoader, device, vocabSize, epoch, optimizer, scheduler, criterion)\n",
    "        val_loss = evaluate(model, maxLen, valDataLoader, len(valDataset), device, vocabSize, criterion)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                         val_loss, math.exp(val_loss)))\n",
    "                                         # Why is math.exp so large????\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    # save best model (two methods)\n",
    "    modelFull = default.MODEL_DIR + os.sep + f'{filename}_epoch{epochs}.pth'\n",
    "    modelWeights = default.MODEL_DIR + os.sep + f'{filename}_weights_epoch{epochs}.pth'\n",
    "    modelFullBest = default.MODEL_DIR + os.sep + f'{filename}_epoch{epochs}_best.pth'\n",
    "    modelWeightsBest = default.MODEL_DIR + os.sep + f'{filename}_weights_epoch{epochs}_best.pth'\n",
    "    # approach 1: save model (class) entirely (uses pickle)\n",
    "    torch.save(model, modelFull)\n",
    "    torch.save(best_model, modelFullBest)\n",
    "    # approach 2: save model weights\n",
    "    torch.save(best_model.state_dict(), modelWeightsBest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XM5G4vD0q9TZ",
   "metadata": {
    "id": "XM5G4vD0q9TZ"
   },
   "source": [
    "### Text Generation\n",
    "\n",
    "Here I've simply taken the code Matt uses to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6acee1",
   "metadata": {
    "id": "eb6acee1"
   },
   "outputs": [],
   "source": [
    "if not TRAIN:\n",
    "    customFilename = 'arxiv_10000'\n",
    "    customEpochs = 10\n",
    "    modelFull = default.MODEL_DIR + os.sep + f'{customFilename}_epoch{customEpochs}_best.pth'\n",
    "    modelWeights = default.MODEL_DIR + os.sep + f'{customFilename}_weights_epoch{customEpochs}_best.pth'\n",
    "    \n",
    "    # approach 1: load model (class) entirely (uses pickle)\n",
    "    modelFullLoad = torch.load(modelFull, map_location=device)\n",
    "\n",
    "    # approach 2: load model weights, need to have some parameter or something \n",
    "    modelLoad = TransformerModel(vocabSize, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "    modelWeightsLoad = modelLoad.load_state_dict( torch.load(modelWeights) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "SoGylu7hq7kI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10928,
     "status": "ok",
     "timestamp": 1632423598082,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "SoGylu7hq7kI",
    "outputId": "b5d43790-e2bf-44a5-95f8-2afb5c481e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,  2961,   212,  2625,   125,  3509,   212,   647, 16714,   782,\n",
      "          146,  2771, 12808,  6830,   271,   818,   245, 13380,  1440,   399,\n",
      "         4644,  2415,   125, 24926,   212,   562,   201,   125,     2])\n",
      "29\n",
      "tensor([[    1],\n",
      "        [ 2961],\n",
      "        [  212],\n",
      "        [ 2625],\n",
      "        [  125],\n",
      "        [ 3509],\n",
      "        [  212],\n",
      "        [  647],\n",
      "        [16714],\n",
      "        [  782],\n",
      "        [  146],\n",
      "        [ 2771],\n",
      "        [12808],\n",
      "        [ 6830],\n",
      "        [  271],\n",
      "        [  818],\n",
      "        [  245],\n",
      "        [13380],\n",
      "        [ 1440],\n",
      "        [  399],\n",
      "        [ 4644],\n",
      "        [ 2415],\n",
      "        [  125],\n",
      "        [24926],\n",
      "        [  212],\n",
      "        [  562],\n",
      "        [  201],\n",
      "        [  125],\n",
      "        [    2]])\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.]])\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "next_guess_int, next_guess_string: tensor(0) <pad>\n",
      "Text prompt:\n",
      " Electrons are protons. They are both really small and make my head spin when I think about it too hard. Protons are also electrons.\n",
      "Number of tokens to generate: 100\n",
      "Generated_text:\n",
      " Electrons are protons. They are both really small and make my head spin when I think about it too hard. Protons are also electrons. <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "# inspect both models\n",
    "#print('model_A info...\\n', modelFullLoad)\n",
    "#print('\\nmodel_B info...\\n', modelWeightsLoad)\n",
    "\n",
    "#print('model_A == model_B:', modelFullLoad == modelWeightsLoad)\n",
    "#model = modelFullLoad\n",
    "# Text generation example\n",
    "\n",
    "#model = modelLoad\n",
    "prompt = 'Electrons are protons. They are both really small and make my head spin when I think about it too hard. Protons are also electrons.'\n",
    "ngen = 100\n",
    "decode_style = 'greedy'\n",
    "model.to('cpu')\n",
    "generated_text = gen_some_text(\n",
    "    model, tknzr, 'cpu', maxLen, text_prompt=prompt, tokens_to_gen=ngen, vis=False,\n",
    "    decode_style=decode_style)\n",
    "print(\"Text prompt:\\n\", prompt)\n",
    "print(\"Number of tokens to generate:\", ngen)\n",
    "print(\"Generated_text:\\n\", generated_text)\n",
    "\n",
    "# TODO: alternative generation\n",
    "# currently 'greedy method'\n",
    "# see: https://huggingface.co/blog/how-to-generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "csNTLYQ3otnm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1629397304410,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "csNTLYQ3otnm",
    "outputId": "6796e070-10a6-4446-bc17-8e971fcb5da4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' the'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr.decode([107])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P4EWiRBvpb0T",
   "metadata": {
    "id": "P4EWiRBvpb0T"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "colab_training_and_evaluation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
