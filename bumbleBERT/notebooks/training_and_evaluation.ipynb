{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04f6ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "\n",
    "from src import default\n",
    "import os, torch, time, math, sys, re, csv\n",
    "import numpy as np\n",
    "from src.data import download as dl, data_preprocessing as dpp, tokenization as tkn\\\n",
    "                        , custom_dataset as cd\n",
    "from torch.utils.data import DataLoader\n",
    "from src.model.transformer_hf import TransformerModel, PadCollate\n",
    "#from src.model.transformer import make_gpt_model # imports don't work\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904013cd",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da73442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen     = 250 # maximum sentence length\n",
    "bsz        = 3 # batch size\n",
    "vocabSize  = None # None if you want to let tokenizer do its thing\n",
    "emsize     = 200 # embedding dimension\n",
    "nhid       = 200 # the dimension of the feedforward network model in torch.nn.TransformerEncoder\n",
    "nlayers    = 2 # the number of torch.nn.TransformerEncoderLayer in torch.nn.TransformerEncoder\n",
    "nhead      = 2 # the number of heads in the multiheadattention models\n",
    "dropout    = 0.2 # the dropout value\n",
    "tknzerType = 'BPE' # type of tokenizing algorithm\n",
    "trainTokenizer = False # whether to train a new tokenizer or use one already trained\n",
    "download   = False # haven't implemented yet, whether to download\n",
    "nbrResults = 1000 # number of data samples to download\n",
    "epochs = 3 # The number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf83d8",
   "metadata": {},
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67302c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Using arxiv_1000.csv for training <<\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "filename = dl.arxiv_api( default.RAW_DATA_DIR, max_results=nbrResults )\n",
    "print(f'>> Using {filename} for training <<')\n",
    "fnameStrip = filename[:-4] # remove .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21983a34",
   "metadata": {},
   "source": [
    "### Format Dataset\n",
    "\n",
    "Uses a custom dataset class, which is an iterable and callable structure that returns a sample from our dataset. Within this custom dataset, can determine all preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "015ae225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "dataset = cd.ArxivDataset(default.RAW_DATA_DIR + os.sep + filename, maxLen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11976e8e",
   "metadata": {},
   "source": [
    "### Training Tokenizer\n",
    "\n",
    "Training of a custom tokenizer. Many options possible here, check the tokenizer training functions to try out various strategies. If he tokenizer for the dataset has already been trained, no need to run this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a132c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tkn.train_custom_tokenizer(tknzerType, dataset, fnameStrip\n",
    "                                            , default.TOK_DIR\n",
    "                                            , vocabSize\n",
    "                                            , **default.special_token_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3089723",
   "metadata": {},
   "source": [
    "### Loading Tokenizer and Splitting Datasets\n",
    "\n",
    "For some reason, torch tokenizers are not callable as trained. This is confusing, but c'est la vie! Instead, need to load it from file it was saved in using the PreTrainedTokenizerFast class (__call__) implemented in here. Once that's done, you can add this tokenizer as a transform to your dataset! Useful.\n",
    "\n",
    "We also split the dataset here into training, testing and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe434fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzrFile = default.TOK_DIR + os.sep + fnameStrip + '_' + tknzerType + '.json'\n",
    "\n",
    "# load PreTrainedTokenizerFast, for __call__. __call__ not implemented in\n",
    "# the base Tokenizer class... that sounds silly, but it is what it is\n",
    "tknzr = tkn.load_tokenizer(tknzrFile, **default.special_token_lst)\n",
    "\n",
    "if vocabSize is None: vocabSize = tknzr.vocab_size\n",
    "\n",
    "# set tknzr as the transform\n",
    "dataset.set_transform( tknzr )\n",
    "\n",
    "# separate dataset into train, test valid TODO : make into a function\n",
    "fracTrain, fracTest, fracVal = ( 0.7, 0.2, 0.1)\n",
    "trainTestVal = [ np.floor(fracTrain*len(dataset))\\\n",
    "                    , np.floor(fracTest*len(dataset))\\\n",
    "                    , len(dataset) - ( np.floor( fracTrain*len(dataset) ) +\n",
    "                    np.floor( fracTest*len(dataset) ) )\n",
    "                    ]\n",
    "\n",
    "trainDataset, testDataset, valDataset =\\\n",
    "        torch.utils.data.random_split(dataset, [int(x) for x in trainTestVal]\n",
    "                                , generator=torch.Generator().manual_seed(42) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bba4cb9",
   "metadata": {},
   "source": [
    "### Creating DataLoaders\n",
    "\n",
    "Training is done on batches, so we need a way to extract groupings of the data in the appropriate format for our transformer model.\n",
    "Note that for transformers which we are training, dataloaders outputs both src (x[:-1] and tgt ([1:]).\n",
    "The collation of batches for different transformer models we have vary. For HuggingFace it's ( maxLen x batch_size ) whereas I think that the Annotated Transformer has ( batch_size x maxLen )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c11048fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "# uses collate function to transform batch to correct dimensions\n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=bsz, shuffle=True\n",
    "                                        , collate_fn = PadCollate(dim=0,\n",
    "                                            maxLen=maxLen,\n",
    "                                            padValue=tknzr.get_vocab()[\"<pad>\"])\n",
    "                                        )\n",
    "valDataLoader = DataLoader(valDataset, batch_size=bsz, shuffle=True\n",
    "                                        , collate_fn = PadCollate(dim=0,\n",
    "                                            maxLen=maxLen,\n",
    "                                            padValue=tknzr.get_vocab()[\"<pad>\"])\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a86ea6d",
   "metadata": {},
   "source": [
    "### Training and Evaluation Functions\n",
    "\n",
    "Training and evaluation are pretty straightforward.\n",
    "\n",
    "***Note*** : I'm not too sure what ppl is... and why it is so large!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce6a1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function - same as in hugging face\n",
    "def train( model, maxLen, dataLoader, nbrSamples, optimizer_, scheduler_\n",
    "                , criterion_, device_ ):\n",
    "\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(maxLen).to(device_)\n",
    "    for i, batch in enumerate(dataLoader):\n",
    "        data = batch[0]; targets = batch[1]\n",
    "        optimizer_.zero_grad()\n",
    "        if data.size(0) != maxLen:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion_(output.view(-1, vocabSize), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer_.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, i, nbrSamples // maxLen,\n",
    "                            scheduler.get_last_lr()[0],\n",
    "                            elapsed * 1000 / log_interval,\n",
    "                            cur_loss, math.exp(cur_loss)))\n",
    "            # 200 / 2 batches... wrong, why?\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "# evaluation function outside of training - same as hugging face\n",
    "def evaluate(eval_model, maxLen, dataLoader, nbrSamples):\n",
    "\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(maxLen).to(device)\n",
    "    with torch.no_grad():\n",
    "        for batch in dataLoader:\n",
    "            data = batch[0]; targets = batch[1]\n",
    "            if data.size(0) != maxLen:\n",
    "                src_mask = model.generate_square_subsequent_mask(\n",
    "                                                    data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, vocabSize)\n",
    "            total_loss += len(data) * criterion(output_flat\n",
    "                                                , targets.reshape(-1)).item()\n",
    "    return total_loss / (nbrSamples - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3955854",
   "metadata": {},
   "source": [
    "### Selecting model\n",
    "\n",
    "Here we choose which model we shall use for training. For now, I've selected the black box Transformer from HuggingFace because the collate_fn I've written gives the correct input size force it... however this can easily be changed! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6de7d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer from huggingface\n",
    "model = TransformerModel(vocabSize, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "\n",
    "# transformer from illustrated transformer\n",
    "#model = make_gpt_model(vocabSize, vocabSize, nlayers, emsize, nhid, nhead, dropout)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780d86f",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75775f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/    2 batches | lr 3.68 | ms/batch 77.35 | loss  3.09 | ppl    22.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "251.84395729411733\n",
      "| end of epoch   1 | time: 18.72s | valid loss 251.84 | valid ppl 23683231137256318917393889330817365423104278032154741069164147614539982561908324822727113530917925435736064000.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/    2 batches | lr 3.49 | ms/batch 77.41 | loss  3.01 | ppl    20.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "252.6922374734975\n",
      "| end of epoch   2 | time: 18.79s | valid loss 252.69 | valid ppl 55315183129112076009966142380257140036137209539330504659969766362428610037529899011090335910894652491028234240.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/    2 batches | lr 3.32 | ms/batch 124.12 | loss  2.88 | ppl    17.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "258.13722317146534\n",
      "| end of epoch   3 | time: 34.02s | valid loss 258.14 | valid ppl 12810662719659911949499447322024126684613034088593738552133500998164512689310203050697166696997975987987274530816.00\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_model = None\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train( model, maxLen, trainDataLoader, len(trainDataset), optimizer\n",
    "                , scheduler, criterion, device)\n",
    "    val_loss = evaluate(model, maxLen, valDataLoader, len(valDataset))\n",
    "    print('-' * 89)\n",
    "    print(val_loss)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "                                     # Why is math.exp so large????\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    \n",
    "# save best model (two methods)\n",
    "# approach 1: save model (class) entirely (uses pickle)\n",
    "torch.save(model, default.MODEL_DIR + os.sep + f'{fnameStrip}_epoch{epochs}.pth')\n",
    "# approach 2: save model weights\n",
    "torch.save(model.state_dict(), default.MODEL_DIR + os.sep + f'{fnameStrip}_weights_epoch{epochs}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7ea970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
