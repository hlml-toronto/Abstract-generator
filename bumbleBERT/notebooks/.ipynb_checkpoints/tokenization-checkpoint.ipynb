{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d955a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv, os, sys\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from src import default\n",
    "from src.data import download as dl, data_preprocessing as dpp, tokenization as tkn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f3c79f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_examples(tokenizer, raw_tokenizer=True, title='default'):\n",
    "    \"\"\"\n",
    "    Example of a \"Raw tokenizer\":\n",
    "        tokenizer = Tokenizer.from_file(tpath)\n",
    "    Example of \"not Raw tokenizer\":\n",
    "        from transformers import PreTrainedTokenizerFast\n",
    "        fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=tpath)\n",
    "    \"\"\"\n",
    "    title_break = '\\n****************************************************************'\n",
    "    # example text\n",
    "    text0 = \"Hello, y'all! How are you ğŸ˜ ?\"\n",
    "    text1 = \"Here is some code spaghetti\"\n",
    "    text2 = \"configuration interaction (CI) wave functions is examined\"\n",
    "    text3 = \"By analogy with the pseudopotential approach for electron-ion interactions\"\n",
    "    text4 = \"Welcome to the ğŸ¤— Tokenizers library.\"\n",
    "    examples = [text0, text1, text2, text3, text4]\n",
    "\n",
    "    if raw_tokenizer:\n",
    "        print('Tokenizer examples (raw_tokenizer=True): %s%s' % (title, title_break))\n",
    "        for idx, text in enumerate(examples):\n",
    "            pre = '(Ex %d)' % idx\n",
    "            print('%s input: %s' % (pre, text))\n",
    "            output = tokenizer.encode(text)\n",
    "            print('%s output type & output.tokens: %s, %s' % (pre, type(output), output.tokens))\n",
    "            print('%s decode(output.ids): %s' % (pre, tokenizer.decode(output.ids)))\n",
    "\n",
    "            # \"use proper decoder\" https://huggingface.co/docs/tokenizers/python/latest/pipeline.html\n",
    "            print('%s decoder on output.ids: %s' % (pre, tokenizer.decode(output.ids)))\n",
    "            print()\n",
    "    else:\n",
    "        print('Tokenizer examples (raw_tokenizer=False): %s%s' % (title, title_break))\n",
    "        for idx, text in enumerate(examples):\n",
    "            pre = '(Ex %d)' % idx\n",
    "            print('%s input: %s' % (idx, text))\n",
    "            output = tokenizer.encode(text)\n",
    "            print('%s output type & output: %s, %s' % (pre, type(output), output))\n",
    "            print('%s decode w/ no cleanup: %s' %\n",
    "                  (pre, tokenizer.decode(output, clean_up_tokenization_spaces=False)))\n",
    "            print('%s decode w/ cleanup: %s' %\n",
    "                  (pre, tokenizer.decode(output, clean_up_tokenization_spaces=True)))\n",
    "            print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff0e67",
   "metadata": {},
   "source": [
    "## Example arxiv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "021b720f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Using arxiv_10.csv for training <<\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "filename = dl.arxiv_api( default.RAW_DATA_DIR )\n",
    "print(f'>> Using {filename} for training <<')\n",
    "\n",
    "# preprocessing\n",
    "proc_data = dpp.arxiv_preprocess_abstract(default.RAW_DATA_DIR\n",
    "                                , default.PROC_DATA_DIR, filename, True )\n",
    "\n",
    "# convert to list/iterator\n",
    "arxiv_iter = dpp.arxiv_abstract_iterator( proc_data )\n",
    "fname_strip_csv = filename[:-4]\n",
    "arxiv_tknzr = tkn.train_custom_tokenizer('BPE', arxiv_iter, fname_strip_csv, default.TOK_DIR\n",
    "                                , **default.special_token_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed1665",
   "metadata": {},
   "source": [
    "## Example wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a319ab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Using wikitext-103-raw for training <<\n"
     ]
    }
   ],
   "source": [
    "# download : already happened.\n",
    "file_dir = 'wikitext-103-raw'\n",
    "print(f'>> Using {file_dir} for training <<')\n",
    "\n",
    "# preprocessing : None for now\n",
    "\n",
    "# convert to list/iterator\n",
    "wiki_iter = dpp.wiki_iterator( file_dir )\n",
    "fname_strip = file_dir[:-4]\n",
    "\n",
    "wiki_tknzr = tkn.train_custom_tokenizer('BPE', wiki_iter, fname_strip, default.TOK_DIR\n",
    "                                        , **default.special_token_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1e830",
   "metadata": {},
   "source": [
    "## Comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e579bf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer examples (raw_tokenizer=True): trained arxviv BPE\n",
      "****************************************************************\n",
      "(Ex 0) input: Hello, y'all! How are you ğŸ˜ ?\n",
      "(Ex 0) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'Ä H', 'el', 'lo', ',', 'Ä ', 'y', '<unk>', 'all', '<unk>', 'Ä H', 'ow', 'Ä are', 'Ä ', 'y', 'o', 'u', 'Ä ', '<unk>', '<unk>', '<unk>', '<unk>', 'Ä ', '<unk>', '<\\\\s>']\n",
      "(Ex 0) decode(output.ids): Ä H el lo , Ä  y all Ä H ow Ä are Ä  y o u Ä  Ä \n",
      "(Ex 0) WordPiece decoder on output.ids: Ä H el lo, Ä  y all Ä H ow Ä are Ä  y o u Ä  Ä \n",
      "\n",
      "(Ex 1) input: Here is some code spaghetti\n",
      "(Ex 1) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'Ä H', 'ere', 'Ä is', 'Ä s', 'om', 'e', 'Ä co', 'de', 'Ä sp', 'a', 'gh', 'et', 't', 'i', '<\\\\s>']\n",
      "(Ex 1) decode(output.ids): Ä H ere Ä is Ä s om e Ä co de Ä sp a gh et t i\n",
      "(Ex 1) WordPiece decoder on output.ids: Ä H ere Ä is Ä s om e Ä co de Ä sp a gh et t i\n",
      "\n",
      "(Ex 2) input: configuration interaction (CI) wave functions is examined\n",
      "(Ex 2) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'Ä configuration', 'Ä interaction', 'Ä (', 'CI', ')', 'Ä wave', 'Ä functions', 'Ä is', 'Ä examined', '<\\\\s>']\n",
      "(Ex 2) decode(output.ids): Ä configuration Ä interaction Ä ( CI ) Ä wave Ä functions Ä is Ä examined\n",
      "(Ex 2) WordPiece decoder on output.ids: Ä configuration Ä interaction Ä ( CI ) Ä wave Ä functions Ä is Ä examined\n",
      "\n",
      "(Ex 3) input: By analogy with the pseudopotential approach for electron-ion interactions\n",
      "(Ex 3) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'Ä By', 'Ä analogy', 'Ä with', 'Ä the', 'Ä p', 'se', 'udop', 'otential', 'Ä approach', 'Ä for', 'Ä electron', '-', 'ion', 'Ä interactions', '<\\\\s>']\n",
      "(Ex 3) decode(output.ids): Ä By Ä analogy Ä with Ä the Ä p se udop otential Ä approach Ä for Ä electron - ion Ä interactions\n",
      "(Ex 3) WordPiece decoder on output.ids: Ä By Ä analogy Ä with Ä the Ä p se udop otential Ä approach Ä for Ä electron - ion Ä interactions\n",
      "\n",
      "(Ex 4) input: Welcome to the ğŸ¤— Tokenizers library.\n",
      "(Ex 4) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'Ä W', 'el', 'com', 'e', 'Ä to', 'Ä the', 'Ä ', '<unk>', '<unk>', '<unk>', '<unk>', 'Ä T', 'o', 'k', 'en', 'iz', 'er', 's', 'Ä l', 'ib', 'rary', '.', '<\\\\s>']\n",
      "(Ex 4) decode(output.ids): Ä W el com e Ä to Ä the Ä  Ä T o k en iz er s Ä l ib rary.\n",
      "(Ex 4) WordPiece decoder on output.ids: Ä W el com e Ä to Ä the Ä  Ä T o k en iz er s Ä l ib rary.\n",
      "\n",
      "Tokenizer examples (raw_tokenizer=True): trained wiki BPE\n",
      "****************************************************************\n",
      "(Ex 0) input: Hello, y'all! How are you ğŸ˜ ?\n",
      "(Ex 0) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'Ä ', 'H', 'e', 'l', 'l', 'o', '<unk>', 'Ä ', '<unk>', '<unk>', 'al', 'l', '<unk>', 'Ä ', 'H', 'o', 'w', 'Ä ', 'a', 'r', 'e', 'Ä ', '<unk>', 'o', 'u', 'Ä ', '<unk>', '<unk>', '<unk>', '<unk>', 'Ä ', '<unk>', '<\\\\s>']\n",
      "(Ex 0) decode(output.ids): Ä  H e l l o Ä  al l Ä  H o w Ä  a r e Ä  o u Ä  Ä \n",
      "(Ex 0) WordPiece decoder on output.ids: Ä  H e l l o Ä  al l Ä  H o w Ä  a r e Ä  o u Ä  Ä \n",
      "\n",
      "(Ex 1) input: Here is some code spaghetti\n",
      "(Ex 1) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'Ä ', 'H', 'e', 'r', 'e', 'Ä ', 'i', 's', 'Ä ', 's', 'o', 'me', 'Ä ', 'c', 'o', 'd', 'e', 'Ä ', 's', '<unk>', 'a', 'g', 'h', 'e', 't', 't', 'i', '<\\\\s>']\n",
      "(Ex 1) decode(output.ids): Ä  H e r e Ä  i s Ä  s o me Ä  c o d e Ä  s a g h e t t i\n",
      "(Ex 1) WordPiece decoder on output.ids: Ä  H e r e Ä  i s Ä  s o me Ä  c o d e Ä  s a g h e t t i\n",
      "\n",
      "(Ex 2) input: configuration interaction (CI) wave functions is examined\n",
      "(Ex 2) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'Ä ', 'c', 'o', 'n', '<unk>', 'i', 'g', 'u', 'ra', 't', 'i', 'o', 'n', 'Ä ', 'in', 'te', 'ract', 'i', 'o', 'n', 'Ä ', '<unk>', '<unk>', '<unk>', '<unk>', 'Ä ', 'w', 'a', 'v', 'e', 'Ä ', '<unk>', 'u', 'n', 'ct', 'i', 'o', 'n', 's', 'Ä ', 'i', 's', 'Ä ', 'e', 'x', 'a', 'm', 'in', 'e', 'd', '<\\\\s>']\n",
      "(Ex 2) decode(output.ids): Ä  c o n i g u ra t i o n Ä  in te ract i o n Ä  Ä  w a v e Ä  u n ct i o n s Ä  i s Ä  e x a m in e d\n",
      "(Ex 2) WordPiece decoder on output.ids: Ä  c o n i g u ra t i o n Ä  in te ract i o n Ä  Ä  w a v e Ä  u n ct i o n s Ä  i s Ä  e x a m in e d\n",
      "\n",
      "(Ex 3) input: By analogy with the pseudopotential approach for electron-ion interactions\n",
      "(Ex 3) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'Ä ', 'B', '<unk>', 'Ä ', 'a', 'n', 'al', 'o', 'g', '<unk>', 'Ä ', 'w', 'i', 't', 'h', 'Ä ', 't', 'h', 'e', 'Ä ', '<unk>', 's', 'e', 'u', 'd', 'o', '<unk>', 'ot', 'en', 't', 'i', 'al', 'Ä ', 'a', '<unk>', '<unk>', 'r', 'o', 'a', 'ch', 'Ä ', '<unk>', 'or', 'Ä ', 'e', 'l', 'e', 'ct', 'r', 'o', 'n', '-', 'i', 'o', 'n', 'Ä ', 'in', 'te', 'ract', 'i', 'o', 'n', 's', '<\\\\s>']\n",
      "(Ex 3) decode(output.ids): Ä  B Ä  a n al o g Ä  w i t h Ä  t h e Ä  s e u d o ot en t i al Ä  a r o a ch Ä  or Ä  e l e ct r o n - i o n Ä  in te ract i o n s\n",
      "(Ex 3) WordPiece decoder on output.ids: Ä  B Ä  a n al o g Ä  w i t h Ä  t h e Ä  s e u d o ot en t i al Ä  a r o a ch Ä  or Ä  e l e ct r o n - i o n Ä  in te ract i o n s\n",
      "\n",
      "(Ex 4) input: Welcome to the ğŸ¤— Tokenizers library.\n",
      "(Ex 4) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'Ä ', '<unk>', 'e', 'l', 'c', 'o', 'me', 'Ä ', 't', 'o', 'Ä ', 't', 'h', 'e', 'Ä ', '<unk>', '<unk>', '<unk>', '<unk>', 'Ä ', 'T', 'o', 'k', 'en', 'i', '<unk>', 'e', 'r', 's', 'Ä ', 'l', 'i', 'b', 'ra', 'r', '<unk>', '.', '<\\\\s>']\n",
      "(Ex 4) decode(output.ids): Ä  e l c o me Ä  t o Ä  t h e Ä  Ä  T o k en i e r s Ä  l i b ra r.\n",
      "(Ex 4) WordPiece decoder on output.ids: Ä  e l c o me Ä  t o Ä  t h e Ä  Ä  T o k en i e r s Ä  l i b ra r.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_examples(arxiv_tknzr, raw_tokenizer=True, title='trained arxviv BPE')\n",
    "tokenizer_examples(wiki_tknzr, raw_tokenizer=True, title='trained wiki BPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c222d89",
   "metadata": {},
   "source": [
    "## Fast and Bert Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "37fe6793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Electron', 'temperature', 'anisotropies', 'and', 'electron', 'beams', 'are', 'nonthermal', 'features', 'of', 'the', 'observed', 'nonequilibrium', 'electron', 'velocity', 'distributions', 'in', 'the', 'solar', 'wind.', 'In', 'collision-poor', 'plasmas', 'these', 'nonequilibrium', 'distributions', 'are', 'expected', 'to', 'be', 'regulated', 'by', 'kinetic', 'instabilities', 'through', 'wave-particle', 'interactions.', 'This', 'study', 'considers', 'electron', 'instabilities', 'driven', 'by', 'the', 'interplay', 'of', 'core', 'electron', 'temperature', 'anisotropies', 'and', 'the', 'electron', 'beam,', 'and', 'firstly', 'gives', 'a', 'comprehensive', 'analysis', 'of', 'instabilities', 'in', 'arbitrary', 'directions', 'to', 'the', 'background', 'magnetic', 'field.', 'It', 'clarifies', 'the', 'dominant', 'parameter', 'regime', '(e.g.,', 'parallel', 'core', 'electron', 'plasma', 'beta', '$\\\\beta_{\\\\mathrm{ec\\\\parallel}}$,', 'core', 'electron', 'temperature', 'anisotropy', '$A_{\\\\mathrm{ec}}\\\\equiv', 'T_{\\\\mathrm{ec\\\\perp}}/T_{\\\\mathrm{ec\\\\parallel}}$,', 'and', 'electron', 'beam', 'velocity', '$V_{\\\\mathrm{eb}}$)', 'for', 'each', 'kind', 'of', 'electron', 'instability', '(e.g.,', 'the', 'electron', 'beam-driven', 'electron', 'acoustic/magnetoacoustic', 'instability,', 'the', 'electron', 'beam-driven', 'whistler', 'instability,', 'the', 'electromagnetic', 'electron', 'cyclotron', 'instability,', 'the', 'electron', 'mirror', 'instability,', 'the', 'electron', 'firehose', 'instability,', 'and', 'the', 'ordinary-mode', 'instability).', 'It', 'finds', 'that', 'the', 'electron', 'beam', 'can', 'destabilize', 'electron', 'acoustic/magnetoacoustic', 'waves', 'in', 'the', 'low-$\\\\beta_{\\\\mathrm{ec\\\\parallel}}$', 'regime,', 'and', 'whistler', 'waves', 'in', 'the', 'medium-', 'and', 'large-$\\\\beta_{\\\\mathrm{ec\\\\parallel}}$', 'regime.', 'It', 'also', 'finds', 'that', 'a', 'new', 'oblique', 'fast-magnetosonic/whistler', 'instability', 'is', 'driven', 'by', 'the', 'electron', 'beam', 'with', '$V_{\\\\mathrm{eb}}\\\\gtrsim7V_{\\\\mathrm{A}}$', 'in', 'a', 'regime', 'where', '$\\\\beta_{\\\\mathrm{ec\\\\parallel}}\\\\sim0.1-2$', 'and', '$A_{\\\\mathrm{ec}}<1$.', 'Moreover,', 'this', 'study', 'presents', 'electromagnetic', 'responses', 'of', 'each', 'kind', 'of', 'electron', 'instability.', 'These', 'results', 'provide', 'a', 'comprehensive', 'overview', 'for', 'electron', 'instability', 'constraints', 'on', 'core', 'electron', 'temperature', 'anisotropies', 'and', 'electron', 'beams', 'in', 'the', 'solar', 'wind.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "\n",
    "# For now\n",
    "from tokenizers import BertWordPieceTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc553cb",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b608bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, normalizers, pre_tokenizers, decoders, processors\n",
    "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
    "from tokenizers.normalizers import NFD, NFKD, NFC, NFKC, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import ByteLevel, Whitespace, WhitespaceSplit, Punctuation, Metaspace,\\\n",
    "                                        CharDelimiterSplit\n",
    "from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordPieceTrainer, WordLevelTrainer\n",
    "from pathlib import Path\n",
    "\n",
    "#from transformers import PreTrainedTokenizerFast, PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d6ec172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_token = \"<s>\"\n",
    "pad_token = \"<pad>\"\n",
    "eos_token = \"</s>\"\n",
    "unk_token = \"<unk>\"\n",
    "mask_token = \"<mask>\"\n",
    "\n",
    "special_token_list = [bos_token, pad_token, eos_token, unk_token, mask_token]\n",
    "\n",
    "class BPE_token(object):\n",
    "    def __init__(self):\n",
    "        # instantiate\n",
    "        self.tokenizer = Tokenizer(BPE())\n",
    "        \n",
    "        # normalization\n",
    "        self.tokenizer.normalizer = Sequence([\n",
    "            NFKC()\n",
    "        ])\n",
    "        \n",
    "        # pre-tokenizer\n",
    "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
    "        \n",
    "        # decoder\n",
    "        self.tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "    def bpe_train(self, iterator):\n",
    "        trainer = BpeTrainer(vocab_size=50000, show_progress=True, inital_alphabet=ByteLevel.alphabet()\n",
    "                                             , special_tokens=special_token_list)\n",
    "        self.tokenizer.train_from_iterator(trainer=trainer, iterator=iterator) # paths is iterator\n",
    "\n",
    "    def save_tokenizer(self, location, prefix=None):\n",
    "        if not os.path.exists(location):\n",
    "            os.makedirs(location)\n",
    "        self.tokenizer.model.save(location, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8f40d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_tokenizer(token_model, data_iterator, token_dir, token_filename, vocab_size=30000, vocab=None\n",
    "                          , max_input_chars_per_word=None):\n",
    "    \"\"\"\n",
    "    Building a Tokenizer using HuggingFace library. The pipeline seems to be:\n",
    "    \n",
    "        - Model           : algorithm that tokenizes, it is a mandatory component. There are\n",
    "                            only 4 models implemented (BPE, Unigram, WordLevel, WordPiece)\n",
    "        - Normalizer      : some preprocessing that could happen before, but doesn't necessarily\n",
    "        - Pre-Tokenizer   : splitting the input according to some rules\n",
    "        - Post-Processing : needing to add some tokens/input after (mostly seems to be eos\n",
    "                            , bos tokens)\n",
    "        - Decoder         : certain previous pipeline steps need to be reversed for proper\n",
    "                            decoding\n",
    "        - Trainer         : The corresponding training algorithm for the model\n",
    "    \n",
    "    Note : Some pre-processing might need to happen beforehand in previous functions (might\n",
    "            be easier using pandas)\n",
    "    \n",
    "    Input\n",
    "        token_model              : algorithm to use for tokenization\n",
    "        data_iterator            : a python iterator that goes through the data to be used for \n",
    "                                    training\n",
    "        token_dir                : directory with tokenizers\n",
    "        vocab_size               : size of the vocabulary to use\n",
    "        token_filename           : filename of particular token we want to train. Will overwrite\n",
    "                                    previously save files.\n",
    "        vocab                    : models other than BPE can use non-mandatory vocab as input\n",
    "        max_input_chars_per_word : used for WordPiece\n",
    "        \n",
    "    Output\n",
    "        tokenizer                : huggingFace Tokenizer object, our fully trainer tokenizer\n",
    "            \n",
    "    \"\"\"\n",
    "    special_token_lst = [unk_token, bos_token, eos_token, pad_token, mask_token]\n",
    "    \n",
    "    normalizer_lst = [NFKC()]; pre_tokenizer_lst = [ByteLevel()]; decoder_lst = []\n",
    "    \n",
    "    bos_idx = special_token_list.index(bos_token); eos_idx = special_token_list.index(eos_token)\n",
    "    \n",
    "    if token_model == 'BPE':\n",
    "        model   = BPE(unk_token=unk_token) \n",
    "        Trainer = BpeTrainer\n",
    "    elif token_model == 'Unigram':\n",
    "        model   = Unigram(vocab=vocab) \n",
    "        Trainer = UnigramTrainer\n",
    "    elif token_model == 'WordLevel':\n",
    "        model   = WordLevel(unk_token=unk_token,vocab=vocab)\n",
    "        Trainer = WordLevelTrainer\n",
    "    elif token_model == 'WordPiece':\n",
    "        model   = WordPiece(unk_token=unk_token,vocab=vocab, max_input_chars_per_word=max_input_chars_per_word)\n",
    "        Trainer = WordPieceTrainer\n",
    "        decoder_lst.append( decoders.WordPiece())\n",
    "    else:\n",
    "        error_msg = f'Error: token_model ({token_model}) not an algorithm in [BPE, Unigram, WordLevel, WordPiece]'\n",
    "        raise SystemExit(error_msg)       \n",
    "    \n",
    "    # instantiation\n",
    "    tokenizer = Tokenizer(model)\n",
    "    \n",
    "    # trainer \n",
    "    trainer = Trainer(vocab_size=vocab_size, show_progress=True, special_tokens=special_tokens_lst)\n",
    "    \n",
    "    # normalizer\n",
    "    tokenizer.normalizer = normalizers.Sequence( normalizer_lst )\n",
    "    \n",
    "    # pre-tokenizer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Sequence( pre_tokenizer_lst )\n",
    "    \n",
    "    # post-processing\n",
    "    tokenizer.post_processor = processors.TemplateProcessing( single=bos_token+\" $A \"+eos_token\n",
    "                                                    #, pair=bos_token+\" $A \"+eos_token\" $B:1 \"+eos_token+\":1\"\n",
    "                                                    , special_tokens=[(bos_token, bos_idx),(eos_token, eos_idx)]\n",
    "                                                    )\n",
    "    \n",
    "    # decoder\n",
    "    if ByteLevel() in pre_tokenizer_lst: decoder_lst.append( decoders.ByteLevel() )\n",
    "    if Metaspace() in pre_tokenizer_lst: decoder_lst.append( decoders.Metaspace() ) \n",
    "    tokenizer.decoder = decoders.Sequence( decoder_lst )\n",
    "\n",
    "    tokenizer.train_from_iterator(trainer=trainer, iterator=data_iterator)\n",
    "    \n",
    "    if not os.path.exists( token_dir ):\n",
    "        os.makedirs( token_dir )\n",
    "    if os.path.exists( token_dir + os.sep + token_filename ):\n",
    "        print(f\"Warning : overwriting previously save tokenizer with same filename ( {token_filename} ).\")\n",
    "    tokenizer.save( token_dir + os.sep + token_filename )\n",
    "        \n",
    "    # TODO : Should I add PreTrained and Fast Tokenizer here? Seems like it might be appropriate.\n",
    "    transformer = False; fast = False\n",
    "    function_from_transformer_todo = None\n",
    "    if transformer:\n",
    "        raise SystemExit(\"HuggingFace transformers library not yet implemented here!\")\n",
    "        if fast: tokenizer = function_from_transformer_todo\n",
    "        else: tokenizer = function_from_transformer_todo\n",
    "                  \n",
    "    return tokenizer\n",
    "    \n",
    "    \n",
    "def load_custom_tokenizer(token_dir, token_filename, transformer=False, fast=False):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        token_dir      : directory with tokenizers saved\n",
    "        token_filename : trained tokenizer that we want to load\n",
    "        transformer    : (bool) whether to use HuggingFace transformers library implementation\n",
    "        fast           : (bool) whether to use HuggingFace transformers fast implementation\n",
    "    Output\n",
    "        tokenizer      : tokenizer from Tokenizer class to be passed to rest of algorithm\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer.from_file(token_dir + os.sep + token_filename)\n",
    "    \n",
    "    function_from_transformer_todo = None\n",
    "    if function_from_transformer != None:\n",
    "        if transformer:\n",
    "            raise SystemExit(\"HuggingFace transformers library not yet implemented here!\")\n",
    "            if fast: tokenizer = function_from_transformer_todo\n",
    "            else: tokenizer = function_from_transformer_todo\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b8335eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1536, 244, 594, 115, 94, 745, 201, 1529, 1470, 108, 93, 589, 832, 94, 830, 834, 101, 93, 523, 701, 11, 441, 1460, 10, 947, 1276, 516, 832, 834, 201, 1380, 143, 159, 1211, 203, 827, 564, 809, 383, 10, 735, 333, 11, 730, 822, 1351, 94, 564, 697, 203, 93, 820, 108, 419, 94, 244, 594, 115, 93, 94, 240, 9, 115, 1467, 1200, 88, 826, 1336, 108, 564, 101, 1447, 1401, 143, 93, 815, 1106, 343, 11, 442, 1539, 93, 1399, 1528, 342, 209, 56, 11, 58, 608, 781, 419, 94, 586, 1076, 438, 395, 215, 214, 78, 246, 49, 382, 765, 419, 94, 244, 1304, 188, 26, 215, 214, 78, 246, 569, 1305, 156, 215, 214, 78, 246, 49, 1162, 1188, 43, 215, 214, 78, 246, 49, 382, 765, 115, 94, 240, 830, 188, 45, 215, 214, 78, 623, 1257, 139, 542, 754, 108, 94, 224, 209, 56, 11, 58, 608, 93, 94, 240, 10, 622, 94, 670, 12, 784, 224, 9, 93, 94, 240, 10, 622, 749, 224, 9, 93, 806, 94, 1463, 224, 9, 93, 94, 1479, 224, 9, 93, 94, 1514, 224, 9, 115, 93, 1513, 10, 1425, 224, 839, 442, 772, 189, 93, 94, 240, 323, 1507, 94, 670, 12, 784, 758, 101, 93, 432, 607, 395, 215, 214, 78, 246, 49, 382, 340, 342, 9, 115, 749, 758, 101, 93, 1111, 10, 115, 550, 607, 395, 215, 214, 78, 246, 49, 382, 340, 342, 11, 442, 1218, 772, 189, 88, 1255, 1509, 1073, 10, 1530, 12, 978, 224, 145, 697, 203, 93, 94, 240, 181, 188, 45, 215, 214, 78, 623, 569, 1420, 20, 45, 215, 214, 78, 26, 340, 101, 88, 342, 748, 438, 395, 215, 214, 78, 246, 49, 382, 569, 497, 13, 11, 14, 10, 15, 5, 115, 188, 26, 215, 214, 78, 246, 1189, 14, 837, 1243, 9, 269, 822, 1274, 806, 1349, 108, 542, 754, 108, 94, 224, 11, 1171, 1390, 1516, 88, 826, 1320, 139, 94, 224, 1493, 249, 419, 94, 244, 594, 115, 94, 745, 101, 93, 523, 701, 11, 2]\n",
      "['<s>', 'Ä Electron', 'Ä temperature', 'Ä anisotropies', 'Ä and', 'Ä electron', 'Ä beams', 'Ä are', 'Ä nonthermal', 'Ä features', 'Ä of', 'Ä the', 'Ä observed', 'Ä nonequilibrium', 'Ä electron', 'Ä velocity', 'Ä distributions', 'Ä in', 'Ä the', 'Ä solar', 'Ä wind', '.', 'Ä In', 'Ä collision', '-', 'poor', 'Ä plasmas', 'Ä these', 'Ä nonequilibrium', 'Ä distributions', 'Ä are', 'Ä expected', 'Ä to', 'Ä be', 'Ä regulated', 'Ä by', 'Ä kinetic', 'Ä instabilities', 'Ä through', 'Ä wave', '-', 'particle', 'Ä interactions', '.', 'Ä This', 'Ä study', 'Ä considers', 'Ä electron', 'Ä instabilities', 'Ä driven', 'Ä by', 'Ä the', 'Ä interplay', 'Ä of', 'Ä core', 'Ä electron', 'Ä temperature', 'Ä anisotropies', 'Ä and', 'Ä the', 'Ä electron', 'Ä beam', ',', 'Ä and', 'Ä firstly', 'Ä gives', 'Ä a', 'Ä comprehensive', 'Ä analysis', 'Ä of', 'Ä instabilities', 'Ä in', 'Ä arbitrary', 'Ä directions', 'Ä to', 'Ä the', 'Ä background', 'Ä magnetic', 'Ä field', '.', 'Ä It', 'Ä clarifies', 'Ä the', 'Ä dominant', 'Ä parameter', 'Ä regime', 'Ä (', 'e', '.', 'g', '.,', 'Ä parallel', 'Ä core', 'Ä electron', 'Ä plasma', 'Ä beta', 'Ä $\\\\', 'beta', '_{\\\\', 'mathrm', '{', 'ec', '\\\\', 'parallel', '}}$,', 'Ä core', 'Ä electron', 'Ä temperature', 'Ä anisotropy', 'Ä $', 'A', '_{\\\\', 'mathrm', '{', 'ec', '}}\\\\', 'equiv', 'Ä T', '_{\\\\', 'mathrm', '{', 'ec', '\\\\', 'perp', '}}/', 'T', '_{\\\\', 'mathrm', '{', 'ec', '\\\\', 'parallel', '}}$,', 'Ä and', 'Ä electron', 'Ä beam', 'Ä velocity', 'Ä $', 'V', '_{\\\\', 'mathrm', '{', 'eb', '}}$)', 'Ä for', 'Ä each', 'Ä kind', 'Ä of', 'Ä electron', 'Ä instability', 'Ä (', 'e', '.', 'g', '.,', 'Ä the', 'Ä electron', 'Ä beam', '-', 'driven', 'Ä electron', 'Ä acoustic', '/', 'magnetoacoustic', 'Ä instability', ',', 'Ä the', 'Ä electron', 'Ä beam', '-', 'driven', 'Ä whistler', 'Ä instability', ',', 'Ä the', 'Ä electromagnetic', 'Ä electron', 'Ä cyclotron', 'Ä instability', ',', 'Ä the', 'Ä electron', 'Ä mirror', 'Ä instability', ',', 'Ä the', 'Ä electron', 'Ä firehose', 'Ä instability', ',', 'Ä and', 'Ä the', 'Ä ordinary', '-', 'mode', 'Ä instability', ').', 'Ä It', 'Ä finds', 'Ä that', 'Ä the', 'Ä electron', 'Ä beam', 'Ä can', 'Ä destabilize', 'Ä electron', 'Ä acoustic', '/', 'magnetoacoustic', 'Ä waves', 'Ä in', 'Ä the', 'Ä low', '-$\\\\', 'beta', '_{\\\\', 'mathrm', '{', 'ec', '\\\\', 'parallel', '}}$', 'Ä regime', ',', 'Ä and', 'Ä whistler', 'Ä waves', 'Ä in', 'Ä the', 'Ä medium', '-', 'Ä and', 'Ä large', '-$\\\\', 'beta', '_{\\\\', 'mathrm', '{', 'ec', '\\\\', 'parallel', '}}$', 'Ä regime', '.', 'Ä It', 'Ä also', 'Ä finds', 'Ä that', 'Ä a', 'Ä new', 'Ä oblique', 'Ä fast', '-', 'magnetosonic', '/', 'whistler', 'Ä instability', 'Ä is', 'Ä driven', 'Ä by', 'Ä the', 'Ä electron', 'Ä beam', 'Ä with', 'Ä $', 'V', '_{\\\\', 'mathrm', '{', 'eb', '}}\\\\', 'gtrsim', '7', 'V', '_{\\\\', 'mathrm', '{', 'A', '}}$', 'Ä in', 'Ä a', 'Ä regime', 'Ä where', 'Ä $\\\\', 'beta', '_{\\\\', 'mathrm', '{', 'ec', '\\\\', 'parallel', '}}\\\\', 'sim', '0', '.', '1', '-', '2', '$', 'Ä and', 'Ä $', 'A', '_{\\\\', 'mathrm', '{', 'ec', '}}<', '1', '$.', 'Ä Moreover', ',', 'Ä this', 'Ä study', 'Ä presents', 'Ä electromagnetic', 'Ä responses', 'Ä of', 'Ä each', 'Ä kind', 'Ä of', 'Ä electron', 'Ä instability', '.', 'Ä These', 'Ä results', 'Ä provide', 'Ä a', 'Ä comprehensive', 'Ä overview', 'Ä for', 'Ä electron', 'Ä instability', 'Ä constraints', 'Ä on', 'Ä core', 'Ä electron', 'Ä temperature', 'Ä anisotropies', 'Ä and', 'Ä electron', 'Ä beams', 'Ä in', 'Ä the', 'Ä solar', 'Ä wind', '.', '</s>']\n",
      " Electron temperature anisotropies and electron beams are nonthermal features of the observed nonequilibrium electron velocity distributions in the solar wind. In collision-poor plasmas these nonequilibrium distributions are expected to be regulated by kinetic instabilities through wave-particle interactions. This study considers electron instabilities driven by the interplay of core electron temperature anisotropies and the electron beam, and firstly gives a comprehensive analysis of instabilities in arbitrary directions to the background magnetic field. It clarifies the dominant parameter regime (e.g., parallel core electron plasma beta $\\beta_{\\mathrm{ec\\parallel}}$, core electron temperature anisotropy $A_{\\mathrm{ec}}\\equiv T_{\\mathrm{ec\\perp}}/T_{\\mathrm{ec\\parallel}}$, and electron beam velocity $V_{\\mathrm{eb}}$) for each kind of electron instability (e.g., the electron beam-driven electron acoustic/magnetoacoustic instability, the electron beam-driven whistler instability, the electromagnetic electron cyclotron instability, the electron mirror instability, the electron firehose instability, and the ordinary-mode instability). It finds that the electron beam can destabilize electron acoustic/magnetoacoustic waves in the low-$\\beta_{\\mathrm{ec\\parallel}}$ regime, and whistler waves in the medium- and large-$\\beta_{\\mathrm{ec\\parallel}}$ regime. It also finds that a new oblique fast-magnetosonic/whistler instability is driven by the electron beam with $V_{\\mathrm{eb}}\\gtrsim7V_{\\mathrm{A}}$ in a regime where $\\beta_{\\mathrm{ec\\parallel}}\\sim0.1-2$ and $A_{\\mathrm{ec}}<1$. Moreover, this study presents electromagnetic responses of each kind of electron instability. These results provide a comprehensive overview for electron instability constraints on core electron temperature anisotropies and electron beams in the solar wind.\n"
     ]
    }
   ],
   "source": [
    "# the folder 'text' contains all the files\n",
    "data_iter = iter(raw_data.summary.tolist())\n",
    "\n",
    "tokenizer = BPE_token()\n",
    "\n",
    "# train the tokenizer model\n",
    "tokenizer.bpe_train(data_iter)\n",
    "\n",
    "# saving the tokenized data in our specified folder \n",
    "save_path = 'tokenized_data'\n",
    "tokenizer.save_tokenizer(save_path)\n",
    "\n",
    "string_tokenized = tokenizer.tokenizer.encode(bos_token + ex_abstract + eos_token )\n",
    "decoded = tokenizer.tokenizer.decode(string_tokenized.ids)\n",
    "print(string_tokenized.ids)\n",
    "print(string_tokenized.tokens)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0486e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
