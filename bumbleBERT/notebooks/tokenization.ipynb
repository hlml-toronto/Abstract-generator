{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d955a184",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (data_preprocessing.py, line 65)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/jbrothschild/.local/share/virtualenvs/HLML-9szG14kG/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3343\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-72ced1461713>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from src.data import download as dl, data_preprocessing as dpp, tokenization as tkn\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"../src/data/data_preprocessing.py\"\u001b[0;36m, line \u001b[0;32m65\u001b[0m\n\u001b[0;31m    def\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv, os, sys\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from src import default\n",
    "from src.data import download as dl, data_preprocessing as dpp, tokenization as tkn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616043cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_examples(tokenizer, raw_tokenizer=True, title='default'):\n",
    "    \"\"\"\n",
    "    Example of a \"Raw tokenizer\":\n",
    "        tokenizer = Tokenizer.from_file(tpath)\n",
    "    Example of \"not Raw tokenizer\":\n",
    "        from transformers import PreTrainedTokenizerFast\n",
    "        fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=tpath)\n",
    "    \"\"\"\n",
    "    title_break = '\\n****************************************************************'\n",
    "    # example text\n",
    "    text0 = \"Hello, y'all! How are you üòÅ ?\"\n",
    "    text1 = \"Here is some code spaghetti\"\n",
    "    text2 = \"configuration interaction (CI) wave functions is examined\"\n",
    "    text3 = \"By analogy with the pseudopotential approach for electron-ion interactions\"\n",
    "    text4 = \"Welcome to the ü§ó Tokenizers library.\"\n",
    "    examples = [text0, text1, text2, text3, text4]\n",
    "\n",
    "    if raw_tokenizer:\n",
    "        print('Tokenizer examples (raw_tokenizer=True): %s%s' % (title, title_break))\n",
    "        for idx, text in enumerate(examples):\n",
    "            pre = '(Ex %d)' % idx\n",
    "            print('%s input: %s' % (pre, text))\n",
    "            output = tokenizer.encode(text)\n",
    "            print('%s output type & output.tokens: %s, %s' % (pre, type(output), output.tokens))\n",
    "            print('%s decode(output.ids): %s' % (pre, tokenizer.decode(output.ids)))\n",
    "\n",
    "            # \"use proper decoder\" https://huggingface.co/docs/tokenizers/python/latest/pipeline.html\n",
    "            print('%s decoder on output.ids: %s' % (pre, tokenizer.decode(output.ids)))\n",
    "            print()\n",
    "    else:\n",
    "        print('Tokenizer examples (raw_tokenizer=False): %s%s' % (title, title_break))\n",
    "        for idx, text in enumerate(examples):\n",
    "            pre = '(Ex %d)' % idx\n",
    "            print('%s input: %s' % (idx, text))\n",
    "            output = tokenizer.encode(text)\n",
    "            print('%s output type & output: %s, %s' % (pre, type(output), output))\n",
    "            print('%s decode w/ no cleanup: %s' %\n",
    "                  (pre, tokenizer.decode(output, clean_up_tokenization_spaces=False)))\n",
    "            print('%s decode w/ cleanup: %s' %\n",
    "                  (pre, tokenizer.decode(output, clean_up_tokenization_spaces=True)))\n",
    "            print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff0e67",
   "metadata": {},
   "source": [
    "## Example arxiv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "021b720f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c100914a8625>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# download data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marxiv_api\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRAW_DATA_DIR\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'>> Using {filename} for training <<'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dl' is not defined"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "filename = dl.arxiv_api( default.RAW_DATA_DIR )\n",
    "print(f'>> Using {filename} for training <<')\n",
    "\n",
    "# preprocessing\n",
    "proc_data = dpp.arxiv_preprocess_abstract(default.RAW_DATA_DIR\n",
    "                                , default.PROC_DATA_DIR, filename, True )\n",
    "\n",
    "# convert to list/iterator\n",
    "arxiv_iter = dpp.arxiv_abstract_iterator( proc_data )\n",
    "fname_strip_csv = filename[:-4]\n",
    "arxiv_tknzr = tkn.train_custom_tokenizer('BPE', arxiv_iter, fname_strip_csv, default.TOK_DIR\n",
    "                                , **default.special_token_lst, vocab_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed1665",
   "metadata": {},
   "source": [
    "## Example wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a319ab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Using wikitext-103-wrd for training <<\n",
      "['/home/jbrothschild/Documents/HLML/Abstract-generator/bumbleBERT/data/raw/wikitext-103-wrd/wiki.test.raw', '/home/jbrothschild/Documents/HLML/Abstract-generator/bumbleBERT/data/raw/wikitext-103-wrd/wiki.train.raw', '/home/jbrothschild/Documents/HLML/Abstract-generator/bumbleBERT/data/raw/wikitext-103-wrd/wiki.valid.raw']\n"
     ]
    }
   ],
   "source": [
    "# download : already happened.\n",
    "file_dir = 'wikitext-103-wrd'\n",
    "print(f'>> Using {file_dir} for training <<')\n",
    "\n",
    "# preprocessing : None for now\n",
    "\n",
    "# convert to list/iterator\n",
    "wiki_iter = dpp.wiki_iterator( file_dir )\n",
    "fname_strip = file_dir[:-4]\n",
    "\n",
    "\n",
    "wiki_tknzr = tkn.train_custom_tokenizer('BPE', wiki_iter, fname_strip, default.TOK_DIR\n",
    "                                        , **default.special_token_lst, vocab_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1e830",
   "metadata": {},
   "source": [
    "## Comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1699482c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer examples (raw_tokenizer=True): trained arxviv BPE\n",
      "****************************************************************\n",
      "(Ex 0) input: Hello, y'all! How are you üòÅ ?\n",
      "(Ex 0) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'ƒ†H', 'el', 'lo', 'ƒ†,', 'ƒ†', 'y', 'ƒ†', '<unk>', 'ƒ†all', 'ƒ†', '<unk>', 'ƒ†H', 'ow', 'ƒ†are', 'ƒ†', 'y', 'ou', 'ƒ†', '<unk>', '<unk>', '<unk>', '<unk>', 'ƒ†', '<unk>', '<\\\\s>']\n",
      "(Ex 0) decode(output.ids):  Hello , y  all  How are you  \n",
      "(Ex 0) decoder on output.ids:  Hello , y  all  How are you  \n",
      "\n",
      "(Ex 1) input: Here is some code spaghetti\n",
      "(Ex 1) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'ƒ†H', 'er', 'e', 'ƒ†is', 'ƒ†s', 'om', 'e', 'ƒ†c', 'od', 'e', 'ƒ†sp', 'ag', 'he', 't', 't', 'i', '<\\\\s>']\n",
      "(Ex 1) decode(output.ids):  Here is some code spaghetti\n",
      "(Ex 1) decoder on output.ids:  Here is some code spaghetti\n",
      "\n",
      "(Ex 2) input: configuration interaction (CI) wave functions is examined\n",
      "(Ex 2) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'ƒ†configuration', 'ƒ†interaction', 'ƒ†(', 'ƒ†CI', 'ƒ†)', 'ƒ†wave', 'ƒ†functions', 'ƒ†is', 'ƒ†examined', '<\\\\s>']\n",
      "(Ex 2) decode(output.ids):  configuration interaction ( CI ) wave functions is examined\n",
      "(Ex 2) decoder on output.ids:  configuration interaction ( CI ) wave functions is examined\n",
      "\n",
      "(Ex 3) input: By analogy with the pseudopotential approach for electron-ion interactions\n",
      "(Ex 3) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'ƒ†By', 'ƒ†analogy', 'ƒ†with', 'ƒ†the', 'ƒ†pseudopotential', 'ƒ†approach', 'ƒ†for', 'ƒ†electron', 'ƒ†-', 'ƒ†ion', 'ƒ†interactions', '<\\\\s>']\n",
      "(Ex 3) decode(output.ids):  By analogy with the pseudopotential approach for electron - ion interactions\n",
      "(Ex 3) decoder on output.ids:  By analogy with the pseudopotential approach for electron - ion interactions\n",
      "\n",
      "(Ex 4) input: Welcome to the ü§ó Tokenizers library.\n",
      "(Ex 4) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'ƒ†W', 'el', 'com', 'e', 'ƒ†to', 'ƒ†the', 'ƒ†', '<unk>', '<unk>', '<unk>', '<unk>', 'ƒ†T', 'o', 'k', 'en', 'iz', 'er', 's', 'ƒ†l', 'ib', 'rary', 'ƒ†.', '<\\\\s>']\n",
      "(Ex 4) decode(output.ids):  Welcome to the  Tokenizers library .\n",
      "(Ex 4) decoder on output.ids:  Welcome to the  Tokenizers library .\n",
      "\n",
      "Tokenizer examples (raw_tokenizer=True): trained wiki BPE\n",
      "****************************************************************\n",
      "(Ex 0) input: Hello, y'all! How are you üòÅ ?\n",
      "(Ex 0) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'ƒ†', 'H', 'e', 'l', 'l', 'o', 'ƒ†', '<unk>', 'ƒ†', '<unk>', 'ƒ†', '<unk>', 'ƒ†', 'al', 'l', 'ƒ†', '<unk>', 'ƒ†', 'H', 'o', 'w', 'ƒ†', 'a', 'r', 'e', 'ƒ†', '<unk>', 'o', 'u', 'ƒ†', '<unk>', '<unk>', '<unk>', '<unk>', 'ƒ†', '<unk>', '<\\\\s>']\n",
      "(Ex 0) decode(output.ids):  Hello    all  How are ou  \n",
      "(Ex 0) decoder on output.ids:  Hello    all  How are ou  \n",
      "\n",
      "(Ex 1) input: Here is some code spaghetti\n",
      "(Ex 1) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'ƒ†', 'H', 'e', 'r', 'e', 'ƒ†', 'i', 's', 'ƒ†', 's', 'o', 'me', 'ƒ†', 'c', 'o', 'd', 'e', 'ƒ†', 's', '<unk>', 'a', 'g', 'h', 'e', 't', 't', 'i', '<\\\\s>']\n",
      "(Ex 1) decode(output.ids):  Here is some code saghetti\n",
      "(Ex 1) decoder on output.ids:  Here is some code saghetti\n",
      "\n",
      "(Ex 2) input: configuration interaction (CI) wave functions is examined\n",
      "(Ex 2) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'ƒ†', 'c', 'o', 'n', '<unk>', 'i', 'g', 'u', 'ra', 't', 'i', 'o', 'n', 'ƒ†', 'in', 'te', 'ract', 'i', 'o', 'n', 'ƒ†', '<unk>', 'ƒ†', '<unk>', '<unk>', 'ƒ†', '<unk>', 'ƒ†w', 'a', 'v', 'e', 'ƒ†', '<unk>', 'u', 'n', 'ct', 'i', 'o', 'n', 's', 'ƒ†', 'i', 's', 'ƒ†', 'e', 'x', 'a', 'm', 'in', 'e', 'd', '<\\\\s>']\n",
      "(Ex 2) decode(output.ids):  coniguration interaction    wave unctions is examined\n",
      "(Ex 2) decoder on output.ids:  coniguration interaction    wave unctions is examined\n",
      "\n",
      "(Ex 3) input: By analogy with the pseudopotential approach for electron-ion interactions\n",
      "(Ex 3) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'ƒ†', 'B', '<unk>', 'ƒ†', 'a', 'n', 'al', 'o', 'g', '<unk>', 'ƒ†w', 'i', 't', 'h', 'ƒ†', 't', 'h', 'e', 'ƒ†', '<unk>', 's', 'e', 'u', 'd', 'o', '<unk>', 'ot', 'en', 't', 'i', 'al', 'ƒ†', 'a', '<unk>', '<unk>', 'r', 'o', 'a', 'ch', 'ƒ†', '<unk>', 'or', 'ƒ†', 'e', 'l', 'e', 'ct', 'r', 'o', 'n', 'ƒ†-', 'ƒ†', 'i', 'o', 'n', 'ƒ†', 'in', 'te', 'ract', 'i', 'o', 'n', 's', '<\\\\s>']\n",
      "(Ex 3) decode(output.ids):  B analog with the seudootential aroach or electron - ion interactions\n",
      "(Ex 3) decoder on output.ids:  B analog with the seudootential aroach or electron - ion interactions\n",
      "\n",
      "(Ex 4) input: Welcome to the ü§ó Tokenizers library.\n",
      "(Ex 4) output type & output.tokens: <class 'tokenizers.Encoding'>, ['<s>', 'ƒ†', '<unk>', 'e', 'l', 'c', 'o', 'me', 'ƒ†', 't', 'o', 'ƒ†', 't', 'h', 'e', 'ƒ†', '<unk>', '<unk>', '<unk>', '<unk>', 'ƒ†', 'T', 'o', 'k', 'en', 'i', '<unk>', 'e', 'r', 's', 'ƒ†', 'l', 'i', 'b', 'ra', 'r', '<unk>', 'ƒ†.', '<\\\\s>']\n",
      "(Ex 4) decode(output.ids):  elcome to the  Tokeniers librar .\n",
      "(Ex 4) decoder on output.ids:  elcome to the  Tokeniers librar .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_examples(arxiv_tknzr, raw_tokenizer=True, title='trained arxviv BPE')\n",
    "tokenizer_examples(wiki_tknzr, raw_tokenizer=True, title='trained wiki BPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a622b",
   "metadata": {},
   "source": [
    "## Fast and Bert Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37fe6793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "\n",
    "# For now\n",
    "from tokenizers import BertWordPieceTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc553cb",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b608bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, normalizers, pre_tokenizers, decoders, processors\n",
    "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
    "from tokenizers.normalizers import NFD, NFKD, NFC, NFKC, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import ByteLevel, Whitespace, WhitespaceSplit, Punctuation, Metaspace,\\\n",
    "                                        CharDelimiterSplit\n",
    "from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordPieceTrainer, WordLevelTrainer\n",
    "from pathlib import Path\n",
    "\n",
    "#from transformers import PreTrainedTokenizerFast, PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6ec172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_token = \"<s>\"\n",
    "pad_token = \"<pad>\"\n",
    "eos_token = \"</s>\"\n",
    "unk_token = \"<unk>\"\n",
    "mask_token = \"<mask>\"\n",
    "\n",
    "special_token_list = [bos_token, pad_token, eos_token, unk_token, mask_token]\n",
    "\n",
    "class BPE_token(object):\n",
    "    def __init__(self):\n",
    "        # instantiate\n",
    "        self.tokenizer = Tokenizer(BPE())\n",
    "        \n",
    "        # normalization\n",
    "        self.tokenizer.normalizer = Sequence([\n",
    "            NFKC()\n",
    "        ])\n",
    "        \n",
    "        # pre-tokenizer\n",
    "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
    "        \n",
    "        # decoder\n",
    "        self.tokenizer.decoder = ByteLevelDecoder()\n",
    "\n",
    "    def bpe_train(self, iterator):\n",
    "        trainer = BpeTrainer(vocab_size=50000, show_progress=True, inital_alphabet=ByteLevel.alphabet()\n",
    "                                             , special_tokens=special_token_list)\n",
    "        self.tokenizer.train_from_iterator(trainer=trainer, iterator=iterator) # paths is iterator\n",
    "\n",
    "    def save_tokenizer(self, location, prefix=None):\n",
    "        if not os.path.exists(location):\n",
    "            os.makedirs(location)\n",
    "        self.tokenizer.model.save(location, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f40d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_tokenizer(token_model, data_iterator, token_dir, token_filename, vocab_size=30000, vocab=None\n",
    "                          , max_input_chars_per_word=None):\n",
    "    \"\"\"\n",
    "    Building a Tokenizer using HuggingFace library. The pipeline seems to be:\n",
    "    \n",
    "        - Model           : algorithm that tokenizes, it is a mandatory component. There are\n",
    "                            only 4 models implemented (BPE, Unigram, WordLevel, WordPiece)\n",
    "        - Normalizer      : some preprocessing that could happen before, but doesn't necessarily\n",
    "        - Pre-Tokenizer   : splitting the input according to some rules\n",
    "        - Post-Processing : needing to add some tokens/input after (mostly seems to be eos\n",
    "                            , bos tokens)\n",
    "        - Decoder         : certain previous pipeline steps need to be reversed for proper\n",
    "                            decoding\n",
    "        - Trainer         : The corresponding training algorithm for the model\n",
    "    \n",
    "    Note : Some pre-processing might need to happen beforehand in previous functions (might\n",
    "            be easier using pandas)\n",
    "    \n",
    "    Input\n",
    "        token_model              : algorithm to use for tokenization\n",
    "        data_iterator            : a python iterator that goes through the data to be used for \n",
    "                                    training\n",
    "        token_dir                : directory with tokenizers\n",
    "        vocab_size               : size of the vocabulary to use\n",
    "        token_filename           : filename of particular token we want to train. Will overwrite\n",
    "                                    previously save files.\n",
    "        vocab                    : models other than BPE can use non-mandatory vocab as input\n",
    "        max_input_chars_per_word : used for WordPiece\n",
    "        \n",
    "    Output\n",
    "        tokenizer                : huggingFace Tokenizer object, our fully trainer tokenizer\n",
    "            \n",
    "    \"\"\"\n",
    "    special_token_lst = [unk_token, bos_token, eos_token, pad_token, mask_token]\n",
    "    \n",
    "    normalizer_lst = [NFKC()]; pre_tokenizer_lst = [ByteLevel()]; decoder_lst = []\n",
    "    \n",
    "    bos_idx = special_token_list.index(bos_token); eos_idx = special_token_list.index(eos_token)\n",
    "    \n",
    "    if token_model == 'BPE':\n",
    "        model   = BPE(unk_token=unk_token) \n",
    "        Trainer = BpeTrainer\n",
    "    elif token_model == 'Unigram':\n",
    "        model   = Unigram(vocab=vocab) \n",
    "        Trainer = UnigramTrainer\n",
    "    elif token_model == 'WordLevel':\n",
    "        model   = WordLevel(unk_token=unk_token,vocab=vocab)\n",
    "        Trainer = WordLevelTrainer\n",
    "    elif token_model == 'WordPiece':\n",
    "        model   = WordPiece(unk_token=unk_token,vocab=vocab, max_input_chars_per_word=max_input_chars_per_word)\n",
    "        Trainer = WordPieceTrainer\n",
    "        decoder_lst.append( decoders.WordPiece())\n",
    "    else:\n",
    "        error_msg = f'Error: token_model ({token_model}) not an algorithm in [BPE, Unigram, WordLevel, WordPiece]'\n",
    "        raise SystemExit(error_msg)       \n",
    "    \n",
    "    # instantiation\n",
    "    tokenizer = Tokenizer(model)\n",
    "    \n",
    "    # trainer \n",
    "    trainer = Trainer(vocab_size=vocab_size, show_progress=True, special_tokens=special_tokens_lst)\n",
    "    \n",
    "    # normalizer\n",
    "    tokenizer.normalizer = normalizers.Sequence( normalizer_lst )\n",
    "    \n",
    "    # pre-tokenizer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Sequence( pre_tokenizer_lst )\n",
    "    \n",
    "    # post-processing\n",
    "    tokenizer.post_processor = processors.TemplateProcessing( single=bos_token+\" $A \"+eos_token\n",
    "                                                    #, pair=bos_token+\" $A \"+eos_token\" $B:1 \"+eos_token+\":1\"\n",
    "                                                    , special_tokens=[(bos_token, bos_idx),(eos_token, eos_idx)]\n",
    "                                                    )\n",
    "    \n",
    "    # decoder\n",
    "    if ByteLevel() in pre_tokenizer_lst: decoder_lst.append( decoders.ByteLevel() )\n",
    "    if Metaspace() in pre_tokenizer_lst: decoder_lst.append( decoders.Metaspace() ) \n",
    "    tokenizer.decoder = decoders.Sequence( decoder_lst )\n",
    "\n",
    "    tokenizer.train_from_iterator(trainer=trainer, iterator=data_iterator)\n",
    "    \n",
    "    if not os.path.exists( token_dir ):\n",
    "        os.makedirs( token_dir )\n",
    "    if os.path.exists( token_dir + os.sep + token_filename ):\n",
    "        print(f\"Warning : overwriting previously save tokenizer with same filename ( {token_filename} ).\")\n",
    "    tokenizer.save( token_dir + os.sep + token_filename )\n",
    "        \n",
    "    # TODO : Should I add PreTrained and Fast Tokenizer here? Seems like it might be appropriate.\n",
    "    transformer = False; fast = False\n",
    "    function_from_transformer_todo = None\n",
    "    if transformer:\n",
    "        raise SystemExit(\"HuggingFace transformers library not yet implemented here!\")\n",
    "        if fast: tokenizer = function_from_transformer_todo\n",
    "        else: tokenizer = function_from_transformer_todo\n",
    "                  \n",
    "    return tokenizer\n",
    "    \n",
    "    \n",
    "def load_custom_tokenizer(token_dir, token_filename, transformer=False, fast=False):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        token_dir      : directory with tokenizers saved\n",
    "        token_filename : trained tokenizer that we want to load\n",
    "        transformer    : (bool) whether to use HuggingFace transformers library implementation\n",
    "        fast           : (bool) whether to use HuggingFace transformers fast implementation\n",
    "    Output\n",
    "        tokenizer      : tokenizer from Tokenizer class to be passed to rest of algorithm\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer.from_file(token_dir + os.sep + token_filename)\n",
    "    \n",
    "    function_from_transformer_todo = None\n",
    "    if function_from_transformer != None:\n",
    "        if transformer:\n",
    "            raise SystemExit(\"HuggingFace transformers library not yet implemented here!\")\n",
    "            if fast: tokenizer = function_from_transformer_todo\n",
    "            else: tokenizer = function_from_transformer_todo\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8335eac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8b5e223d6531>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# the folder 'text' contains all the files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBPE_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_data' is not defined"
     ]
    }
   ],
   "source": [
    "# the folder 'text' contains all the files\n",
    "data_iter = iter(raw_data.summary.tolist())\n",
    "\n",
    "tokenizer = BPE_token()\n",
    "\n",
    "# train the tokenizer model\n",
    "tokenizer.bpe_train(data_iter)\n",
    "\n",
    "# saving the tokenized data in our specified folder \n",
    "save_path = 'tokenized_data'\n",
    "tokenizer.save_tokenizer(save_path)\n",
    "\n",
    "string_tokenized = tokenizer.tokenizer.encode(bos_token + ex_abstract + eos_token )\n",
    "decoded = tokenizer.tokenizer.decode(string_tokenized.ids)\n",
    "print(string_tokenized.ids)\n",
    "print(string_tokenized.tokens)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0486e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
