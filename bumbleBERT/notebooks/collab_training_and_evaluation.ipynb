{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"collab_training_and_evaluation.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZmtW9scbJ9Y","executionInfo":{"status":"ok","timestamp":1626372266854,"user_tz":240,"elapsed":26809,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}},"outputId":"c225dd61-9cf7-4358-d195-1e2dfb4e2ff9"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","%cd /content/gdrive/MyDrive/Github/Abstract-generator/bumbleBERT/notebooks"],"id":"MZmtW9scbJ9Y","execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/Github/Abstract-generator/bumbleBERT/notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hokd2wsGcghW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626372276787,"user_tz":240,"elapsed":7584,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}},"outputId":"d52248ea-14d2-4ebc-e68d-a258464cee1e"},"source":["!pip install feedparser tokenizers transformers"],"id":"hokd2wsGcghW","execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting feedparser\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/15bf6781a861bbc5dd801d467f26448fb322bfedcd30f2e62b148d104dfb/feedparser-6.0.8-py3-none-any.whl (81kB)\n","\u001b[K     |████████████████████████████████| 81kB 5.8MB/s \n","\u001b[?25hCollecting tokenizers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 44.6MB/s \n","\u001b[?25hCollecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n","\u001b[K     |████████████████████████████████| 2.5MB 50.1MB/s \n","\u001b[?25hCollecting sgmllib3k\n","  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub==0.0.12\n","  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 35.4MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.5.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sgmllib3k\n","  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp37-none-any.whl size=6067 sha256=3f42f80c72448d79c902e559e5baadd9f5b4a3259c60c413f2496b7e0b3bc9b5\n","  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n","Successfully built sgmllib3k\n","Installing collected packages: sgmllib3k, feedparser, tokenizers, huggingface-hub, sacremoses, transformers\n","Successfully installed feedparser-6.0.8 huggingface-hub-0.0.12 sacremoses-0.0.45 sgmllib3k-1.0.0 tokenizers-0.10.3 transformers-4.8.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"04f6ae9e","executionInfo":{"status":"ok","timestamp":1626372286897,"user_tz":240,"elapsed":8189,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}}},"source":["import os, torch, time, math, sys, re, csv\n","import numpy as np\n","\n","sys.path.append('..' + os.sep )\n","from src import default\n","\n","from src.data import download as dl, data_preprocessing as dpp, tokenization as tkn\\\n","                        , custom_dataset as cd\n","from torch.utils.data import DataLoader\n","from src.model.transformer_hf import TransformerModel\n","from src.model.batching import CustomBatch\n","#from src.model.transformer import make_gpt_model # imports don't work\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"id":"04f6ae9e","execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"904013cd"},"source":["### Parameters"],"id":"904013cd"},{"cell_type":"code","metadata":{"id":"da73442b","executionInfo":{"status":"ok","timestamp":1626372366566,"user_tz":240,"elapsed":207,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}}},"source":["maxLen     = 100 # maximum sentence length\n","bsz        = 3 # batch size\n","vocabSize  = None # None if you want to let tokenizer do its thing\n","emsize     = 200 # embedding dimension\n","nhid       = 200 # the dimension of the feedforward network model in torch.nn.TransformerEncoder\n","nlayers    = 6 # the number of torch.nn.TransformerEncoderLayer in torch.nn.TransformerEncoder\n","nhead      = 8 # the number of heads in the multiheadattention models\n","dropout    = 0.2 # the dropout value\n","tknzerType = 'BPE' # type of tokenizing algorithm\n","trainTokenizer = True # whether to train a new tokenizer or use one already trained\n","download   = False # haven't implemented yet, whether to download\n","nbrResults = 10000 # number of data samples to download\n","epochs = 10 # The number of epochs"],"id":"da73442b","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"33bf83d8"},"source":["### Download Dataset"],"id":"33bf83d8"},{"cell_type":"code","metadata":{"id":"67302c43","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626372680691,"user_tz":240,"elapsed":313960,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}},"outputId":"b93cf0c2-5f5e-41e6-d1c0-acc5679dd869"},"source":["# download data\n","filename = dl.arxiv_api( default.RAW_DATA_DIR, max_results=nbrResults )\n","print(f'>> Using {filename} for training <<')\n","fnameStrip = filename[:-4] # remove .csv"],"id":"67302c43","execution_count":9,"outputs":[{"output_type":"stream","text":[">> Using arxiv_10000.csv for training <<\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"21983a34"},"source":["### Format Dataset\n","\n","Uses a custom dataset class, which is an iterable and callable structure that returns a sample from our dataset. Within this custom dataset, can determine all preprocessing."],"id":"21983a34"},{"cell_type":"code","metadata":{"id":"015ae225","executionInfo":{"status":"ok","timestamp":1626372681851,"user_tz":240,"elapsed":1176,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}}},"source":["# create dataset\n","dataset = cd.ArxivDataset(default.RAW_DATA_DIR + os.sep + filename, maxLen)"],"id":"015ae225","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"11976e8e"},"source":["### Training Tokenizer\n","\n","Training of a custom tokenizer. Many options possible here, check the tokenizer training functions to try out various strategies. If he tokenizer for the dataset has already been trained, no need to run this again."],"id":"11976e8e"},{"cell_type":"code","metadata":{"id":"d3a132c4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626372686569,"user_tz":240,"elapsed":4723,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}},"outputId":"435ba8a5-2c86-4494-fdde-23955927b7b1"},"source":["_ = tkn.train_custom_tokenizer(tknzerType, dataset, fnameStrip\n","                                            , default.TOK_DIR\n","                                            , vocabSize\n","                                            , **default.special_token_lst)"],"id":"d3a132c4","execution_count":11,"outputs":[{"output_type":"stream","text":["Warning : overwriting previously save tokenizer with                        same filename ( arxiv_10000 ).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f3089723"},"source":["### Loading Tokenizer and Splitting Datasets\n","\n","For some reason, torch tokenizers are not callable as trained. This is confusing, but c'est la vie! Instead, need to load it from file it was saved in using the PreTrainedTokenizerFast class (__call__) implemented in here. Once that's done, you can add this tokenizer as a transform to your dataset! Useful.\n","\n","We also split the dataset here into training, testing and validation datasets."],"id":"f3089723"},{"cell_type":"code","metadata":{"id":"fe434fb7","executionInfo":{"status":"ok","timestamp":1626372686571,"user_tz":240,"elapsed":17,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}}},"source":["tknzrFile = default.TOK_DIR + os.sep + fnameStrip + '_' + tknzerType + '.json'\n","\n","# load PreTrainedTokenizerFast, for __call__. __call__ not implemented in\n","# the base Tokenizer class... that sounds silly, but it is what it is\n","tknzr = tkn.load_tokenizer(tknzrFile, **default.special_token_lst)\n","\n","if vocabSize is None: vocabSize = tknzr.vocab_size\n","\n","# set tknzr as the transform\n","dataset.set_transform( tknzr )\n","\n","# separate dataset into train, test valid TODO : make into a function\n","fracTrain, fracTest, fracVal = ( 0.7, 0.2, 0.1)\n","trainTestVal = [ np.floor(fracTrain*len(dataset))\\\n","                    , np.floor(fracTest*len(dataset))\\\n","                    , len(dataset) - ( np.floor( fracTrain*len(dataset) ) +\n","                    np.floor( fracTest*len(dataset) ) )\n","                    ]\n","\n","trainDataset, testDataset, valDataset =\\\n","        torch.utils.data.random_split(dataset, [int(x) for x in trainTestVal]\n","                                , generator=torch.Generator().manual_seed(42) )"],"id":"fe434fb7","execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4bba4cb9"},"source":["### Creating DataLoaders\n","\n","Training is done on batches, so we need a way to extract groupings of the data in the appropriate format for our transformer model.\n","Note that for transformers which we are training, dataloaders outputs both src (x[:-1] and tgt ([1:]).\n","The collation of batches for different transformer models we have vary. For HuggingFace it's ( maxLen x batch_size ) whereas I think that the Annotated Transformer has ( batch_size x maxLen ).\n","\n","NOTE : Do not use the tokenizer before the training if you use num_workers>0!\n","FastTokenizer does not play nicely with forking if you use it before the forking of your data:\n","https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning"],"id":"4bba4cb9"},{"cell_type":"code","metadata":{"id":"c11048fc","executionInfo":{"status":"ok","timestamp":1626372686572,"user_tz":240,"elapsed":15,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}}},"source":["# create dataloaders\n","# uses collate function to transform batch to correct dimensions\n","def collate_wrapper(batch):\n","    return CustomBatch(batch, dim=0, maxLenModel=maxLen, padValue=tknzr.get_vocab()[\"<pad>\"])\n","\n","trainDataLoader = DataLoader(trainDataset, batch_size=bsz, shuffle=True\n","                                        , num_workers=2\n","                                        , collate_fn=collate_wrapper\n","                                        , pin_memory=True\n","                                        )\n","valDataLoader = DataLoader(valDataset, batch_size=bsz, shuffle=True\n","                                        , num_workers=2\n","                                        , collate_fn=collate_wrapper\n","                                        , pin_memory=True\n","                                        )"],"id":"c11048fc","execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5a86ea6d"},"source":["### Training and Evaluation Functions\n","\n","Training and evaluation are pretty straightforward.\n","\n","***Note*** : I'm not too sure what ppl is... and why it is so large!"],"id":"5a86ea6d"},{"cell_type":"code","metadata":{"id":"ce6a1915","executionInfo":{"status":"ok","timestamp":1626372686573,"user_tz":240,"elapsed":14,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}}},"source":["# training function - same as in hugging face\n","def train( model, maxLen, dataLoader, nbrSamples, optimizer_, scheduler_\n","                , criterion_, device_ ):\n","\n","    model.train() # Turn on the train mode\n","    total_loss = 0.\n","    start_time = time.time()\n","    src_mask = model.generate_square_subsequent_mask(maxLen).to(device_)\n","    for i, batch in enumerate(dataLoader):\n","        #print((batch.src).is_pinned())\n","        src = (batch.src).to(device); tgt = (batch.tgt).to(device)\n","\n","        optimizer_.zero_grad()\n","        if src.size(0) != maxLen:\n","            src_mask = model.generate_square_subsequent_mask(src.size(0)).to(device)\n","\n","        output = model(src, src_mask)\n","        loss = criterion_(output.view(-1, vocabSize), tgt.reshape(-1))\n","        loss.backward()\n","        torch.torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","        optimizer_.step()\n","\n","        total_loss += loss.item()\n","        log_interval = 200\n","        if i % log_interval == 0 and i > 0:\n","            cur_loss = total_loss / log_interval\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n","                  'lr {:02.2f} | ms/batch {:5.2f} | '\n","                  'loss {:5.2f} | ppl {:8.2f}'.format(\n","                    epoch, i, len(dataLoader),\n","                            scheduler.get_last_lr()[0],\n","                            elapsed * 1000 / log_interval,\n","                            cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","            start_time = time.time()\n","\n","\n","# evaluation function outside of training - same as hugging face\n","def evaluate(eval_model, maxLen, dataLoader, nbrSamples):\n","\n","    eval_model.eval() # Turn on the evaluation mode\n","    total_loss = 0.\n","    src_mask = model.generate_square_subsequent_mask(maxLen).to(device)\n","    with torch.no_grad():\n","        for batch in dataLoader:\n","            src = (batch.src).to(device); tgt = (batch.tgt).to(device)\n","            if src.size(0) != maxLen:\n","                src_mask = model.generate_square_subsequent_mask(\n","                                                    src.size(0)).to(device)\n","            output = eval_model(src, src_mask)\n","            output_flat = output.view(-1, vocabSize)\n","            total_loss += len(src) * criterion(output_flat\n","                                                , tgt.reshape(-1)).item()\n","    return total_loss / (nbrSamples - 1)"],"id":"ce6a1915","execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e3955854"},"source":["### Selecting model\n","\n","Here we choose which model we shall use for training. For now, I've selected the black box Transformer from HuggingFace because the collate_fn I've written gives the correct input size force it... however this can easily be changed! "],"id":"e3955854"},{"cell_type":"code","metadata":{"id":"6de7d252","executionInfo":{"status":"ok","timestamp":1626372698640,"user_tz":240,"elapsed":12077,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}}},"source":["# transformer from huggingface\n","model = TransformerModel(vocabSize, emsize, nhead, nhid, nlayers, dropout).to(device)\n","#model = TransformerModel(vocabSize, emsize, 10, nhid, nlayers, dropout).to(device)\n","\n","# transformer from illustrated transformer\n","#model = make_gpt_model(vocabSize, vocabSize, nlayers, emsize, nhid, nhead, dropout)\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","lr = 5.0 # learning rate\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"],"id":"6de7d252","execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9780d86f"},"source":["### Training\n","\n","Training loop!"],"id":"9780d86f"},{"cell_type":"code","metadata":{"id":"75775f0e","colab":{"base_uri":"https://localhost:8080/"},"outputId":"130dad1e-2847-4f44-8c46-636a17dea087"},"source":["best_val_loss = float(\"inf\")\n","best_model = None\n","for epoch in range(1, epochs + 1):\n","    epoch_start_time = time.time()\n","    train( model, maxLen, trainDataLoader, len(trainDataset), optimizer\n","                , scheduler, criterion, device)\n","    val_loss = evaluate(model, maxLen, valDataLoader, len(valDataset))\n","    print('-' * 89)\n","    print(val_loss)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                     val_loss, math.exp(val_loss)))\n","                                     # Why is math.exp so large????\n","    print('-' * 89)\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        best_model = model\n","\n","    scheduler.step()\n","\n","    \n","# save best model (two methods)\n","# approach 1: save model (class) entirely (uses pickle)\n","torch.save(model, default.MODEL_DIR + os.sep + f'{fnameStrip}_epoch{epochs}.pth')\n","# approach 2: save model weights\n","torch.save(model.state_dict(), default.MODEL_DIR + os.sep + f'{fnameStrip}_weights_epoch{epochs}.pth')"],"id":"75775f0e","execution_count":null,"outputs":[{"output_type":"stream","text":["| epoch   1 |   200/ 2334 batches | lr 5.00 | ms/batch 23.49 | loss  8.64 | ppl  5636.96\n","| epoch   1 |   400/ 2334 batches | lr 5.00 | ms/batch 22.80 | loss  7.58 | ppl  1948.89\n","| epoch   1 |   600/ 2334 batches | lr 5.00 | ms/batch 21.65 | loss  7.30 | ppl  1486.86\n","| epoch   1 |   800/ 2334 batches | lr 5.00 | ms/batch 21.42 | loss  7.12 | ppl  1240.09\n","| epoch   1 |  1000/ 2334 batches | lr 5.00 | ms/batch 21.51 | loss  7.10 | ppl  1209.74\n","| epoch   1 |  1200/ 2334 batches | lr 5.00 | ms/batch 21.55 | loss  7.05 | ppl  1150.03\n","| epoch   1 |  1400/ 2334 batches | lr 5.00 | ms/batch 21.99 | loss  6.96 | ppl  1055.34\n","| epoch   1 |  1600/ 2334 batches | lr 5.00 | ms/batch 22.92 | loss  6.98 | ppl  1079.15\n","| epoch   1 |  1800/ 2334 batches | lr 5.00 | ms/batch 22.68 | loss  6.97 | ppl  1066.80\n","| epoch   1 |  2000/ 2334 batches | lr 5.00 | ms/batch 23.34 | loss  6.97 | ppl  1067.60\n","| epoch   1 |  2200/ 2334 batches | lr 5.00 | ms/batch 22.71 | loss  6.89 | ppl   978.92\n","-----------------------------------------------------------------------------------------\n","226.1148284429067\n","| end of epoch   1 | time: 57.77s | valid loss 226.11 | valid ppl 158643495078663456091069088049250952348584769346576930112874336978547016724247039458433827470311424.00\n","-----------------------------------------------------------------------------------------\n","| epoch   2 |   200/ 2334 batches | lr 4.75 | ms/batch 24.67 | loss  6.85 | ppl   948.14\n","| epoch   2 |   400/ 2334 batches | lr 4.75 | ms/batch 23.35 | loss  6.84 | ppl   935.59\n","| epoch   2 |   600/ 2334 batches | lr 4.75 | ms/batch 23.22 | loss  6.86 | ppl   956.46\n","| epoch   2 |   800/ 2334 batches | lr 4.75 | ms/batch 22.24 | loss  6.86 | ppl   950.47\n","| epoch   2 |  1000/ 2334 batches | lr 4.75 | ms/batch 21.33 | loss  6.80 | ppl   899.48\n","| epoch   2 |  1200/ 2334 batches | lr 4.75 | ms/batch 21.43 | loss  6.77 | ppl   873.02\n","| epoch   2 |  1400/ 2334 batches | lr 4.75 | ms/batch 21.30 | loss  6.85 | ppl   939.98\n","| epoch   2 |  1600/ 2334 batches | lr 4.75 | ms/batch 21.10 | loss  6.77 | ppl   871.31\n","| epoch   2 |  1800/ 2334 batches | lr 4.75 | ms/batch 21.44 | loss  6.77 | ppl   873.81\n","| epoch   2 |  2000/ 2334 batches | lr 4.75 | ms/batch 21.28 | loss  6.76 | ppl   860.52\n","| epoch   2 |  2200/ 2334 batches | lr 4.75 | ms/batch 21.40 | loss  6.78 | ppl   881.02\n","-----------------------------------------------------------------------------------------\n","227.27788964262953\n","| end of epoch   2 | time: 56.39s | valid loss 227.28 | valid ppl 507613694908338836369943920416601210339570594204439131691058386991359398167902045554157376713523200.00\n","-----------------------------------------------------------------------------------------\n","| epoch   3 |   200/ 2334 batches | lr 4.51 | ms/batch 21.92 | loss  6.78 | ppl   883.38\n","| epoch   3 |   400/ 2334 batches | lr 4.51 | ms/batch 21.60 | loss  6.69 | ppl   804.71\n","| epoch   3 |   600/ 2334 batches | lr 4.51 | ms/batch 22.00 | loss  6.74 | ppl   847.74\n","| epoch   3 |   800/ 2334 batches | lr 4.51 | ms/batch 21.52 | loss  6.75 | ppl   855.82\n","| epoch   3 |  1000/ 2334 batches | lr 4.51 | ms/batch 21.85 | loss  6.75 | ppl   852.95\n","| epoch   3 |  1200/ 2334 batches | lr 4.51 | ms/batch 21.55 | loss  6.74 | ppl   849.10\n","| epoch   3 |  1400/ 2334 batches | lr 4.51 | ms/batch 21.56 | loss  6.75 | ppl   851.36\n","| epoch   3 |  1600/ 2334 batches | lr 4.51 | ms/batch 21.29 | loss  6.75 | ppl   857.48\n","| epoch   3 |  1800/ 2334 batches | lr 4.51 | ms/batch 21.89 | loss  6.71 | ppl   816.99\n","| epoch   3 |  2000/ 2334 batches | lr 4.51 | ms/batch 21.39 | loss  6.72 | ppl   832.24\n","| epoch   3 |  2200/ 2334 batches | lr 4.51 | ms/batch 21.40 | loss  6.75 | ppl   854.32\n","-----------------------------------------------------------------------------------------\n","224.3111654883987\n","| end of epoch   3 | time: 55.80s | valid loss 224.31 | valid ppl 26127713240037970258310062387635904564915728778737141770443698272299350717616445045907275372298240.00\n","-----------------------------------------------------------------------------------------\n","| epoch   4 |   200/ 2334 batches | lr 4.29 | ms/batch 22.54 | loss  6.75 | ppl   850.40\n","| epoch   4 |   400/ 2334 batches | lr 4.29 | ms/batch 21.51 | loss  6.72 | ppl   824.86\n","| epoch   4 |   600/ 2334 batches | lr 4.29 | ms/batch 21.28 | loss  6.66 | ppl   782.33\n","| epoch   4 |   800/ 2334 batches | lr 4.29 | ms/batch 21.31 | loss  6.68 | ppl   792.75\n","| epoch   4 |  1000/ 2334 batches | lr 4.29 | ms/batch 21.50 | loss  6.73 | ppl   834.12\n","| epoch   4 |  1200/ 2334 batches | lr 4.29 | ms/batch 21.34 | loss  6.70 | ppl   815.15\n","| epoch   4 |  1400/ 2334 batches | lr 4.29 | ms/batch 21.65 | loss  6.71 | ppl   818.41\n","| epoch   4 |  1600/ 2334 batches | lr 4.29 | ms/batch 21.22 | loss  6.70 | ppl   813.96\n","| epoch   4 |  1800/ 2334 batches | lr 4.29 | ms/batch 21.14 | loss  6.70 | ppl   815.29\n","| epoch   4 |  2000/ 2334 batches | lr 4.29 | ms/batch 21.49 | loss  6.71 | ppl   817.00\n","| epoch   4 |  2200/ 2334 batches | lr 4.29 | ms/batch 21.55 | loss  6.71 | ppl   817.87\n","-----------------------------------------------------------------------------------------\n","241.47630560075913\n","| end of epoch   4 | time: 55.23s | valid loss 241.48 | valid ppl 744435427284428481793476898347978735417422164593549698509889851180887880248171023348840852952474676363264.00\n","-----------------------------------------------------------------------------------------\n","| epoch   5 |   200/ 2334 batches | lr 4.07 | ms/batch 22.18 | loss  6.70 | ppl   815.16\n","| epoch   5 |   400/ 2334 batches | lr 4.07 | ms/batch 21.16 | loss  6.68 | ppl   798.18\n","| epoch   5 |   600/ 2334 batches | lr 4.07 | ms/batch 21.18 | loss  6.67 | ppl   788.32\n","| epoch   5 |   800/ 2334 batches | lr 4.07 | ms/batch 21.25 | loss  6.66 | ppl   782.37\n","| epoch   5 |  1000/ 2334 batches | lr 4.07 | ms/batch 21.22 | loss  6.67 | ppl   791.69\n","| epoch   5 |  1200/ 2334 batches | lr 4.07 | ms/batch 21.51 | loss  6.69 | ppl   806.09\n","| epoch   5 |  1400/ 2334 batches | lr 4.07 | ms/batch 21.58 | loss  6.66 | ppl   780.63\n","| epoch   5 |  1600/ 2334 batches | lr 4.07 | ms/batch 21.29 | loss  6.65 | ppl   776.25\n","| epoch   5 |  1800/ 2334 batches | lr 4.07 | ms/batch 21.35 | loss  6.66 | ppl   781.97\n","| epoch   5 |  2000/ 2334 batches | lr 4.07 | ms/batch 21.30 | loss  6.68 | ppl   792.40\n","| epoch   5 |  2200/ 2334 batches | lr 4.07 | ms/batch 21.22 | loss  6.64 | ppl   764.48\n","-----------------------------------------------------------------------------------------\n","219.43673102204147\n","| end of epoch   5 | time: 54.84s | valid loss 219.44 | valid ppl 199600401140087676633745236126534215918934649934334970677210263534742956199386190777191873118208.00\n","-----------------------------------------------------------------------------------------\n","| epoch   6 |   200/ 2334 batches | lr 3.87 | ms/batch 21.93 | loss  6.71 | ppl   820.77\n","| epoch   6 |   400/ 2334 batches | lr 3.87 | ms/batch 21.47 | loss  6.67 | ppl   791.63\n","| epoch   6 |   600/ 2334 batches | lr 3.87 | ms/batch 21.48 | loss  6.68 | ppl   792.63\n","| epoch   6 |   800/ 2334 batches | lr 3.87 | ms/batch 21.69 | loss  6.67 | ppl   787.47\n","| epoch   6 |  1000/ 2334 batches | lr 3.87 | ms/batch 21.61 | loss  6.65 | ppl   770.56\n","| epoch   6 |  1200/ 2334 batches | lr 3.87 | ms/batch 21.83 | loss  6.65 | ppl   770.46\n","| epoch   6 |  1400/ 2334 batches | lr 3.87 | ms/batch 21.65 | loss  6.64 | ppl   762.76\n","| epoch   6 |  1600/ 2334 batches | lr 3.87 | ms/batch 21.66 | loss  6.65 | ppl   772.48\n","| epoch   6 |  1800/ 2334 batches | lr 3.87 | ms/batch 21.51 | loss  6.64 | ppl   764.61\n","| epoch   6 |  2000/ 2334 batches | lr 3.87 | ms/batch 21.30 | loss  6.65 | ppl   771.18\n","| epoch   6 |  2200/ 2334 batches | lr 3.87 | ms/batch 21.53 | loss  6.70 | ppl   809.51\n","-----------------------------------------------------------------------------------------\n","220.56225094637713\n","| end of epoch   6 | time: 55.29s | valid loss 220.56 | valid ppl 615132257745676012729715508656788756618021794609160611263749916936291411716523934469706213228544.00\n","-----------------------------------------------------------------------------------------\n","| epoch   7 |   200/ 2334 batches | lr 3.68 | ms/batch 21.79 | loss  6.70 | ppl   808.56\n","| epoch   7 |   400/ 2334 batches | lr 3.68 | ms/batch 21.28 | loss  6.64 | ppl   765.20\n","| epoch   7 |   600/ 2334 batches | lr 3.68 | ms/batch 21.21 | loss  6.64 | ppl   766.05\n","| epoch   7 |   800/ 2334 batches | lr 3.68 | ms/batch 21.42 | loss  6.66 | ppl   778.53\n","| epoch   7 |  1000/ 2334 batches | lr 3.68 | ms/batch 21.69 | loss  6.65 | ppl   770.32\n","| epoch   7 |  1200/ 2334 batches | lr 3.68 | ms/batch 21.80 | loss  6.64 | ppl   764.71\n","| epoch   7 |  1400/ 2334 batches | lr 3.68 | ms/batch 21.64 | loss  6.64 | ppl   762.85\n","| epoch   7 |  1600/ 2334 batches | lr 3.68 | ms/batch 21.42 | loss  6.68 | ppl   793.84\n","| epoch   7 |  1800/ 2334 batches | lr 3.68 | ms/batch 21.50 | loss  6.64 | ppl   764.81\n","| epoch   7 |  2000/ 2334 batches | lr 3.68 | ms/batch 21.26 | loss  6.65 | ppl   774.43\n","| epoch   7 |  2200/ 2334 batches | lr 3.68 | ms/batch 21.74 | loss  6.63 | ppl   759.71\n","-----------------------------------------------------------------------------------------\n","218.6628160319171\n","| end of epoch   7 | time: 55.25s | valid loss 218.66 | valid ppl 92056487525234942050212892402600246287650847829643062226212423091835605351845899853971090046976.00\n","-----------------------------------------------------------------------------------------\n","| epoch   8 |   200/ 2334 batches | lr 3.49 | ms/batch 21.82 | loss  6.63 | ppl   759.96\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cc7ea970"},"source":[""],"id":"cc7ea970","execution_count":null,"outputs":[]}]}