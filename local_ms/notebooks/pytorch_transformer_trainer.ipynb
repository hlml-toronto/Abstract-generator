{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rTk268Ljh0sv"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_OsFOADh0sx"
   },
   "source": [
    "[link text](https://)\n",
    "Sequence-to-Sequence Modeling with nn.Transformer and TorchText\n",
    "===============================================================\n",
    "\n",
    "This is a tutorial on how to train a sequence-to-sequence model\n",
    "that uses the\n",
    "`nn.Transformer <https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer>`__ module.\n",
    "\n",
    "PyTorch 1.2 release includes a standard transformer module based on the\n",
    "paper `Attention is All You\n",
    "Need <https://arxiv.org/pdf/1706.03762.pdf>`__. The transformer model\n",
    "has been proved to be superior in quality for many sequence-to-sequence\n",
    "problems while being more parallelizable. The ``nn.Transformer`` module\n",
    "relies entirely on an attention mechanism (another module recently\n",
    "implemented as `nn.MultiheadAttention <https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention>`__) to draw global dependencies\n",
    "between input and output. The ``nn.Transformer`` module is now highly\n",
    "modularized such that a single component (like `nn.TransformerEncoder <https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder>`__\n",
    "in this tutorial) can be easily adapted/composed.\n",
    "\n",
    "![](https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/_static/img/transformer_architecture.jpg?raw=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DTaNWBsOoPP6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.datasets import WikiText2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.9 (default, Jan 26 2021, 15:33:00) \n",
      "[GCC 8.4.0]\n",
      "/media/homes/msmart/Development/VENVS/HLML2021\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)\n",
    "print(sys.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MS = os.path.dirname(os.path.abspath(''))\n",
    "sys.path.append(LOCAL_MS)\n",
    "\n",
    "from model import TransformerModel, load_model, train, evaluate\n",
    "from model_usage import gen_some_text\n",
    "from model_utils import gen_tokenizer_and_vocab, data_process, batchify, get_batch\n",
    "from settings import BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handler: Set Device (CUDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available(): True\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('torch.cuda.is_available():', torch.cuda.is_available())\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuLGmhAWoPP6"
   },
   "source": [
    "**Handler: Colab vs. Local Jupyter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "o-wGlSADoPP7",
    "outputId": "0119af7b-8211-4110-f90c-2d47c5487552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in colab: False\n",
      "Notebook output dir: output\n"
     ]
    }
   ],
   "source": [
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    NOTEBOOK_OUTDIR = 'MyDrive' + os.sep + 'output'\n",
    "    %cd '/content/drive'\n",
    "    TESTMODE = False\n",
    "    batch_size = 64 #32\n",
    "    eval_batch_size = 64 #32\n",
    "    epochs = 50  # The number of epochs\n",
    "else:\n",
    "    NOTEBOOK_OUTDIR = 'output'\n",
    "    TESTMODE = False       # True\n",
    "    if torch.cuda.is_available():\n",
    "        batch_size = 64       # 3, or 64/128 on server gpu\n",
    "        eval_batch_size = 64  # 3, or 64/128 on server gpu\n",
    "    else:\n",
    "        batch_size = 3        # 3 on cpu\n",
    "        eval_batch_size = 3   # 3 on cpu\n",
    "    epochs = 50  # The number of epochs\n",
    "\n",
    "os.makedirs(NOTEBOOK_OUTDIR, exist_ok=True)\n",
    "DATADIR = NOTEBOOK_OUTDIR + os.sep + '.data'\n",
    "\n",
    "print('Running in colab:', IN_COLAB)\n",
    "print('Notebook output dir:', NOTEBOOK_OUTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDoippnch0sy"
   },
   "source": [
    "Define the model\n",
    "----------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRsSsFnPh0sz"
   },
   "source": [
    "In this tutorial, we train ``nn.TransformerEncoder`` model on a\n",
    "language modeling task. The language modeling task is to assign a\n",
    "probability for the likelihood of a given word (or a sequence of words)\n",
    "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
    "layer first, followed by a positional encoding layer to account for the order\n",
    "of the word (see the next paragraph for more details). The\n",
    "``nn.TransformerEncoder`` consists of multiple layers of\n",
    "`nn.TransformerEncoderLayer <https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer>`__. Along with the input sequence, a square\n",
    "attention mask is required because the self-attention layers in\n",
    "``nn.TransformerEncoder`` are only allowed to attend the earlier positions in\n",
    "the sequence. For the language modeling task, any tokens on the future\n",
    "positions should be masked. To have the actual words, the output\n",
    "of ``nn.TransformerEncoder`` model is sent to the final Linear\n",
    "layer, which is followed by a log-Softmax function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p733KjQoh0s0"
   },
   "source": [
    "``PositionalEncoding`` module injects some information about the\n",
    "relative or absolute position of the tokens in the sequence. The\n",
    "positional encodings have the same dimension as the embeddings so that\n",
    "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
    "different frequencies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZO1xVjDAh0s1"
   },
   "source": [
    "Load and batch data\n",
    "-------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFOwEqVNh0s2"
   },
   "source": [
    "This tutorial uses ``torchtext`` to generate Wikitext-2 dataset. The\n",
    "vocab object is built based on the train dataset and is used to numericalize\n",
    "tokens into tensors. Starting from sequential data, the ``batchify()``\n",
    "function arranges the dataset into columns, trimming off any tokens remaining\n",
    "after the data has been divided into batches of size ``batch_size``.\n",
    "For instance, with the alphabet as the sequence (total length of 26)\n",
    "and a batch size of 4, we would divide the alphabet into 4 sequences of\n",
    "length 6:\n",
    "\n",
    "\\begin{align}\\begin{bmatrix}\n",
    "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
    "  \\end{bmatrix}\n",
    "  \\Rightarrow\n",
    "  \\begin{bmatrix}\n",
    "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
    "  \\end{bmatrix}\\end{align}\n",
    "\n",
    "These columns are treated as independent by the model, which means that\n",
    "the dependence of ``G`` and ``F`` can not be learned, but allows more\n",
    "efficient batch processing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ECcX7oEjh0s2"
   },
   "outputs": [],
   "source": [
    "tokenizer, vocab = gen_tokenizer_and_vocab()\n",
    "\n",
    "train_iter, val_iter, test_iter = WikiText2(root=DATADIR)\n",
    "train_data = data_process(train_iter, vocab, tokenizer)\n",
    "val_data = data_process(val_iter, vocab, tokenizer)\n",
    "test_data = data_process(test_iter, vocab, tokenizer)\n",
    "\n",
    "train_data = batchify(train_data, batch_size, device)\n",
    "val_data = batchify(val_data, eval_batch_size, device)\n",
    "test_data = batchify(test_data, eval_batch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvxDKBUdoPQD"
   },
   "source": [
    "**Code Testing block: shorten the size of the training data to make training loop quicker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "v9o2kHSToPQD",
    "outputId": "af1c7833-04e4-42c1-8a1f-d1768ef926b8"
   },
   "outputs": [],
   "source": [
    "data_reduce = 0.05\n",
    "\n",
    "if TESTMODE:\n",
    "    print('TRUNCATING DATA (testmode_ipynb=True)\\nShapes before:')\n",
    "    print(train_data.shape)\n",
    "    print(test_data.shape)\n",
    "    print(val_data.shape)\n",
    "    train_data = train_data[0:int(data_reduce * train_data.shape[0]), :]\n",
    "    test_data = test_data[0:int(data_reduce * test_data.shape[0]), :]\n",
    "    val_data = val_data[0:int(data_reduce * val_data.shape[0]), :]\n",
    "    print('Shapes after:')\n",
    "    print(train_data.shape)\n",
    "    print(test_data.shape)\n",
    "    print(val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXmvfbAfh0s3"
   },
   "source": [
    "``get_batch()`` function generates the input and target sequence for\n",
    "the transformer model. It subdivides the source data into chunks of\n",
    "length ``bptt``. For the language modeling task, the model needs the\n",
    "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
    "we’d get the following two Variables for ``i`` = 0:\n",
    "\n",
    "![](https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/_static/img/transformer_input_target.png?raw=1)\n",
    "\n",
    "\n",
    "It should be noted that the chunks are along dimension 0, consistent\n",
    "with the ``S`` dimension in the Transformer model. The batch dimension\n",
    "``N`` is along dimension 1.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "br5Wu4yPh0s4"
   },
   "outputs": [],
   "source": [
    "assert BPTT == 35  # TODO override/add flexibility later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKYt_D4wh0s4"
   },
   "source": [
    "Initiate an instance\n",
    "--------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_ejvBQ4h0s4"
   },
   "source": [
    "The model is set up with the hyperparameter below. The vocab size is\n",
    "equal to the length of the vocab object.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ElCNW7sWh0s4"
   },
   "outputs": [],
   "source": [
    "ntokens = len(vocab.stoi) # the size of vocabulary\n",
    "\n",
    "# Model 'A'\n",
    "\"\"\"emsize = 200           # embedding dimension\n",
    "nhid = 200                # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2               # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2                 # the number of heads in the multiheadattention models\n",
    "dropout = 0.05            # the dropout value\"\"\"\n",
    "\n",
    "# Model 'B'\n",
    "emsize = 512              # embedding dimension\n",
    "nhid = 2048               # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 12              # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8                 # the number of heads in the multiheadattention models\n",
    "dropout = 0.3             # the dropout value\n",
    "\n",
    "# Model 'C'\n",
    "\"\"\"\n",
    "emsize = 1024             # embedding dimension\n",
    "nhid = 2048               # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 12              # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 16                # the number of heads in the multiheadattention models\n",
    "dropout = 0.3             # the dropout value\"\"\"\n",
    "\n",
    "# Model 'D' - like GPT-1 - it doesn't fit\n",
    "# - \"BPE with 40,000 merged [53]\" using spaCy tokenizer\n",
    "# - 512 token samples (their BPTT)\n",
    "# - 100 epochs on minibatches of 64 continuous text streams (each of length BPTT)\n",
    "# - adam 2.5e-4 with specific LR schedule\n",
    "# - weight init of N(0, 0.02)\n",
    "\"\"\"\n",
    "emsize = 768              # embedding dimension\n",
    "nhid = 3072               # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 12              # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 12                # the number of heads in the multiheadattention models\n",
    "dropout = 0.1             # the dropout value\"\"\"\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmkcIDCvh0s5"
   },
   "source": [
    "Run the model\n",
    "-------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6N9258vph0s5"
   },
   "source": [
    "`CrossEntropyLoss <https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__\n",
    "is applied to track the loss and\n",
    "`SGD <https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD>`__\n",
    "implements stochastic gradient descent method as the optimizer. The initial\n",
    "learning rate is set to 5.0. `StepLR <https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR>`__ is\n",
    "applied to adjust the learn rate through epochs. During the\n",
    "training, we use\n",
    "`nn.utils.clip_grad_norm\\_ <https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_>`__\n",
    "function to scale all the gradient together to prevent exploding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hSWQPh_Kh0s5"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_str = 'SGD'\n",
    "schedule_str = 'scheduleA'\n",
    "\n",
    "valid_optimizer_str = ['SGD', 'adam', 'adamW']\n",
    "valid_schedule_str = ['constant', 'scheduleA']\n",
    "assert optimizer_str in valid_optimizer_str\n",
    "assert schedule_str in valid_schedule_str\n",
    "\n",
    "if schedule_str == 'constant':\n",
    "    gamma = 1.0\n",
    "else:\n",
    "    assert schedule_str == 'scheduleA'\n",
    "    gamma = 0.95    \n",
    "\n",
    "if optimizer_str == 'SGD':\n",
    "    lr = 0.5  # learning rate (originally: 5.0, 0.1, 0.5 (best))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=gamma)\n",
    "elif optimizer_str == 'adam':\n",
    "    lr = 2.5e-4  # default 1e-3, best 1e-4, try also 2.5e-4\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=gamma)\n",
    "    print('TODO: check usage of scheduler with adam -- none for now')\n",
    "else:\n",
    "    assert optimizer_str == 'adamW'\n",
    "    lr = 2.5e-4  # default 1e-3, best 1e-4\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=gamma)\n",
    "    print('TODO: check usage of scheduler with adamW -- none for now')\n",
    "    \n",
    "# TODO try two-stage LR like in GPT-1 -- increase 0 to eta_0 over k steps then cosine schedule after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally generate text each epoch to illustrate model state during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text_each_epoch = True\n",
    "# default behaviour generate sentence 1 greedily, then nseed sentences with beta=1\n",
    "if gen_text_each_epoch:\n",
    "    text_prompt = 'The dog ran'\n",
    "    decode_seeds = [0, 1, 2]\n",
    "    decode_betas = [1.0, 1.0, 1.0]\n",
    "    nongreedy_style = 'sample_full'\n",
    "    \n",
    "def gen_some_text_wrapper(generator_model, decode_style, decode_seed=0, decode_beta=1.0):\n",
    "    generated_text = gen_some_text(\n",
    "        generator_model, tokenizer, vocab, device,\n",
    "        text_prompt=text_prompt,\n",
    "        tokens_to_gen=25,\n",
    "        decode_style=decode_style,\n",
    "        decode_seed=decode_seed,\n",
    "        decode_beta=decode_beta,\n",
    "        sidestep_unk=False,\n",
    "        vis=False,\n",
    "        verbose=False)\n",
    "    return generated_text\n",
    "\n",
    "def decode_during_training(generator_model, epoch):\n",
    "    \n",
    "    \n",
    "    decode_start_time = time.time()\n",
    "    \n",
    "    print('Generated text at epoch %d: %s ...' % (epoch, text_prompt))\n",
    "    # First get greedy decoding\n",
    "    greedy_text = gen_some_text_wrapper(generator_model, 'greedy')\n",
    "    print(\"Greedy decoding:\\n\\t%s\" % (greedy_text))\n",
    "    # Now get several sampler decodings\n",
    "    for idx in range(len(decode_seeds)):\n",
    "        generated_text = gen_some_text_wrapper(generator_model, nongreedy_style, decode_seeds[idx], decode_betas[idx])\n",
    "        print(\"(%s, seed=%d, beta=%.2f):\\n\\t%s\" % (nongreedy_style, decode_seeds[idx], decode_betas[idx], generated_text))\n",
    "    print('-' * 89)\n",
    "    print('Epoch {:3d} | example generation time: {:5.2f}s'.format(epoch, (time.time() - decode_start_time)))\n",
    "    print('-' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrLNZrMIh0s6"
   },
   "source": [
    "Loop over epochs. Save the model if the validation loss is the best\n",
    "we've seen so far. Adjust the learning rate after each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiehSV-toPQG",
    "outputId": "854cb597-ea80-45ef-f809-17b9e9a64e3e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text at epoch 0: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t tracys tracys tracys tracys tracys tracys comprised tracys comprised comprised comprised comprised comprised comprised comprised comprised comprised comprised comprised comprised comprised comprised comprised comprised comprised\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t argentina trail gaa 1770s guarded cds franchomme carbon position 499 sad perished abdicated teddy banana depot earls drawbridge lowell alexander siddharama vegetarian dianna outweighed popularizing\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t squad gliding cousins shawnna stalingrad vāda cash bits grinder chasuble cabled douglas artistic hildreth kenyon 1161 devotee khenty polity rodríguez pullo olds pas think scarlet\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t salman stylish runways luxury agitation effigy m2 ransome 1955 tucker hydnum romantically marc 341 penguin accommodations iii mundane bacterial landmarks commemoration constance root syncretism chains\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   0 | example generation time:  1.49s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   1 |   200/  915 batches | lr 0.50 | ms/batch 175.87 | loss  7.90 | ppl  2710.61\n",
      "| epoch   1 |   400/  915 batches | lr 0.50 | ms/batch 175.19 | loss  7.34 | ppl  1533.98\n",
      "| epoch   1 |   600/  915 batches | lr 0.50 | ms/batch 175.53 | loss  7.14 | ppl  1262.65\n",
      "| epoch   1 |   800/  915 batches | lr 0.50 | ms/batch 175.93 | loss  6.96 | ppl  1053.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 165.95s | valid loss  6.51 | valid ppl   671.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 1: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t , the first , and the first <unk> , and the first , and the first <unk> , and the first , and the <unk>\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t to <unk> in hurlford were and franchomme . position henry sad on the <unk> banana to 2014 of the equal , vegetarian to wheeler operation\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the 2 of most least ' s alex , role douglas five to triana . the first bjørn the pullo nine work think scarlet\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t when the received to structural to best 0000 of the magazine romantically , and a species 6 of after this goods in the syncretism he\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   1 | example generation time:  1.18s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/  915 batches | lr 0.47 | ms/batch 177.02 | loss  6.75 | ppl   857.98\n",
      "| epoch   2 |   400/  915 batches | lr 0.47 | ms/batch 176.02 | loss  6.66 | ppl   779.03\n",
      "| epoch   2 |   600/  915 batches | lr 0.47 | ms/batch 176.21 | loss  6.57 | ppl   713.25\n",
      "| epoch   2 |   800/  915 batches | lr 0.47 | ms/batch 176.11 | loss  6.55 | ppl   699.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 166.54s | valid loss  6.23 | valid ppl   507.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 2: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t , and the first <unk> , and the first <unk> , and <unk> , and <unk> , and <unk> , and <unk> , and <unk>\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t to <unk> , with being <unk> , captured position henry sad , and <unk> , named 2014 of the equal , vegetarian to card operation\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the 2 @ , @ . @ 000 was february douglas five to be used ( . @ 000 km nine work ticks scarlet\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t when <unk> and luxury in this best 0000 of the magazine romantically , effects @-@ species 6 , after this goods in the syncretism opened\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   2 | example generation time:  1.38s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/  915 batches | lr 0.45 | ms/batch 176.86 | loss  6.48 | ppl   648.90\n",
      "| epoch   3 |   400/  915 batches | lr 0.45 | ms/batch 176.24 | loss  6.42 | ppl   612.66\n",
      "| epoch   3 |   600/  915 batches | lr 0.45 | ms/batch 176.17 | loss  6.35 | ppl   569.90\n",
      "| epoch   3 |   800/  915 batches | lr 0.45 | ms/batch 176.21 | loss  6.34 | ppl   567.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 166.56s | valid loss  6.04 | valid ppl   418.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 3: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t , and the first <unk> , and the <unk> , and the <unk> , and the <unk> , and <unk> , and <unk> , and\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t to <unk> , with being formed judge . position henry sad were moved in a named door of the equal , vegetarian to be operation\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the 2 mph , least the structure @-@ sleep , douglas five subject of the devotee . a rodríguez pullo nine work think scarlet\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t when <unk> received luxury in this best new 1955 to a haitian , effects a species 6 @ 000 by goods in the following chains\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   3 | example generation time:  1.36s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/  915 batches | lr 0.43 | ms/batch 177.31 | loss  6.30 | ppl   542.23\n",
      "| epoch   4 |   400/  915 batches | lr 0.43 | ms/batch 176.33 | loss  6.25 | ppl   519.37\n",
      "| epoch   4 |   600/  915 batches | lr 0.43 | ms/batch 176.08 | loss  6.19 | ppl   486.59\n",
      "| epoch   4 |   800/  915 batches | lr 0.43 | ms/batch 176.19 | loss  6.19 | ppl   489.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 166.66s | valid loss  5.91 | valid ppl   368.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 4: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t , and the <unk> , and the <unk> , and the <unk> , and <unk> , <unk> , <unk> , <unk> , <unk> , <unk>\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t to le marlow . = = franchomme is position henry society on june june april 15 2014 , new equal at vegetarian 1 june january\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the 2 mph , given its structure . as illinois douglas five years , the devotee was removed for the nine work think scarlet\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t when the death was in this best new 1955 to a haitian , to a species 6 @ , @ 000 in the following chains\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   4 | example generation time:  1.24s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/  915 batches | lr 0.41 | ms/batch 177.23 | loss  6.17 | ppl   476.45\n",
      "| epoch   5 |   400/  915 batches | lr 0.41 | ms/batch 176.34 | loss  6.13 | ppl   458.72\n",
      "| epoch   5 |   600/  915 batches | lr 0.41 | ms/batch 176.15 | loss  6.07 | ppl   431.87\n",
      "| epoch   5 |   800/  915 batches | lr 0.41 | ms/batch 176.41 | loss  6.08 | ppl   436.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 166.68s | valid loss  5.82 | valid ppl   337.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 5: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t , and the <unk> , and the <unk> , and the <unk> , and the <unk> , and the <unk> , <unk> , <unk> ,\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t to le marlow . = = franchomme is position henry society on june <unk> , named 2014 ( 0 @ . @ 1 ° operation\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the 2 mph , and report to alex , illinois douglas five years of the devotee . perfect regiment was nine work by scarlet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sample_full, seed=2, beta=1.00):\n",
      "\t when the second message in this best new 1955 to a haitian , to a species 6 @ , @ 000 in the following kingdom\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   5 | example generation time:  1.26s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/  915 batches | lr 0.39 | ms/batch 177.10 | loss  6.07 | ppl   431.02\n",
      "| epoch   6 |   400/  915 batches | lr 0.39 | ms/batch 176.38 | loss  6.03 | ppl   415.99\n",
      "| epoch   6 |   600/  915 batches | lr 0.39 | ms/batch 176.40 | loss  5.97 | ppl   392.80\n",
      "| epoch   6 |   800/  915 batches | lr 0.39 | ms/batch 176.29 | loss  5.99 | ppl   398.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 166.72s | valid loss  5.75 | valid ppl   313.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 6: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t , and the <unk> , and the <unk> , and the <unk> , and the <unk> of the <unk> , and the <unk> , and\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t to le sources . = = franchomme = = henry society = = in april depot , the first equal at vegetarian 1 years was\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the 2 mph ( 2 @ . @ 1 in ) five years of the devotee . perfect regiment was nine work by scarlet\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t when the second message in this kingdom , 1955 to a haitian , to a species 6 @ , @ 000 in the following kingdom\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   6 | example generation time:  1.21s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/  915 batches | lr 0.37 | ms/batch 177.02 | loss  5.98 | ppl   396.12\n",
      "| epoch   7 |   400/  915 batches | lr 0.37 | ms/batch 176.37 | loss  5.95 | ppl   383.21\n",
      "| epoch   7 |   600/  915 batches | lr 0.37 | ms/batch 176.36 | loss  5.89 | ppl   363.14\n",
      "| epoch   7 |   800/  915 batches | lr 0.37 | ms/batch 176.43 | loss  5.91 | ppl   369.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 166.68s | valid loss  5.68 | valid ppl   293.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 7: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t to the <unk> , and the <unk> of the <unk> , and the <unk> of the <unk> , and the <unk> of the <unk> ,\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t to le sources . = = franchomme = = henry society = = in april russia , the first equal at vegetarian 1 years was\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the 2 mph ( 3 @ . @ 9 in ) five years of the devotee . perfect regiment was nine and scored scarlet\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t for the second message in this kingdom , 1955 to a haitian , to a species 6 @ , @ 000 in the sunday title\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   7 | example generation time:  1.15s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/  915 batches | lr 0.35 | ms/batch 177.28 | loss  5.91 | ppl   368.54\n",
      "| epoch   8 |   400/  915 batches | lr 0.35 | ms/batch 176.39 | loss  5.88 | ppl   357.90\n",
      "| epoch   8 |   600/  915 batches | lr 0.35 | ms/batch 176.35 | loss  5.83 | ppl   339.06\n",
      "| epoch   8 |   800/  915 batches | lr 0.35 | ms/batch 176.43 | loss  5.85 | ppl   346.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 166.74s | valid loss  5.63 | valid ppl   279.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 8: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t to the <unk> , and the <unk> of the <unk> , and the <unk> of the <unk> , and the <unk> of the <unk> ,\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t to le sources . = = franchomme = = henry society = = in april russia , the youngest ' s vegetarian widowed of operation\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the 2 mph ( 3 @ . @ 9 in ) five years of the devotee . a total conspiracy nine work think scarlet\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t for <unk> and luxury in this tv new zealand to a haitian , to a species iii , after this goods in the sunday types\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   8 | example generation time:  1.20s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/  915 batches | lr 0.33 | ms/batch 177.20 | loss  5.85 | ppl   347.46\n",
      "| epoch   9 |   400/  915 batches | lr 0.33 | ms/batch 176.31 | loss  5.82 | ppl   337.03\n",
      "| epoch   9 |   600/  915 batches | lr 0.33 | ms/batch 176.20 | loss  5.77 | ppl   319.53\n",
      "| epoch   9 |   800/  915 batches | lr 0.33 | ms/batch 176.22 | loss  5.79 | ppl   327.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 166.64s | valid loss  5.59 | valid ppl   268.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 9: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t to the <unk> , and the <unk> of the <unk> , and the <unk> of the <unk> , and the <unk> of the <unk> ,\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t to le sources . = = franchomme = = henry society = = in april russia , the youngest ' s vegetarian colours of operation\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the 2 mph ( 3 @ . @ 9 in ) five years of the devotee . perfect regiment was nine and scored 38\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t for <unk> and luxury in this tv new zealand to a haitian , and a species iii , after this goods in the sunday types\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   9 | example generation time:  1.29s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/  915 batches | lr 0.32 | ms/batch 177.15 | loss  5.80 | ppl   329.57\n",
      "| epoch  10 |   400/  915 batches | lr 0.32 | ms/batch 176.15 | loss  5.77 | ppl   318.96\n",
      "| epoch  10 |   600/  915 batches | lr 0.32 | ms/batch 176.28 | loss  5.71 | ppl   302.09\n",
      "| epoch  10 |   800/  915 batches | lr 0.32 | ms/batch 176.24 | loss  5.74 | ppl   311.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 166.65s | valid loss  5.55 | valid ppl   257.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 10: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t to the <unk> , and the <unk> of the <unk> , and the <unk> of the <unk> , the <unk> of the <unk> , and\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t to le sources . = = franchomme = = henry society = = in april russia , the youngest ' s vegetarian colours of operation\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the relation to be given to some alex , in douglas five years later . the first single regiment was nine and scored the\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t for the second message in france and new zealand times was romantically completely to a signal rate of modern landmarks goods in london . he\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch  10 | example generation time:  1.31s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/  915 batches | lr 0.30 | ms/batch 177.13 | loss  5.75 | ppl   313.85\n",
      "| epoch  11 |   400/  915 batches | lr 0.30 | ms/batch 176.19 | loss  5.72 | ppl   303.97\n",
      "| epoch  11 |   600/  915 batches | lr 0.30 | ms/batch 176.30 | loss  5.67 | ppl   289.09\n",
      "| epoch  11 |   800/  915 batches | lr 0.30 | ms/batch 176.33 | loss  5.70 | ppl   297.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 166.65s | valid loss  5.52 | valid ppl   248.99\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text at epoch 11: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t to the <unk> , and the <unk> of the <unk> , the <unk> <unk> , and the <unk> of the <unk> , and the <unk>\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t to le challenge in the <unk> judge . position henry sad staff and <unk> banana named environmental <unk> , while the vegetarian colours of operation\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the relation to be given to some alex , in douglas five years later . the first single regiment was nine and scored the\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t when the second message in france and new zealand times was romantically completely to a signal rate of modern landmarks goods in london . he\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch  11 | example generation time:  1.28s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   200/  915 batches | lr 0.28 | ms/batch 177.33 | loss  5.71 | ppl   300.64\n",
      "| epoch  12 |   400/  915 batches | lr 0.28 | ms/batch 176.26 | loss  5.67 | ppl   291.32\n",
      "| epoch  12 |   600/  915 batches | lr 0.28 | ms/batch 176.48 | loss  5.62 | ppl   276.91\n",
      "| epoch  12 |   800/  915 batches | lr 0.28 | ms/batch 176.56 | loss  5.65 | ppl   285.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 166.78s | valid loss  5.49 | valid ppl   241.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 12: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t to the <unk> , and the <unk> of the <unk> , the <unk> of the <unk> , and the <unk> of the <unk> , the\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t to le downloads . = = franchomme = = henry society = = beginning after his children gave them , the un colours of operation\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the relation to be given to some alex sleep . throughout five years , the devotee was removed for the nine work of the\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t when the 8th message in france and new zealand times was romantically completely to a signal rate of modern landmarks goods in london . he\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch  12 | example generation time:  1.25s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/  915 batches | lr 0.27 | ms/batch 177.37 | loss  5.67 | ppl   289.19\n",
      "| epoch  13 |   400/  915 batches | lr 0.27 | ms/batch 176.77 | loss  5.63 | ppl   280.02\n",
      "| epoch  13 |   600/  915 batches | lr 0.27 | ms/batch 176.79 | loss  5.58 | ppl   265.59\n",
      "| epoch  13 |   800/  915 batches | lr 0.27 | ms/batch 176.55 | loss  5.62 | ppl   274.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 167.01s | valid loss  5.46 | valid ppl   234.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 13: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t to the <unk> of the <unk> , and the <unk> of the <unk> , the <unk> <unk> , <unk> , <unk> , <unk> , <unk>\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t to le downloads . = = franchomme = = henry society were moved in a depot in the new zealand , with widowed of operation\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the relation to be given to some alex sleep . most five years later . the first single regiment had nine work over the\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t when the 8th message in france and new zealand times was romantically completely to a signal rate of modern landmarks goods in london . he\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch  13 | example generation time:  1.29s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/  915 batches | lr 0.26 | ms/batch 177.54 | loss  5.63 | ppl   278.77\n",
      "| epoch  14 |   400/  915 batches | lr 0.26 | ms/batch 176.75 | loss  5.60 | ppl   270.21\n",
      "| epoch  14 |   600/  915 batches | lr 0.26 | ms/batch 176.64 | loss  5.55 | ppl   256.49\n",
      "| epoch  14 |   800/  915 batches | lr 0.26 | ms/batch 176.94 | loss  5.58 | ppl   265.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 167.07s | valid loss  5.43 | valid ppl   228.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 14: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t to the <unk> of the <unk> , and the <unk> of the <unk> , the <unk> of the <unk> , and the <unk> of the\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t to le sources . = = franchomme = = henry society = = beginning after his children gave his equal at vegetarian 1 years ,\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the relation to be given to some alex sleep . douglas were subject to take devotee . perfect james pullo , and think he\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t when the helium was in france and new zealand times was romantically completely to a signal rate of modern landmarks goods in london . he\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch  14 | example generation time:  1.20s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/  915 batches | lr 0.24 | ms/batch 177.61 | loss  5.60 | ppl   269.96\n",
      "| epoch  15 |   400/  915 batches | lr 0.24 | ms/batch 176.73 | loss  5.57 | ppl   261.22\n",
      "| epoch  15 |   600/  915 batches | lr 0.24 | ms/batch 176.98 | loss  5.51 | ppl   248.28\n",
      "| epoch  15 |   800/  915 batches | lr 0.24 | ms/batch 176.65 | loss  5.55 | ppl   257.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 167.08s | valid loss  5.41 | valid ppl   222.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 15: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t to the <unk> of the <unk> , and the <unk> of the <unk> , the <unk> of the <unk> , and the <unk> of the\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t to le <unk> . = = franchomme = = henry society were moved in a depot in the new zealand , with widowed of operation\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the relation to be given to some alex sleep . douglas were subject to take devotee . perfect james pullo , and think he\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t when the 8th typhoon in france and new zealand to the haitian century . a signal rate of modern landmarks goods in the sunday century\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch  15 | example generation time:  1.31s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/  915 batches | lr 0.23 | ms/batch 177.39 | loss  5.57 | ppl   261.97\n",
      "| epoch  16 |   400/  915 batches | lr 0.23 | ms/batch 176.53 | loss  5.54 | ppl   253.62\n",
      "| epoch  16 |   600/  915 batches | lr 0.23 | ms/batch 176.72 | loss  5.49 | ppl   241.07\n",
      "| epoch  16 |   800/  915 batches | lr 0.23 | ms/batch 176.47 | loss  5.52 | ppl   250.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 166.92s | valid loss  5.39 | valid ppl   218.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 16: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t to the <unk> of the <unk> , and the <unk> of the <unk> <unk> , the <unk> of the <unk> <unk> , <unk> , <unk>\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t to le <unk> . = = franchomme = = henry society were moved into a depot in the town , after vegetarian widowed wheeler was\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the relation to be given to some alex sleep . douglas were subject to take devotee . perfect james pullo , and think he\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t when the helium was in france and new zealand times was romantically completely to a signal rate of modern landmarks goods in london . he\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch  16 | example generation time:  1.22s\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  17 |   200/  915 batches | lr 0.22 | ms/batch 177.45 | loss  5.54 | ppl   255.03\n",
      "| epoch  17 |   400/  915 batches | lr 0.22 | ms/batch 176.60 | loss  5.51 | ppl   246.53\n",
      "| epoch  17 |   600/  915 batches | lr 0.22 | ms/batch 176.31 | loss  5.46 | ppl   234.59\n",
      "| epoch  17 |   800/  915 batches | lr 0.22 | ms/batch 176.49 | loss  5.50 | ppl   243.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 166.86s | valid loss  5.37 | valid ppl   214.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 17: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t to the <unk> of the <unk> , and the <unk> of the <unk> , the <unk> of the <unk> , and the <unk> of the\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t by <unk> , in which <unk> judge captured position henry society on june 16 , russia . the youngest ' s vegetarian colours of operation\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the relation to be given to some alex sleep . douglas were subject to take devotee . perfect prospered had nine work on scarlet\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t when the helium was in france and new zealand times was romantically completely to a signal rate of modern landmarks goods in london . he\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch  17 | example generation time:  1.31s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/  915 batches | lr 0.21 | ms/batch 177.57 | loss  5.51 | ppl   248.15\n",
      "| epoch  18 |   400/  915 batches | lr 0.21 | ms/batch 176.71 | loss  5.48 | ppl   240.74\n",
      "| epoch  18 |   600/  915 batches | lr 0.21 | ms/batch 176.67 | loss  5.43 | ppl   228.47\n",
      "| epoch  18 |   800/  915 batches | lr 0.21 | ms/batch 176.69 | loss  5.47 | ppl   237.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 167.05s | valid loss  5.35 | valid ppl   210.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 18: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t to the <unk> of the <unk> , and the <unk> of the <unk> <unk> , the <unk> of the <unk> <unk> , <unk> , <unk>\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t by <unk> , in which <unk> judge captured position henry society on june 16 , russia . the youngest ' s vegetarian widowed of operation\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the relation to be given to some alex sleep . douglas were subject to take devotee . perfect prospered had nine work on scarlet\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t when the helium was in france and new zealand times was romantically completely to a signal rate of modern landmarks goods in london . he\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch  18 | example generation time:  1.28s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/  915 batches | lr 0.20 | ms/batch 177.39 | loss  5.49 | ppl   242.42\n",
      "| epoch  19 |   400/  915 batches | lr 0.20 | ms/batch 176.77 | loss  5.46 | ppl   235.11\n",
      "| epoch  19 |   600/  915 batches | lr 0.20 | ms/batch 176.84 | loss  5.41 | ppl   223.25\n",
      "| epoch  19 |   800/  915 batches | lr 0.20 | ms/batch 176.70 | loss  5.45 | ppl   232.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 167.05s | valid loss  5.34 | valid ppl   207.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 19: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t to the <unk> of the <unk> , and the <unk> of the <unk> of the <unk> . the <unk> of the <unk> <unk> , the\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t by <unk> , hurlford , and judge . the most sad staff has finished one to reach them in a 28 – 1 defeat operation\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the relation to be given to the strict telescope . douglas were added to take devotee captain perfect , providing nine and 20 scarlet\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t when the helium was in france and new zealand times was romantically completely to a signal rate of modern landmarks goods in london . he\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch  19 | example generation time:  1.33s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/  915 batches | lr 0.19 | ms/batch 177.24 | loss  5.47 | ppl   237.57\n",
      "| epoch  20 |   400/  915 batches | lr 0.19 | ms/batch 176.68 | loss  5.44 | ppl   229.89\n",
      "| epoch  20 |   600/  915 batches | lr 0.19 | ms/batch 176.74 | loss  5.39 | ppl   218.57\n",
      "| epoch  20 |   800/  915 batches | lr 0.19 | ms/batch 176.59 | loss  5.43 | ppl   227.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 166.98s | valid loss  5.32 | valid ppl   204.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 20: The dog ran ...\n",
      "Greedy decoding:\n",
      "\t to the <unk> of the <unk> , and the <unk> of the <unk> of the <unk> . the <unk> of the <unk> <unk> , the\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\t by le <unk> de janeiro and judge . the henry society were moved in defaced to environmental europe , while the un colours of operation\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\t through the relation to be given to the strict telescope . douglas were added to take devotee captain perfect the london , and think he\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\t when the helium was in this tv new zealand times was romantically completely governed . he also kept it as a private member of chains\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch  20 | example generation time:  1.27s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   200/  915 batches | lr 0.18 | ms/batch 177.43 | loss  5.45 | ppl   232.31\n",
      "| epoch  21 |   400/  915 batches | lr 0.18 | ms/batch 176.74 | loss  5.42 | ppl   225.23\n",
      "| epoch  21 |   600/  915 batches | lr 0.18 | ms/batch 176.70 | loss  5.37 | ppl   214.29\n"
     ]
    }
   ],
   "source": [
    "batch_per_epoch = len(train_data) // BPTT\n",
    "if (len(train_data) - 1) % BPTT != 0:\n",
    "    batch_per_epoch += 1\n",
    "\n",
    "training_ppl_train_fine = np.zeros(epochs * batch_per_epoch)\n",
    "training_ppl_val_coarse = np.zeros(epochs)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model = None\n",
    "\n",
    "if gen_text_each_epoch:\n",
    "    decode_during_training(model, 0)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        \n",
    "    epoch_start_time = time.time()\n",
    "    loss_per_batch = train(model, device, train_data, ntokens, optimizer, scheduler, criterion, epoch)\n",
    "    \n",
    "    # print validation loss info each epoch\n",
    "    validation_loss = evaluate(model, val_data, device, ntokens, criterion)\n",
    "    validation_ppl = math.exp(validation_loss)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     validation_loss, validation_ppl))\n",
    "    print('-' * 89)\n",
    "    \n",
    "    # store loss info\n",
    "    training_ppl_val_coarse[epoch-1] = validation_ppl\n",
    "    training_ppl_train_fine[(epoch-1) * batch_per_epoch: (epoch)*batch_per_epoch] = np.exp(loss_per_batch)\n",
    "    \n",
    "    # save best model so far\n",
    "    if validation_loss < best_val_loss:\n",
    "        best_val_loss = validation_loss\n",
    "        torch.save(model.state_dict(), NOTEBOOK_OUTDIR + os.sep + 'bestval_model_weights.pth')\n",
    "        torch.save(model, NOTEBOOK_OUTDIR + os.sep + 'bestval_model.pth')\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    # optionally generate text based on fixed text-prompt\n",
    "    if gen_text_each_epoch:\n",
    "        decode_during_training(model, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Model**\n",
    "\n",
    "Approach 1: save model weights,\n",
    "load as: \n",
    "\n",
    "  `model.load_state_dict(torch.load('model_weights.pth'))`    \n",
    "  `model.eval()`\n",
    "  \n",
    "Approach 2: save model class object entirely (uses pickle),\n",
    "load as: \n",
    "  `model = torch.load('model.pth')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), NOTEBOOK_OUTDIR + os.sep + 'end_model_weights.pth')\n",
    "torch.save(model, NOTEBOOK_OUTDIR + os.sep + 'end_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJhCBOg1h0s6"
   },
   "source": [
    "Evaluate the model with the test dataset\n",
    "-------------------------------------\n",
    "\n",
    "Assess the model from the end of training (last epoch) on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3804,
     "status": "ok",
     "timestamp": 1622775216924,
     "user": {
      "displayName": "Matthew Smart",
      "photoUrl": "",
      "userId": "06726801805653264331"
     },
     "user_tz": 240
    },
    "id": "zL_3xrzEh0s6",
    "outputId": "7aa7d3a8-e176-47a8-ea5e-9620ea371c71"
   },
   "outputs": [],
   "source": [
    "datachoices = {'train': train_data, \n",
    "               'val': val_data,\n",
    "               'test': test_data}\n",
    "\n",
    "for datalabel in ['train', 'val', 'test']:\n",
    "    data_loss = evaluate(model, datachoices[datalabel], device, ntokens, criterion)\n",
    "    print('=' * 89)\n",
    "    print('| End of training | {} loss {:5.2f} | {} ppl {:8.2f}'.format(\n",
    "        datalabel, data_loss, datalabel, math.exp(data_loss)))\n",
    "    print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best model (lowest validation loss) to check the result with the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(NOTEBOOK_OUTDIR + os.sep + 'bestval_model.pth',\n",
    "                        device, \n",
    "                        as_pickle=True)\n",
    "\n",
    "datachoices = {'train': train_data, \n",
    "               'val': val_data,\n",
    "               'test': test_data}\n",
    "\n",
    "for datalabel in ['train', 'val', 'test']:\n",
    "    data_loss = evaluate(best_model, datachoices[datalabel], device, ntokens, criterion)\n",
    "    print('=' * 89)\n",
    "    print('| Best model | {} loss {:5.2f} | {} ppl {:8.2f}'.format(\n",
    "        datalabel, data_loss, datalabel, math.exp(data_loss)))\n",
    "    print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQikQeFBoPQI"
   },
   "source": [
    "# Plot training timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TG5T326ioPQI",
    "outputId": "64954352-eb06-4150-ac4d-ee4d18661eea"
   },
   "outputs": [],
   "source": [
    "np.savetxt(NOTEBOOK_OUTDIR + os.sep + 'training_ppl_train_fine.txt', training_ppl_train_fine)\n",
    "np.savetxt(NOTEBOOK_OUTDIR + os.sep + 'training_ppl_val_coarse.txt', training_ppl_val_coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qp7e1uToPQI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_axis = np.arange(1, epochs + 1)\n",
    "epochs_axis_fine = np.linspace(0, epochs, epochs * batch_per_epoch + 1)[:-1]\n",
    "\n",
    "def plot_training_timeseries(fname='training_performance', ext='.jpg', logy=True, xlims=None, ylims=None):\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(epochs_axis, training_ppl_val_coarse, '--ok', label='val', zorder=2)\n",
    "    plt.plot(epochs_axis_fine, training_ppl_train_fine, 'b', label='train', zorder=1, alpha=0.5)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('ppl (validation set)')\n",
    "    plt.legend()\n",
    "    if logy:\n",
    "        plt.yscale('log')\n",
    "        fname += '_logy'\n",
    "    if xlims is not None:\n",
    "        plt.xlim(xlims[0], xlims[1])\n",
    "    if ylims is not None:\n",
    "        plt.ylim(ylims[0], ylims[1])\n",
    "    plt.savefig(NOTEBOOK_OUTDIR + os.sep + fname + ext)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkwIdFDUoPQI",
    "outputId": "bf37757d-5ad1-42b8-dee3-f21a53136c1f"
   },
   "outputs": [],
   "source": [
    "plot_training_timeseries(logy=True)\n",
    "plot_training_timeseries(logy=False, ylims=(0, np.max(training_ppl_val_coarse) * 1.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPE-PdGZoPQJ",
    "outputId": "b5176056-e800-47e3-800b-ef9ad64e289f"
   },
   "outputs": [],
   "source": [
    "print('Epoch after which validation loss is minimized (defines best_model):', np.argmin(training_ppl_val_coarse) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9bCsWNnoPQJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch_transformer_trainer.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/dca13261bbb4e9809d1a3aa521d22dd7/transformer_tutorial.ipynb",
     "timestamp": 1622001366637
    }
   ]
  },
  "kernelspec": {
   "display_name": "HLML2021",
   "language": "python",
   "name": "hlml2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
