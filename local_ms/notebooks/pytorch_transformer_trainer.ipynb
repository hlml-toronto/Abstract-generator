{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rTk268Ljh0sv"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_OsFOADh0sx"
   },
   "source": [
    "[link text](https://)\n",
    "Sequence-to-Sequence Modeling with nn.Transformer and TorchText\n",
    "===============================================================\n",
    "\n",
    "This is a tutorial on how to train a sequence-to-sequence model\n",
    "that uses the\n",
    "`nn.Transformer <https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer>`__ module.\n",
    "\n",
    "PyTorch 1.2 release includes a standard transformer module based on the\n",
    "paper `Attention is All You\n",
    "Need <https://arxiv.org/pdf/1706.03762.pdf>`__. The transformer model\n",
    "has been proved to be superior in quality for many sequence-to-sequence\n",
    "problems while being more parallelizable. The ``nn.Transformer`` module\n",
    "relies entirely on an attention mechanism (another module recently\n",
    "implemented as `nn.MultiheadAttention <https://pytorch.org/docs/master/nn.html?highlight=multiheadattention#torch.nn.MultiheadAttention>`__) to draw global dependencies\n",
    "between input and output. The ``nn.Transformer`` module is now highly\n",
    "modularized such that a single component (like `nn.TransformerEncoder <https://pytorch.org/docs/master/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder>`__\n",
    "in this tutorial) can be easily adapted/composed.\n",
    "\n",
    "![](https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/_static/img/transformer_architecture.jpg?raw=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handler: Colab vs. Local Jupyter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in colab: False\n",
      "Notebook output dir: nbdata\n"
     ]
    }
   ],
   "source": [
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    NOTEBOOK_OUTDIR = \"MyDrive\" \n",
    "    cd /content/drive\n",
    "else:\n",
    "    NOTEBOOK_OUTDIR = 'output'\n",
    "    os.makedirs(NOTEBOOK_OUTDIR, exist_ok=True)\n",
    "\n",
    "print('Running in colab:', IN_COLAB)\n",
    "print('Notebook output dir:', NOTEBOOK_OUTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handler: Set Device (CUDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available(): False\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('torch.cuda.is_available():', torch.cuda.is_available())\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDoippnch0sy"
   },
   "source": [
    "Define the model\n",
    "----------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRsSsFnPh0sz"
   },
   "source": [
    "In this tutorial, we train ``nn.TransformerEncoder`` model on a\n",
    "language modeling task. The language modeling task is to assign a\n",
    "probability for the likelihood of a given word (or a sequence of words)\n",
    "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
    "layer first, followed by a positional encoding layer to account for the order\n",
    "of the word (see the next paragraph for more details). The\n",
    "``nn.TransformerEncoder`` consists of multiple layers of\n",
    "`nn.TransformerEncoderLayer <https://pytorch.org/docs/master/nn.html?highlight=transformerencoderlayer#torch.nn.TransformerEncoderLayer>`__. Along with the input sequence, a square\n",
    "attention mask is required because the self-attention layers in\n",
    "``nn.TransformerEncoder`` are only allowed to attend the earlier positions in\n",
    "the sequence. For the language modeling task, any tokens on the future\n",
    "positions should be masked. To have the actual words, the output\n",
    "of ``nn.TransformerEncoder`` model is sent to the final Linear\n",
    "layer, which is followed by a log-Softmax function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HQ0YNozvh0sz"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p733KjQoh0s0"
   },
   "source": [
    "``PositionalEncoding`` module injects some information about the\n",
    "relative or absolute position of the tokens in the sequence. The\n",
    "positional encodings have the same dimension as the embeddings so that\n",
    "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
    "different frequencies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UGdfZ8ixh0s1"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZO1xVjDAh0s1"
   },
   "source": [
    "Load and batch data\n",
    "-------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFOwEqVNh0s2"
   },
   "source": [
    "This tutorial uses ``torchtext`` to generate Wikitext-2 dataset. The\n",
    "vocab object is built based on the train dataset and is used to numericalize\n",
    "tokens into tensors. Starting from sequential data, the ``batchify()``\n",
    "function arranges the dataset into columns, trimming off any tokens remaining\n",
    "after the data has been divided into batches of size ``batch_size``.\n",
    "For instance, with the alphabet as the sequence (total length of 26)\n",
    "and a batch size of 4, we would divide the alphabet into 4 sequences of\n",
    "length 6:\n",
    "\n",
    "\\begin{align}\\begin{bmatrix}\n",
    "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
    "  \\end{bmatrix}\n",
    "  \\Rightarrow\n",
    "  \\begin{bmatrix}\n",
    "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
    "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
    "  \\end{bmatrix}\\end{align}\n",
    "\n",
    "These columns are treated as independent by the model, which means that\n",
    "the dependence of ``G`` and ``F`` can not be learned, but allows more\n",
    "efficient batch processing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15537,
     "status": "ok",
     "timestamp": 1622761734210,
     "user": {
      "displayName": "Matthew Smart",
      "photoUrl": "",
      "userId": "06726801805653264331"
     },
     "user_tz": 240
    },
    "id": "ECcX7oEjh0s2",
    "outputId": "9b49c451-90db-45ea-f232-1efb1cc54bf4"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "for line in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter)\n",
    "\n",
    "def data_process(raw_text_iter):\n",
    "  data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                       dtype=torch.long) for item in raw_text_iter]\n",
    "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 32\n",
    "eval_batch_size = 32\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Testing block: shorten the size of the training data to make training loop quicker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUNCATING DATA (testmode_ipynb=True)\n",
      "Shapes before:\n",
      "torch.Size([204999, 10])\n",
      "torch.Size([24185, 10])\n",
      "torch.Size([21441, 10])\n",
      "Shapes after:\n",
      "torch.Size([10249, 10])\n",
      "torch.Size([1209, 10])\n",
      "torch.Size([1072, 10])\n"
     ]
    }
   ],
   "source": [
    "testmode_ipynb = True\n",
    "data_reduce = 0.05\n",
    "\n",
    "if testmode_ipynb:\n",
    "    print('TRUNCATING DATA (testmode_ipynb=True)\\nShapes before:')\n",
    "    print(train_data.shape)\n",
    "    print(test_data.shape)\n",
    "    print(val_data.shape)\n",
    "    train_data = train_data[0:int(data_reduce * train_data.shape[0]), :]\n",
    "    test_data = test_data[0:int(data_reduce * test_data.shape[0]), :]\n",
    "    val_data = val_data[0:int(data_reduce * val_data.shape[0]), :]\n",
    "    print('Shapes after:')\n",
    "    print(train_data.shape)\n",
    "    print(test_data.shape)\n",
    "    print(val_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXmvfbAfh0s3"
   },
   "source": [
    "``get_batch()`` function generates the input and target sequence for\n",
    "the transformer model. It subdivides the source data into chunks of\n",
    "length ``bptt``. For the language modeling task, the model needs the\n",
    "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
    "we’d get the following two Variables for ``i`` = 0:\n",
    "\n",
    "![](https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/_static/img/transformer_input_target.png?raw=1)\n",
    "\n",
    "\n",
    "It should be noted that the chunks are along dimension 0, consistent\n",
    "with the ``S`` dimension in the Transformer model. The batch dimension\n",
    "``N`` is along dimension 1.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "br5Wu4yPh0s4"
   },
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "def get_batch_alt(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target_alt = source[i+1:i+1+seq_len]\n",
    "    return data, target_alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKYt_D4wh0s4"
   },
   "source": [
    "Initiate an instance\n",
    "--------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_ejvBQ4h0s4"
   },
   "source": [
    "The model is set up with the hyperparameter below. The vocab size is\n",
    "equal to the length of the vocab object.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ElCNW7sWh0s4"
   },
   "outputs": [],
   "source": [
    "ntokens = len(vocab.stoi) # the size of vocabulary\n",
    "emsize = 200              # embedding dimension\n",
    "nhid = 200                # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2               # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2                 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2             # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmkcIDCvh0s5"
   },
   "source": [
    "Run the model\n",
    "-------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6N9258vph0s5"
   },
   "source": [
    "`CrossEntropyLoss <https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__\n",
    "is applied to track the loss and\n",
    "`SGD <https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD>`__\n",
    "implements stochastic gradient descent method as the optimizer. The initial\n",
    "learning rate is set to 5.0. `StepLR <https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR>`__ is\n",
    "applied to adjust the learn rate through epochs. During the\n",
    "training, we use\n",
    "`nn.utils.clip_grad_norm\\_ <https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_>`__\n",
    "function to scale all the gradient together to prevent exploding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hSWQPh_Kh0s5"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    \n",
    "    batch_indices = np.arange(0, train_data.size(0) - 1, bptt)\n",
    "    loss_per_batch = 0.0 * batch_indices  # record the training loss for each batch\n",
    "    \n",
    "    print(batch_indices)\n",
    "    print(train_data.size(0))\n",
    "    \n",
    "    for batch, i in enumerate(batch_indices):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # TODO check if scaling is correct\n",
    "        loss_per_batch[batch] = loss\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_last_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "    return loss_per_batch\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrLNZrMIh0s6"
   },
   "source": [
    "Loop over epochs. Save the model if the validation loss is the best\n",
    "we've seen so far. Adjust the learning rate after each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2930154,
     "status": "ok",
     "timestamp": 1622775213128,
     "user": {
      "displayName": "Matthew Smart",
      "photoUrl": "",
      "userId": "06726801805653264331"
     },
     "user_tz": 240
    },
    "id": "51SeKWoph0s6",
    "outputId": "13899213-e03d-44cb-8839-991b708fe531",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0    35    70   105   140   175   210   245   280   315   350   385\n",
      "   420   455   490   525   560   595   630   665   700   735   770   805\n",
      "   840   875   910   945   980  1015  1050  1085  1120  1155  1190  1225\n",
      "  1260  1295  1330  1365  1400  1435  1470  1505  1540  1575  1610  1645\n",
      "  1680  1715  1750  1785  1820  1855  1890  1925  1960  1995  2030  2065\n",
      "  2100  2135  2170  2205  2240  2275  2310  2345  2380  2415  2450  2485\n",
      "  2520  2555  2590  2625  2660  2695  2730  2765  2800  2835  2870  2905\n",
      "  2940  2975  3010  3045  3080  3115  3150  3185  3220  3255  3290  3325\n",
      "  3360  3395  3430  3465  3500  3535  3570  3605  3640  3675  3710  3745\n",
      "  3780  3815  3850  3885  3920  3955  3990  4025  4060  4095  4130  4165\n",
      "  4200  4235  4270  4305  4340  4375  4410  4445  4480  4515  4550  4585\n",
      "  4620  4655  4690  4725  4760  4795  4830  4865  4900  4935  4970  5005\n",
      "  5040  5075  5110  5145  5180  5215  5250  5285  5320  5355  5390  5425\n",
      "  5460  5495  5530  5565  5600  5635  5670  5705  5740  5775  5810  5845\n",
      "  5880  5915  5950  5985  6020  6055  6090  6125  6160  6195  6230  6265\n",
      "  6300  6335  6370  6405  6440  6475  6510  6545  6580  6615  6650  6685\n",
      "  6720  6755  6790  6825  6860  6895  6930  6965  7000  7035  7070  7105\n",
      "  7140  7175  7210  7245  7280  7315  7350  7385  7420  7455  7490  7525\n",
      "  7560  7595  7630  7665  7700  7735  7770  7805  7840  7875  7910  7945\n",
      "  7980  8015  8050  8085  8120  8155  8190  8225  8260  8295  8330  8365\n",
      "  8400  8435  8470  8505  8540  8575  8610  8645  8680  8715  8750  8785\n",
      "  8820  8855  8890  8925  8960  8995  9030  9065  9100  9135  9170  9205\n",
      "  9240  9275  9310  9345  9380  9415  9450  9485  9520  9555  9590  9625\n",
      "  9660  9695  9730  9765  9800  9835  9870  9905  9940  9975 10010 10045\n",
      " 10080 10115 10150 10185 10220]\n",
      "10249\n",
      "| epoch   1 |   200/  292 batches | lr 5.00 | ms/batch 434.05 | loss  8.12 | ppl  3370.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 131.53s | valid loss  6.74 | valid ppl   849.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "[    0    35    70   105   140   175   210   245   280   315   350   385\n",
      "   420   455   490   525   560   595   630   665   700   735   770   805\n",
      "   840   875   910   945   980  1015  1050  1085  1120  1155  1190  1225\n",
      "  1260  1295  1330  1365  1400  1435  1470  1505  1540  1575  1610  1645\n",
      "  1680  1715  1750  1785  1820  1855  1890  1925  1960  1995  2030  2065\n",
      "  2100  2135  2170  2205  2240  2275  2310  2345  2380  2415  2450  2485\n",
      "  2520  2555  2590  2625  2660  2695  2730  2765  2800  2835  2870  2905\n",
      "  2940  2975  3010  3045  3080  3115  3150  3185  3220  3255  3290  3325\n",
      "  3360  3395  3430  3465  3500  3535  3570  3605  3640  3675  3710  3745\n",
      "  3780  3815  3850  3885  3920  3955  3990  4025  4060  4095  4130  4165\n",
      "  4200  4235  4270  4305  4340  4375  4410  4445  4480  4515  4550  4585\n",
      "  4620  4655  4690  4725  4760  4795  4830  4865  4900  4935  4970  5005\n",
      "  5040  5075  5110  5145  5180  5215  5250  5285  5320  5355  5390  5425\n",
      "  5460  5495  5530  5565  5600  5635  5670  5705  5740  5775  5810  5845\n",
      "  5880  5915  5950  5985  6020  6055  6090  6125  6160  6195  6230  6265\n",
      "  6300  6335  6370  6405  6440  6475  6510  6545  6580  6615  6650  6685\n",
      "  6720  6755  6790  6825  6860  6895  6930  6965  7000  7035  7070  7105\n",
      "  7140  7175  7210  7245  7280  7315  7350  7385  7420  7455  7490  7525\n",
      "  7560  7595  7630  7665  7700  7735  7770  7805  7840  7875  7910  7945\n",
      "  7980  8015  8050  8085  8120  8155  8190  8225  8260  8295  8330  8365\n",
      "  8400  8435  8470  8505  8540  8575  8610  8645  8680  8715  8750  8785\n",
      "  8820  8855  8890  8925  8960  8995  9030  9065  9100  9135  9170  9205\n",
      "  9240  9275  9310  9345  9380  9415  9450  9485  9520  9555  9590  9625\n",
      "  9660  9695  9730  9765  9800  9835  9870  9905  9940  9975 10010 10045\n",
      " 10080 10115 10150 10185 10220]\n",
      "10249\n",
      "| epoch   2 |   200/  292 batches | lr 4.75 | ms/batch 452.03 | loss  6.46 | ppl   639.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 132.02s | valid loss  6.42 | valid ppl   614.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "[    0    35    70   105   140   175   210   245   280   315   350   385\n",
      "   420   455   490   525   560   595   630   665   700   735   770   805\n",
      "   840   875   910   945   980  1015  1050  1085  1120  1155  1190  1225\n",
      "  1260  1295  1330  1365  1400  1435  1470  1505  1540  1575  1610  1645\n",
      "  1680  1715  1750  1785  1820  1855  1890  1925  1960  1995  2030  2065\n",
      "  2100  2135  2170  2205  2240  2275  2310  2345  2380  2415  2450  2485\n",
      "  2520  2555  2590  2625  2660  2695  2730  2765  2800  2835  2870  2905\n",
      "  2940  2975  3010  3045  3080  3115  3150  3185  3220  3255  3290  3325\n",
      "  3360  3395  3430  3465  3500  3535  3570  3605  3640  3675  3710  3745\n",
      "  3780  3815  3850  3885  3920  3955  3990  4025  4060  4095  4130  4165\n",
      "  4200  4235  4270  4305  4340  4375  4410  4445  4480  4515  4550  4585\n",
      "  4620  4655  4690  4725  4760  4795  4830  4865  4900  4935  4970  5005\n",
      "  5040  5075  5110  5145  5180  5215  5250  5285  5320  5355  5390  5425\n",
      "  5460  5495  5530  5565  5600  5635  5670  5705  5740  5775  5810  5845\n",
      "  5880  5915  5950  5985  6020  6055  6090  6125  6160  6195  6230  6265\n",
      "  6300  6335  6370  6405  6440  6475  6510  6545  6580  6615  6650  6685\n",
      "  6720  6755  6790  6825  6860  6895  6930  6965  7000  7035  7070  7105\n",
      "  7140  7175  7210  7245  7280  7315  7350  7385  7420  7455  7490  7525\n",
      "  7560  7595  7630  7665  7700  7735  7770  7805  7840  7875  7910  7945\n",
      "  7980  8015  8050  8085  8120  8155  8190  8225  8260  8295  8330  8365\n",
      "  8400  8435  8470  8505  8540  8575  8610  8645  8680  8715  8750  8785\n",
      "  8820  8855  8890  8925  8960  8995  9030  9065  9100  9135  9170  9205\n",
      "  9240  9275  9310  9345  9380  9415  9450  9485  9520  9555  9590  9625\n",
      "  9660  9695  9730  9765  9800  9835  9870  9905  9940  9975 10010 10045\n",
      " 10080 10115 10150 10185 10220]\n",
      "10249\n",
      "| epoch   3 |   200/  292 batches | lr 4.51 | ms/batch 391.10 | loss  5.88 | ppl   357.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 117.19s | valid loss  6.58 | valid ppl   718.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "[    0    35    70   105   140   175   210   245   280   315   350   385\n",
      "   420   455   490   525   560   595   630   665   700   735   770   805\n",
      "   840   875   910   945   980  1015  1050  1085  1120  1155  1190  1225\n",
      "  1260  1295  1330  1365  1400  1435  1470  1505  1540  1575  1610  1645\n",
      "  1680  1715  1750  1785  1820  1855  1890  1925  1960  1995  2030  2065\n",
      "  2100  2135  2170  2205  2240  2275  2310  2345  2380  2415  2450  2485\n",
      "  2520  2555  2590  2625  2660  2695  2730  2765  2800  2835  2870  2905\n",
      "  2940  2975  3010  3045  3080  3115  3150  3185  3220  3255  3290  3325\n",
      "  3360  3395  3430  3465  3500  3535  3570  3605  3640  3675  3710  3745\n",
      "  3780  3815  3850  3885  3920  3955  3990  4025  4060  4095  4130  4165\n",
      "  4200  4235  4270  4305  4340  4375  4410  4445  4480  4515  4550  4585\n",
      "  4620  4655  4690  4725  4760  4795  4830  4865  4900  4935  4970  5005\n",
      "  5040  5075  5110  5145  5180  5215  5250  5285  5320  5355  5390  5425\n",
      "  5460  5495  5530  5565  5600  5635  5670  5705  5740  5775  5810  5845\n",
      "  5880  5915  5950  5985  6020  6055  6090  6125  6160  6195  6230  6265\n",
      "  6300  6335  6370  6405  6440  6475  6510  6545  6580  6615  6650  6685\n",
      "  6720  6755  6790  6825  6860  6895  6930  6965  7000  7035  7070  7105\n",
      "  7140  7175  7210  7245  7280  7315  7350  7385  7420  7455  7490  7525\n",
      "  7560  7595  7630  7665  7700  7735  7770  7805  7840  7875  7910  7945\n",
      "  7980  8015  8050  8085  8120  8155  8190  8225  8260  8295  8330  8365\n",
      "  8400  8435  8470  8505  8540  8575  8610  8645  8680  8715  8750  8785\n",
      "  8820  8855  8890  8925  8960  8995  9030  9065  9100  9135  9170  9205\n",
      "  9240  9275  9310  9345  9380  9415  9450  9485  9520  9555  9590  9625\n",
      "  9660  9695  9730  9765  9800  9835  9870  9905  9940  9975 10010 10045\n",
      " 10080 10115 10150 10185 10220]\n",
      "10249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |   200/  292 batches | lr 4.29 | ms/batch 373.81 | loss  5.43 | ppl   228.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 113.51s | valid loss  6.60 | valid ppl   737.72\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "epochs = 4  # The number of epochs\n",
    "   \n",
    "batch_per_epoch = len(train_data) // bptt\n",
    "if (len(train_data) - 1) % bptt != 0:\n",
    "    batch_per_epoch += 1\n",
    "\n",
    "training_ppl_train_fine = np.zeros(epochs * batch_per_epoch)\n",
    "training_ppl_val_coarse = np.zeros(epochs)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_per_batch = train()\n",
    "    \n",
    "    # print validation loss info each epoch\n",
    "    validation_loss = evaluate(model, val_data)\n",
    "    validation_ppl = math.exp(validation_loss)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     validation_loss, validation_ppl))\n",
    "    print('-' * 89)\n",
    "    \n",
    "    # store loss info\n",
    "    training_ppl_val_coarse[epoch-1] = validation_ppl\n",
    "    training_ppl_train_fine[(epoch-1) * batch_per_epoch: (epoch)*batch_per_epoch] = np.exp(loss_per_batch)\n",
    "    \n",
    "    # save best model so far\n",
    "    if validation_loss < best_val_loss:\n",
    "        best_val_loss = validation_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJhCBOg1h0s6"
   },
   "source": [
    "Evaluate the model with the test dataset\n",
    "-------------------------------------\n",
    "\n",
    "Apply the best model to check the result with the test dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3804,
     "status": "ok",
     "timestamp": 1622775216924,
     "user": {
      "displayName": "Matthew Smart",
      "photoUrl": "",
      "userId": "06726801805653264331"
     },
     "user_tz": 240
    },
    "id": "zL_3xrzEh0s6",
    "outputId": "7aa7d3a8-e176-47a8-ea5e-9620ea371c71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  6.49 | test ppl   659.56\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HR49CErYKibd"
   },
   "source": [
    "**Save Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1622775217056,
     "user": {
      "displayName": "Matthew Smart",
      "photoUrl": "",
      "userId": "06726801805653264331"
     },
     "user_tz": 240
    },
    "id": "FcdB4FZrLi3p",
    "outputId": "a541c8e3-4714-45ae-f332-bbdc16ff3639"
   },
   "outputs": [],
   "source": [
    "# These two lines may need to be in their own blocks (for interpreting non python)\n",
    "if IN_COLAB:\n",
    "    cd /content/drive\n",
    "    !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hyw0tiY0K70T"
   },
   "outputs": [],
   "source": [
    "# Approach 1: save model weights\n",
    "# load as: \n",
    "#   model.load_state_dict(torch.load('model_weights.pth'))\n",
    "#   model.eval()\n",
    "torch.save(model.state_dict(), NOTEBOOK_OUTDIR + os.sep + 'model_weights.pth')\n",
    "\n",
    "# Approach 2: save model (class) entirely (uses pickle)\n",
    "# load as: \n",
    "#   model = torch.load('model.pth')\n",
    "torch.save(model, NOTEBOOK_OUTDIR + os.sep + 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.savetxt(NOTEBOOK_OUTDIR + os.sep + 'training_ppl_train_fine.txt', training_ppl_train_fine)\n",
    "np.savetxt(NOTEBOOK_OUTDIR + os.sep + 'training_ppl_val_coarse.txt', training_ppl_val_coarse)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_axis = np.arange(1, epochs + 1)\n",
    "epochs_axis_fine = np.linspace(0, epochs, epochs * batch_per_epoch + 1)[:-1]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(epochs_axis, training_ppl_val_coarse, '--ok', label='val', zorder=2)\n",
    "plt.plot(epochs_axis_fine, training_ppl_train_fine, 'b', label='train', zorder=1, alpha=0.5)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('ppl (validation set)')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.savefig(NOTEBOOK_OUTDIR + os.sep + 'training_performance.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37830.85064057 10611.02200795 29608.03815448 ...   215.2253255\n",
      "   279.47947306   182.82413941]\n"
     ]
    }
   ],
   "source": [
    "print(training_ppl_train_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "local_transformer_tutorial.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/dca13261bbb4e9809d1a3aa521d22dd7/transformer_tutorial.ipynb",
     "timestamp": 1622001366637
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
