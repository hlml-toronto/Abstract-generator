{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "MZmtW9scbJ9Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1632422718135,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "MZmtW9scbJ9Y",
    "outputId": "af1efac7-2891-49b0-ca37-afde5eb8d45a"
   },
   "outputs": [],
   "source": [
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    %cd /content/drive/MyDrive/Github/Abstract-generator/bumbleBERT/notebooks\n",
    "    \n",
    "    %%capture\n",
    "    !pip install feedparser tokenizers transformers;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04f6ae9e",
   "metadata": {
    "executionInfo": {
     "elapsed": 3839,
     "status": "ok",
     "timestamp": 1632422319956,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "04f6ae9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:\\Development\\Repositories\\Abstract-generator\\local_ms\n"
     ]
    }
   ],
   "source": [
    "import os, torch, time, math, sys, re, csv\n",
    "import numpy as np\n",
    "\n",
    "PACKAGE_ROOT = os.path.dirname(os.path.abspath(''))\n",
    "print(PACKAGE_ROOT)\n",
    "sys.path.append(PACKAGE_ROOT)\n",
    "\n",
    "\n",
    "from src import settings\n",
    "import src.data.dataset_class_alt as dsc\n",
    "import src.data.dataloader_class_alt as dlc\n",
    "\n",
    "from src.model.transformer_torch import TransformerModel\n",
    "from src.model.generate_text import gen_some_text\n",
    "\n",
    "from src.model.train_evaluate import train_version_jeremy as train\n",
    "from src.model.train_evaluate import evaluate_version_jeremy as evaluate\n",
    "\n",
    "#from src.model.transformer import make_gpt_model # imports don't work\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904013cd",
   "metadata": {
    "id": "904013cd"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da73442b",
   "metadata": {
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1632422325957,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "da73442b"
   },
   "outputs": [],
   "source": [
    "# ARCHITECTURE\n",
    "maxLen     = 40 # maximum sentence length\n",
    "vocabSize  = None # None if you want to let tokenizer do its thing\n",
    "emsize     = 512 # embedding dimension\n",
    "nhid       = 2048 # the dimension of the feedforward network model in torch.nn.TransformerEncoder\n",
    "nlayers    = 12 # the number of torch.nn.TransformerEncoderLayer in torch.nn.TransformerEncoder\n",
    "nhead      = 8 # the number of heads in the multiheadattention models\n",
    "dropout    = 0.2 # the dropout value\n",
    "batchSize = 10 #32\n",
    "valBatchSize = 10 #32, not used right now.\n",
    "epochs     = 10  # The number of epochs\n",
    "\n",
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21983a34",
   "metadata": {
    "id": "21983a34"
   },
   "source": [
    "### Format Dataset\n",
    "\n",
    "Uses a custom dataset class, which is an iterable and callable structure that returns a sample from our dataset. Within this custom dataset, can determine all preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015ae225",
   "metadata": {
    "executionInfo": {
     "elapsed": 2574,
     "status": "ok",
     "timestamp": 1632422336336,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "015ae225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning : overwriting previously save tokenizer with                        same filename ( I:\\Development\\Repositories\\Abstract-generator\\local_ms\\saved_tokenizers\\BPE_wikitext-2-raw.json ).\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "#dataset = dsc.ArxivDataset()\n",
    "dataset = dsc.WikiTextDataset()\n",
    "\n",
    "#train tokenizer (or use one already trained)\n",
    "tknzr_type = 'BPE'\n",
    "flag_tknzr_train = True\n",
    "flag_tknzr_fast = True\n",
    "\n",
    "_ = dataset.tokenizer(flag_tknzr_train, tknzr_type, flag_tknzr_fast=flag_tknzr_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bba4cb9",
   "metadata": {
    "id": "4bba4cb9"
   },
   "source": [
    "### Creating DataLoaders\n",
    "\n",
    "Training is done on batches, so we need a way to extract groupings of the data in the appropriate format for our transformer model.\n",
    "Note that for transformers which we are training, dataloaders outputs both src (x[:-1] and tgt ([1:]).\n",
    "The collation of batches for different transformer models we have vary. For HuggingFace it's ( maxLen x batch_size ) whereas I think that the Annotated Transformer has ( batch_size x maxLen ).\n",
    "\n",
    "I created a custom Dataloader class that wraps splitting the dataset and also outputs different dataloaders for each.\n",
    "\n",
    "NOTE : Do not use the tokenizer before the training if you use num_workers>0!\n",
    "FastTokenizer does not play nicely with forking if you use it before the forking of your data:\n",
    "https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c11048fc",
   "metadata": {
    "executionInfo": {
     "elapsed": 502,
     "status": "ok",
     "timestamp": 1632422351212,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "c11048fc"
   },
   "outputs": [],
   "source": [
    "dataloader = dlc.CustomDataloader(dataset, batchSize, maxLen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3955854",
   "metadata": {
    "id": "e3955854"
   },
   "source": [
    "### Selecting model\n",
    "\n",
    "Here we choose which model we shall use for training. For now, I've selected the black box Transformer from HuggingFace because the collate_fn I've written gives the correct input size force it... however this can easily be changed! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6de7d252",
   "metadata": {
    "executionInfo": {
     "elapsed": 969,
     "status": "ok",
     "timestamp": 1632422760770,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "6de7d252"
   },
   "outputs": [],
   "source": [
    "# transformer from huggingface\n",
    "# TODO : Change to the Annotated Transformer if I want\n",
    "model = TransformerModel(dataset.vocab_size, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "\n",
    "# criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()#ignore_index=tknzr.get_vocab()[\"<pad>\"])\n",
    "\n",
    "# optimizer\n",
    "paramsAdam  = [{'params' : model.parameters(), 'lr' : 1e-3, 'betas' : (0.9, 0.999), 'eps' : 1e-08, 'weight_decay' : 0.0}]\n",
    "paramsAdamW = [{'params' : model.parameters(), 'lr' : 5e-5, 'betas' : (0.9, 0.999), 'eps' : 1e-08, 'weight_decay' : 0.0}]\n",
    "paramsSGD   = [{'params' : model.parameters(), 'lr' : 0.5, 'momentum' : 0.0, 'dampening' : 0.0, 'weight_decay' : 0.0}]\n",
    "\n",
    "#optimizer = torch.optim.SGD( paramsSGD )\n",
    "#optimizer = torch.optim.Adam( paramsAdam )\n",
    "optimizer = torch.optim.AdamW( paramsAdamW )\n",
    "\n",
    "# scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95) # 1.0 to signify no decay rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780d86f",
   "metadata": {
    "id": "9780d86f"
   },
   "source": [
    "### Training\n",
    "\n",
    "Training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75775f0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 745071,
     "status": "ok",
     "timestamp": 1632423535568,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "75775f0e",
    "outputId": "00a9192c-d755-4c4b-a3f3-cf15547131fe"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'CustomDataloader.__init__.<locals>.collate_wrapper'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0a75d7a0ca6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mepoch_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxLen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxLen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdsetValid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m89\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\Development\\Repositories\\Abstract-generator\\local_ms\\src\\model\\train_evaluate.py\u001b[0m in \u001b[0;36mtrain_version_jeremy\u001b[1;34m(model, dataLoader, device, vocabSize, epoch, optimizer_, scheduler_, criterion_, maxLen)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmaxLen\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0msrc_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_square_subsequent_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxLen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataLoader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[1;31m#print((batch.src).is_pinned())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mtgt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matts\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matts\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matts\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matts\\appdata\\local\\programs\\python\\python38\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matts\\appdata\\local\\programs\\python\\python38\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matts\\appdata\\local\\programs\\python\\python38\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matts\\appdata\\local\\programs\\python\\python38\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\matts\\appdata\\local\\programs\\python\\python38\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'CustomDataloader.__init__.<locals>.collate_wrapper'"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # fasttokenizer should not be used before forking. Something\n",
    "                                                # to figure out. What this does is suppress some warning messages \n",
    "                                                # https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning\n",
    "                                                # doesn't seem to affect the timing though\n",
    "if TRAIN:\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model, dataloader.train, device, dataset.vocab_size, epoch, optimizer, scheduler, criterion, maxLen)\n",
    "        val_loss = evaluate(model, dataloader.valid, device, dataset.vocab_size, criterion, maxLen, len(dataloader.dsetValid))\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                         val_loss, math.exp(val_loss)))\n",
    "                                         # Why is math.exp so large????\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # save best model (two methods)\n",
    "    modelFull = default.MODEL_DIR + os.sep + f'{dataset.name}_epoch{epochs}.pth'\n",
    "    modelWeights = default.MODEL_DIR + os.sep + f'{dataset.name}_weights_epoch{epochs}.pth'\n",
    "    modelFullBest = default.MODEL_DIR + os.sep + f'{dataset.name}_epoch{epochs}_best.pth'\n",
    "    modelWeightsBest = default.MODEL_DIR + os.sep + f'{dataset.name}_weights_epoch{epochs}_best.pth'\n",
    "    # approach 1: save model (class) entirely (uses pickle)\n",
    "    torch.save(model, modelFull)\n",
    "    torch.save(best_model, modelFullBest)\n",
    "    # approach 2: save model weights\n",
    "    torch.save(best_model.state_dict(), modelWeightsBest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XM5G4vD0q9TZ",
   "metadata": {
    "id": "XM5G4vD0q9TZ"
   },
   "source": [
    "### Text Generation\n",
    "\n",
    "Here I've simply taken the code Matt uses to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6acee1",
   "metadata": {
    "id": "eb6acee1"
   },
   "outputs": [],
   "source": [
    "if not TRAIN:\n",
    "    customFilename = 'arxiv_10000'\n",
    "    customEpochs = 10\n",
    "    modelFull = default.MODEL_DIR + os.sep + f'{customFilename}_epoch{customEpochs}_best.pth'\n",
    "    modelWeights = default.MODEL_DIR + os.sep + f'{customFilename}_weights_epoch{customEpochs}_best.pth'\n",
    "    \n",
    "    # approach 1: load model (class) entirely (uses pickle)\n",
    "    modelFullLoad = torch.load(modelFull, map_location=device)\n",
    "\n",
    "    # approach 2: load model weights, need to have some parameter or something \n",
    "    modelLoad = TransformerModel(vocabSize, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "    modelWeightsLoad = modelLoad.load_state_dict( torch.load(modelWeights) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SoGylu7hq7kI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10928,
     "status": "ok",
     "timestamp": 1632423598082,
     "user": {
      "displayName": "Jeremy Rothschild",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64",
      "userId": "12754514505135179603"
     },
     "user_tz": 240
    },
    "id": "SoGylu7hq7kI",
    "outputId": "b5d43790-e2bf-44a5-95f8-2afb5c481e8c"
   },
   "outputs": [],
   "source": [
    "# inspect both models\n",
    "#print('model_A info...\\n', modelFullLoad)\n",
    "#print('\\nmodel_B info...\\n', modelWeightsLoad)\n",
    "\n",
    "#print('model_A == model_B:', modelFullLoad == modelWeightsLoad)\n",
    "#model = modelFullLoad\n",
    "# Text generation example\n",
    "\n",
    "#model = modelLoad\n",
    "prompt = 'The dog ran'\n",
    "ngen = 100\n",
    "decode_style = 'sample_topp' #greedy, sample_topp\n",
    "model.to('cpu')\n",
    "generated_text = gen_some_text(\n",
    "    best_model, dataset.transform, 'cpu', maxLen, text_prompt=prompt, tokens_to_gen=ngen, vis=False,\n",
    "    decode_style=decode_style)\n",
    "print(\"Text prompt:\\n\", prompt)\n",
    "print(\"Number of tokens to generate:\", ngen)\n",
    "print(\"Generated_text:\\n\", generated_text)\n",
    "\n",
    "# TODO: alternative generation\n",
    "# currently 'greedy method'\n",
    "# see: https://huggingface.co/blog/how-to-generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7303c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "colab_training_and_evaluation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
