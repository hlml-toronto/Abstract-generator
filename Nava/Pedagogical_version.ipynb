{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkZBan6-z_ZI"
      },
      "source": [
        "## First Step: Data Extraction and Preperation\n",
        "\n",
        "### Get data from the arXiv database:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXpbde5k_ONh",
        "outputId": "c17b8ae1-6541-493b-9011-fc3918026d80"
      },
      "source": [
        "!pip install feedparser\n",
        "\n",
        "import urllib.request\n",
        "import feedparser\n",
        "\n",
        "# Base api query url\n",
        "base_url = 'http://export.arxiv.org/api/query?';\n",
        "\n",
        "# Search parameters\n",
        "search_query = 'all:electron' # search for electron in all fields\n",
        "start = 0                     # retreive the first 5 results\n",
        "max_results = 10\n",
        "\n",
        "query = 'search_query=%s&start=%i&max_results=%i' % (search_query,\n",
        "                                                     start,\n",
        "                                                     max_results)\n",
        "\n",
        "# perform a GET request using the base_url and query\n",
        "response = urllib.request.urlopen(base_url+query).read()\n",
        "\n",
        "# parse the response using feedparser\n",
        "feed = feedparser.parse(response)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting feedparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/b8/babbdef2a52fea529f43ca4c08f9917ecca1b7ffb27c340baecb746f39cb/feedparser-6.0.6-py3-none-any.whl (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30kB 30.1MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 18.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61kB 12.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 6.1MB/s \n",
            "\u001b[?25hCollecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp37-none-any.whl size=6067 sha256=a4727d2ca00c82db2893c61ff0f425ee98f03b452d9661dec08f0d9dce3227cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.6 sgmllib3k-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCygoWCJ0VP8"
      },
      "source": [
        "### convert it into a list + dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZj127blp3uA"
      },
      "source": [
        "#coloums of interasts (maybe?)\n",
        "col=['title', 'summary', 'authors', 'arxiv_primary_category', 'tags']\n",
        "\n",
        "# Run through each entry, and fill the information into a list\n",
        "data_list=[]\n",
        "for c in col:\n",
        "  abstract_list=[]\n",
        "  for entry in feed.entries:\n",
        "    abstract_list.append(entry.get(c))\n",
        "  data_list.append(abstract_list)\n",
        "\n",
        "# convert into a panda dataframe (maybe more visible + have some pros I might need)\n",
        "import pandas as pd\n",
        "data_df = pd.DataFrame(data_list,index=col)\n",
        "data_df=data_df.T\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU5l6yin0uJU"
      },
      "source": [
        "data_df.to_csv('arXiv10.csv') "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "2cihhiBucihA",
        "outputId": "ca73ce55-48f7-4854-85ce-7bd88fa203e1"
      },
      "source": [
        "data_df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>authors</th>\n",
              "      <th>arxiv_primary_category</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Impact of Electron-Electron Cusp on Configurat...</td>\n",
              "      <td>The effect of the electron-electron cusp on th...</td>\n",
              "      <td>[{'name': 'David Prendergast'}, {'name': 'M. N...</td>\n",
              "      <td>{'term': 'cond-mat.str-el', 'scheme': 'http://...</td>\n",
              "      <td>[{'term': 'cond-mat.str-el', 'scheme': 'http:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Electron thermal conductivity owing to collisi...</td>\n",
              "      <td>We calculate the thermal conductivity of elect...</td>\n",
              "      <td>[{'name': 'P. S. Shternin'}, {'name': 'D. G. Y...</td>\n",
              "      <td>{'term': 'astro-ph', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>[{'term': 'astro-ph', 'scheme': 'http://arxiv....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Electron pairing: from metastable electron pai...</td>\n",
              "      <td>Starting from the shell structure in atoms and...</td>\n",
              "      <td>[{'name': 'Guo-Qiang Hai'}, {'name': 'Ladir Câ...</td>\n",
              "      <td>{'term': 'cond-mat.str-el', 'scheme': 'http://...</td>\n",
              "      <td>[{'term': 'cond-mat.str-el', 'scheme': 'http:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Electron Temperature Anisotropy and Electron B...</td>\n",
              "      <td>Electron temperature anisotropies and electron...</td>\n",
              "      <td>[{'name': 'Heyu Sun'}, {'name': 'Jinsong Zhao'...</td>\n",
              "      <td>{'term': 'physics.space-ph', 'scheme': 'http:/...</td>\n",
              "      <td>[{'term': 'physics.space-ph', 'scheme': 'http:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hamiltonian of a many-electron system with sin...</td>\n",
              "      <td>Based on the metastable electron-pair energy b...</td>\n",
              "      <td>[{'name': 'G. -Q. Hai'}, {'name': 'F. M. Peete...</td>\n",
              "      <td>{'term': 'cond-mat.supr-con', 'scheme': 'http:...</td>\n",
              "      <td>[{'term': 'cond-mat.supr-con', 'scheme': 'http...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Electron-Electron Bremsstrahlung Emission and ...</td>\n",
              "      <td>Although both electron-ion and electron-electr...</td>\n",
              "      <td>[{'name': 'Eduard P. Kontar'}, {'name': 'A. Go...</td>\n",
              "      <td>{'term': 'astro-ph', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>[{'term': 'astro-ph', 'scheme': 'http://arxiv....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Improved scenario of baryogenesis</td>\n",
              "      <td>It is assumed that, in the primordial plasma, ...</td>\n",
              "      <td>[{'name': 'D. L. Khokhlov'}]</td>\n",
              "      <td>{'term': 'astro-ph', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>[{'term': 'astro-ph', 'scheme': 'http://arxiv....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Exact Electron-Pairing Ground States of Tight-...</td>\n",
              "      <td>We present a class of exactly solvable models ...</td>\n",
              "      <td>[{'name': 'Akinori Tanaka'}]</td>\n",
              "      <td>{'term': 'cond-mat.str-el', 'scheme': 'http://...</td>\n",
              "      <td>[{'term': 'cond-mat.str-el', 'scheme': 'http:/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Insights into the Electron-Electron Interactio...</td>\n",
              "      <td>The effective electron-electron interaction in...</td>\n",
              "      <td>[{'name': 'Carl A. Kukkonen'}, {'name': 'Kun C...</td>\n",
              "      <td>{'term': 'cond-mat.quant-gas', 'scheme': 'http...</td>\n",
              "      <td>[{'term': 'cond-mat.quant-gas', 'scheme': 'htt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Electron-electron interactions in a weakly scr...</td>\n",
              "      <td>We probe the strength of electron-electron int...</td>\n",
              "      <td>[{'name': 'I. Karakurt'}, {'name': 'A. J. Dahm'}]</td>\n",
              "      <td>{'term': 'cond-mat.str-el', 'scheme': 'http://...</td>\n",
              "      <td>[{'term': 'cond-mat.str-el', 'scheme': 'http:/...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                                               tags\n",
              "0  Impact of Electron-Electron Cusp on Configurat...  ...  [{'term': 'cond-mat.str-el', 'scheme': 'http:/...\n",
              "1  Electron thermal conductivity owing to collisi...  ...  [{'term': 'astro-ph', 'scheme': 'http://arxiv....\n",
              "2  Electron pairing: from metastable electron pai...  ...  [{'term': 'cond-mat.str-el', 'scheme': 'http:/...\n",
              "3  Electron Temperature Anisotropy and Electron B...  ...  [{'term': 'physics.space-ph', 'scheme': 'http:...\n",
              "4  Hamiltonian of a many-electron system with sin...  ...  [{'term': 'cond-mat.supr-con', 'scheme': 'http...\n",
              "5  Electron-Electron Bremsstrahlung Emission and ...  ...  [{'term': 'astro-ph', 'scheme': 'http://arxiv....\n",
              "6                  Improved scenario of baryogenesis  ...  [{'term': 'astro-ph', 'scheme': 'http://arxiv....\n",
              "7  Exact Electron-Pairing Ground States of Tight-...  ...  [{'term': 'cond-mat.str-el', 'scheme': 'http:/...\n",
              "8  Insights into the Electron-Electron Interactio...  ...  [{'term': 'cond-mat.quant-gas', 'scheme': 'htt...\n",
              "9  Electron-electron interactions in a weakly scr...  ...  [{'term': 'cond-mat.str-el', 'scheme': 'http:/...\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liF3C_P68jTs"
      },
      "source": [
        "### tokenizing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL1_OauX_O5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21de4000-c887-40e7-bc89-f996ab6b5545"
      },
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "import spacy\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp37-none-any.whl size=829180945 sha256=c9b048119799cfff369924e83c9c9f266da3e5ca2b6121d124ac0c3a66bd2411\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mc_qspcw/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrBRUil1zsps"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#taking just the titles (data_list[0]). (maybe to use the summary instaed?)\n",
        "# using lower case. removing extra spaces and '\\n ' \n",
        "doc=[nlp.tokenizer(text.lower().replace('\\n ','').strip()) for text in data_list[0]]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INkanj23b4oz"
      },
      "source": [
        "for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeOuK7w6XZa0",
        "outputId": "d459d5a9-9103-4764-ee5d-6679b9ab2cc5"
      },
      "source": [
        "II=5;\n",
        "print([token.rank for token in doc[II]])\n",
        "print([token.lex_id for token in doc[II]])\n",
        "print([token.text for token in doc[II]])\n",
        "print([nlp.vocab.strings[token.text] for token in doc[II]])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[15686, 33, 15686, 358821, 29900, 7, 3, 23757, 8, 15686, 18254, 62174, 13, 4085, 17115]\n",
            "[15686, 33, 15686, 358821, 29900, 7, 3, 23757, 8, 15686, 18254, 62174, 13, 4085, 17115]\n",
            "['electron', '-', 'electron', 'bremsstrahlung', 'emission', 'and', 'the', 'inference', 'of', 'electron', 'flux', 'spectra', 'in', 'solar', 'flares']\n",
            "[14911849430818137050, 9153284864653046197, 14911849430818137050, 5074391312506727947, 7093072741639743635, 2283656566040971221, 7425985699627899538, 10770840912364104747, 886050111519832510, 14911849430818137050, 7386502683099369518, 721847661104588153, 3002984154512732771, 3825196732376443040, 54522159426843760]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmDuJLXd4bdd",
        "outputId": "e5466cca-ed2e-47c3-baa8-75e89cafb427"
      },
      "source": [
        "# Words_vec= words in the text \n",
        "# Vocab = unique words in the text\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "Words=[]\n",
        "Max_Length=30;\n",
        "Words_vec= np.empty([max_results,Max_Length],dtype='int')\n",
        "\n",
        "\n",
        "for i in range(9):\n",
        "  #len([token.lex_id for token in doc[i]])\n",
        "  Words_vec[i,:len([token.lex_id for token in doc[i]])]=[token.lex_id for token in doc[i]]\n",
        "  for token in doc[i]:\n",
        "    #print(token.text, token.has_vector, token.vector, token.vector_norm ,\"\\n\")\n",
        "    #print(token.text, token.has_vector, token.lex_id ,\"\\n\")\n",
        "    Words.append(token.text.lower())\n",
        "    \n",
        "\n",
        "Vocab = []\n",
        "[Vocab.append(x) for x in Words if x not in Vocab]\n",
        "print('number of unique words in our data=',len(Vocab))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of unique words in our data= 68\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhcciczvd5QA"
      },
      "source": [
        "Input_list=[]\n",
        "for sample in range(len(doc)):\n",
        "  Input_list.append([token.rank for token in doc[sample]])\n",
        "Output_list=Input_list;\n",
        "Input_Output_Data_list=[Input_list,Output_list]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuanLbDSMRLp",
        "outputId": "db93abbc-4637-44cb-f59b-f667fb37c58b"
      },
      "source": [
        "print(Input_list[6],doc[6])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3371, 2322, 8, 429374] improved scenario of baryogenesis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crgSghzkWfaM"
      },
      "source": [
        "## next: to understand / complete\n",
        "\n",
        "train_list=  Input_list\n",
        "label=Output_list\n",
        "validation_list=train_list\n",
        "test_list=train_list"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkIqD548dHjO",
        "outputId": "a0b40aab-1a96-4db5-81b9-9200a139f91f"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for _sample in batch:\n",
        "        label_list.append(torch.tensor(_sample))\n",
        "        text_list.append(torch.tensor(_sample))\n",
        "    return pad_sequence(label_list, padding_value=0.0), pad_sequence(text_list, padding_value=0.0)\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "def create_iterators(batch_size=batch_size):\n",
        "    \"\"\"Heler function to create the iterators\"\"\"\n",
        "    dataloaders = []\n",
        "    for split in [train_list, validation_list, test_list]:\n",
        "        dataloader = DataLoader(\n",
        "            split, batch_size=batch_size,\n",
        "            collate_fn=collate_batch\n",
        "            )\n",
        "        dataloaders.append(dataloader)\n",
        "    return dataloaders\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = create_iterators()\n",
        "\n",
        "next(iter(train_iterator))\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[  1868,  15686,  15686,  15686, 247117],\n",
              "         [     8,   9884,  14652,   3405,      8],\n",
              "         [ 15686,  49985,     64, 198147,      6],\n",
              "         [    33,  27361,     57,      7,    181],\n",
              "         [ 15686,      5, 188891,  15686,     33],\n",
              "         [ 34898,  23120,  15686,   8928,  15686],\n",
              "         [    20,    331,   1908,  13132,    376],\n",
              "         [  9055,  24230,      5,     57,     24],\n",
              "         [  4167,  13604,      0,  15686,    469],\n",
              "         [ 21514,      0,      0,  17913,     33],\n",
              "         [     0,      0,      0,  98497,  15686],\n",
              "         [     0,      0,      0,     13,      7],\n",
              "         [     0,      0,      0,      3,  15686],\n",
              "         [     0,      0,      0,   4085,     33],\n",
              "         [     0,      0,      0,   2647,   1908],\n",
              "         [     0,      0,      0,      0,   1180],\n",
              "         [     0,      0,      0,      0,     13],\n",
              "         [     0,      0,      0,      0,      6],\n",
              "         [     0,      0,      0,      0,    193],\n",
              "         [     0,      0,      0,      0,     33],\n",
              "         [     0,      0,      0,      0,  10116],\n",
              "         [     0,      0,      0,      0,  23742],\n",
              "         [     0,      0,      0,      0,   1194]]),\n",
              " tensor([[  1868,  15686,  15686,  15686, 247117],\n",
              "         [     8,   9884,  14652,   3405,      8],\n",
              "         [ 15686,  49985,     64, 198147,      6],\n",
              "         [    33,  27361,     57,      7,    181],\n",
              "         [ 15686,      5, 188891,  15686,     33],\n",
              "         [ 34898,  23120,  15686,   8928,  15686],\n",
              "         [    20,    331,   1908,  13132,    376],\n",
              "         [  9055,  24230,      5,     57,     24],\n",
              "         [  4167,  13604,      0,  15686,    469],\n",
              "         [ 21514,      0,      0,  17913,     33],\n",
              "         [     0,      0,      0,  98497,  15686],\n",
              "         [     0,      0,      0,     13,      7],\n",
              "         [     0,      0,      0,      3,  15686],\n",
              "         [     0,      0,      0,   4085,     33],\n",
              "         [     0,      0,      0,   2647,   1908],\n",
              "         [     0,      0,      0,      0,   1180],\n",
              "         [     0,      0,      0,      0,     13],\n",
              "         [     0,      0,      0,      0,      6],\n",
              "         [     0,      0,      0,      0,    193],\n",
              "         [     0,      0,      0,      0,     33],\n",
              "         [     0,      0,      0,      0,  10116],\n",
              "         [     0,      0,      0,      0,  23742],\n",
              "         [     0,      0,      0,      0,   1194]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBhvp2gOw9nN"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}
