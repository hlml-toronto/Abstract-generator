{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c18ec27a",
   "metadata": {
    "id": "c18ec27a"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "MZmtW9scbJ9Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZmtW9scbJ9Y",
    "outputId": "0c8df7f6-e405-443f-970c-cf59cd9ff5e8"
   },
   "outputs": [],
   "source": [
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    %cd /content/drive/MyDrive/Documents/HLML/Abstract-generator/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2p8t9jjUlx_O",
   "metadata": {
    "id": "2p8t9jjUlx_O"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "if IN_COLAB:\n",
    "  !pip install feedparser tokenizers transformers scipy==1.7.1 sickle;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04f6ae9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04f6ae9e",
    "outputId": "8e8580ef-1f10-4688-fe27-43227296b63b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jrothschild/Projects/Abstract-generator\n"
     ]
    }
   ],
   "source": [
    "import os, torch, time, math, sys, re, csv\n",
    "import numpy as np\n",
    "\n",
    "PACKAGE_ROOT = os.path.dirname(os.path.abspath(''))\n",
    "print(PACKAGE_ROOT)\n",
    "sys.path.append(PACKAGE_ROOT)\n",
    "\n",
    "from src import settings\n",
    "import src.data.dataset_class as dsc\n",
    "import src.data.dataloader_class as dlc\n",
    "\n",
    "from src.model.generate_text import gen_some_text, decode_during_training\n",
    "\n",
    "from src.model.train_evaluate import train_version_jeremy as train\n",
    "from src.model.train_evaluate import evaluate_version_jeremy as evaluate\n",
    "\n",
    "#from src.model.transformer import make_gpt_model # imports don't work\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904013cd",
   "metadata": {
    "id": "904013cd"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "da73442b",
   "metadata": {
    "id": "da73442b"
   },
   "outputs": [],
   "source": [
    "# ARCHITECTURE\n",
    "# TODO : Make a class that sets all this, validates it's in use\n",
    "max_len_sentence     = 40 # maximum sentence length\n",
    "vocab_size  = None # None if you want to let tokenizer do its thing\n",
    "emsize     = 512 # embedding dimension\n",
    "nhid       = 2048 # the dimension of the feedforward network model in torch.nn.TransformerEncoder\n",
    "nlayers    = 12 # the number of torch.nn.TransformerEncoderLayer in torch.nn.TransformerEncoder\n",
    "nhead      = 8 # the number of heads in the multiheadattention models\n",
    "dropout    = 0.2 # the dropout value\n",
    "batch_size = 10 #32\n",
    "val_batch_size = 10 #32, not used right now.\n",
    "epochs     = 10  # The number of epochs\n",
    "\n",
    "#train tokenizer (or use one already trained)\n",
    "tknzr_type = 'BPE'\n",
    "flag_tknzr_train = True\n",
    "flag_tknzr_fast = True\n",
    "\n",
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21983a34",
   "metadata": {
    "id": "21983a34"
   },
   "source": [
    "### Format Dataset\n",
    "\n",
    "Uses a custom dataset class, which is an iterable and callable structure that returns a sample from our dataset. Within this custom dataset, can determine all preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "015ae225",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "015ae225",
    "outputId": "d96c4d7e-6a85-4e17-9b2b-5809129017da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Warning : overwriting previously save tokenizer with                        same filename ( /home/jrothschild/Projects/Abstract-generator/saved_tokenizers/BPE_arxiv_all_electron_10000.json ).\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "dataset = dsc.ArxivDataset()\n",
    "#dataset = dsc.WikiTextDataset()\n",
    "\n",
    "_ = dataset.tokenizer(flag_tknzr_train, tknzr_type, flag_tknzr_fast=flag_tknzr_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3955854",
   "metadata": {
    "id": "e3955854"
   },
   "source": [
    "### Selecting model\n",
    "\n",
    "Here we choose which model we shall use for training. For now, I've selected the black box Transformer from HuggingFace because the collate_fn I've written gives the correct input size force it... however this can easily be changed! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "WAlDNZ1FMfKz",
   "metadata": {
    "id": "WAlDNZ1FMfKz"
   },
   "outputs": [],
   "source": [
    "HF = False  # if False, uses AIAYN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bba4cb9",
   "metadata": {
    "id": "4bba4cb9"
   },
   "source": [
    "### Creating DataLoaders\n",
    "\n",
    "Training is done on batches, so we need a way to extract groupings of the data in the appropriate format for our transformer model.\n",
    "Note that for transformers which we are training, dataloaders outputs both src (x[:-1] and tgt ([1:]).\n",
    "The collation of batches for different transformer models we have vary. For HuggingFace it's ( max_len x batch_size ) whereas I think that the Annotated Transformer has ( batch_size x max_len ).\n",
    "\n",
    "I created a custom Dataloader class that wraps splitting the dataset and also outputs different dataloaders for each.\n",
    "\n",
    "NOTE : Do not use the tokenizer before the training if you use num_workers>0!\n",
    "FastTokenizer does not play nicely with forking if you use it before the forking of your data:\n",
    "https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c11048fc",
   "metadata": {
    "id": "c11048fc"
   },
   "outputs": [],
   "source": [
    "if HF:\n",
    "    dim = 0\n",
    "else:\n",
    "    dim = 1\n",
    "dataloader = dlc.CustomDataloader(dataset, batch_size, max_len_sentence, dim=dim)  # num_workers need to be changed to something less for Windows users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6de7d252",
   "metadata": {
    "id": "6de7d252"
   },
   "outputs": [],
   "source": [
    "if HF:\n",
    "    from src.model.transformer_torch import TransformerModel, make_std_mask\n",
    "\n",
    "    # transformer from huggingface\n",
    "    # TODO : Change to the Annotated Transformer if I want\n",
    "    model = TransformerModel(dataset.vocab_size, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "\n",
    "    # criterion\n",
    "    criterion = torch.nn.CrossEntropyLoss()#ignore_index=tknzr.get_vocab()[\"<pad>\"])\n",
    "\n",
    "    # optimizer TODO : why these parameters?\n",
    "    paramsAdam  = [{'params' : model.parameters(), 'lr' : 1e-3, 'betas' : (0.9, 0.999), 'eps' : 1e-08, 'weight_decay' : 0.0}]\n",
    "    paramsAdamW = [{'params' : model.parameters(), 'lr' : 5e-5, 'betas' : (0.9, 0.999), 'eps' : 1e-08, 'weight_decay' : 0.0}]\n",
    "    paramsSGD   = [{'params' : model.parameters(), 'lr' : 0.5, 'momentum' : 0.0, 'dampening' : 0.0, 'weight_decay' : 0.0}]\n",
    "\n",
    "    optimizer = torch.optim.SGD( paramsSGD )\n",
    "    #optimizer = torch.optim.Adam( paramsAdam )\n",
    "    #optimizer = torch.optim.AdamW( paramsAdamW )\n",
    "    \n",
    "    # scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95) # 1.0 to signify no decay rate\n",
    "\n",
    "else:\n",
    "    from src.model.transformer_aiayn import make_model, LabelSmoothing, NoamOpt, make_std_mask\n",
    "\n",
    "    model = make_model(dataset.vocab_size, N=nlayers, d_model=emsize, d_ff=nhid, h=nhead, dropout=dropout).to(device)\n",
    "\n",
    "    criterion = LabelSmoothing(size=dataset.vocab_size, padding_idx=0, smoothing=0.0)  # should this be SimpleLossCompute???\n",
    "\n",
    "    optimizer = NoamOpt(model.embed[0].d_model, 1, 4000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "    \n",
    "    scheduler = optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "xNEXSEoMb-9-",
   "metadata": {
    "id": "xNEXSEoMb-9-"
   },
   "outputs": [],
   "source": [
    "gen_text_each_epoch = True\n",
    "# default behaviour generate sentence 1 greedily, then nseed sentences with beta=1\n",
    "if gen_text_each_epoch:\n",
    "    text_prompt = 'The dog ran'\n",
    "    decode_seeds = [0, 1, 2]\n",
    "    decode_betas = [1.0, 1.0, 1.0]\n",
    "    nongreedy_style = 'sample_full'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780d86f",
   "metadata": {
    "id": "9780d86f"
   },
   "source": [
    "### Training\n",
    "\n",
    "Training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "75775f0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75775f0e",
    "outputId": "accd075c-485e-483d-dfcb-88220b6d1cd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/  700 batches | lr 0.00 | ms/batch 41.26 | loss  2.08 | ppl     7.99\n",
      "| epoch   1 |   400/  700 batches | lr 0.00 | ms/batch 41.33 | loss  2.06 | ppl     7.84\n",
      "| epoch   1 |   600/  700 batches | lr 0.00 | ms/batch 40.86 | loss  2.04 | ppl     7.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 30.01s | valid loss  2.04 | valid ppl     7.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 1: The dog ran ...\n",
      "Greedy decoding:\n",
      "\tThe dog ran  of  in  electron  -  beams  model  in  on  -  -  model  model  on  on  -  -  model  model  on  on  -  -  model  model  on\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\tThe dog ran  medium  charges  of  We  two  study  -  path  model  reported  within  times  a  in  two  Landau  -  -  umber  coupling  in  through  interacting  model  single\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\tThe dog ran  to  in  Wigner  -  e  coupled O  evaluation  bremsstrahlung  of  in  electron  GaAs  -  place  collisions  filling  on  upon  -  electron  spectral  occupied  concentric  ion\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\tThe dog ran  has  processes  scientific  It  space  in  matrix  dimensional  measurements  spatially  measurements  electron  in  current  dimensional  in  (  parallel  )  heating  properties  in  of  -  charged\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   1 | example generation time:  1.84s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/  700 batches | lr 0.00 | ms/batch 41.77 | loss  1.93 | ppl     6.88\n",
      "| epoch   2 |   400/  700 batches | lr 0.00 | ms/batch 40.77 | loss  1.91 | ppl     6.78\n",
      "| epoch   2 |   600/  700 batches | lr 0.00 | ms/batch 41.22 | loss  1.92 | ppl     6.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 30.00s | valid loss  1.97 | valid ppl     7.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 2: The dog ran ...\n",
      "Greedy decoding:\n",
      "\tThe dog ran  of  in  electrons  systems  in  in  systems  systems  in  in  systems  systems  in  in  systems  systems  in  in  systems  systems  in  in  systems  systems  in\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\tThe dog ran  of  charges  quasi  in  -  collisional  -  path  theory  matrix  for  times  a  between  two  assumed  dimensional  the  coupled  coupling  to  through  interacting  classical  single\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\tThe dog ran  for  in  Wigner  systems  e  in  is  evaluation  description  of  of  electron  in  coupling  interacting ,  electrons  we  upon  connection  electron  (  exchange  concentric  coupling\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\tThe dog ran  has  processes  tunable  in  space  polar  matrix  mechanisms  formalism  in  as  electron  inorganic  current  occurs  occurs  in  via  Spin  heating  transfer  in  of  particular  charged\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   2 | example generation time:  1.84s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/  700 batches | lr 0.00 | ms/batch 41.82 | loss  1.82 | ppl     6.19\n",
      "| epoch   3 |   400/  700 batches | lr 0.00 | ms/batch 39.95 | loss  1.83 | ppl     6.24\n",
      "| epoch   3 |   600/  700 batches | lr 0.00 | ms/batch 40.26 | loss  1.84 | ppl     6.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 29.63s | valid loss  1.93 | valid ppl     6.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 3: The dog ran ...\n",
      "Greedy decoding:\n",
      "\tThe dog ran  of  in  the  the  energy  energy  in  in  the  the  energy  energy  in  in  the  the  energy  energy  in  in  the  the  energy  energy  in\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\tThe dog ran  of  as  quasi  applied  two  magnetic  -  field  theory  inelastic  for  correlations  a  on  two  Landau  Falicov  3  -- .  colliding  A  is  model  used\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\tThe dog ran  for  in  Wigner  recent  e  observations .,  evaluation  anoscale  of  by  electron  in  -  the  collisions  filling  on  upon  -  current  spectral  conditions  concentric  the\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\tThe dog ran  the  processes  existence  carbon  of  polar  electrons  mechanisms  as  in  as  a  inorganic  current  occurs  occurs  theoretically  via  propose  heating  implemented  in  a  particular  charged\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   3 | example generation time:  1.83s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/  700 batches | lr 0.00 | ms/batch 42.23 | loss  1.75 | ppl     5.76\n",
      "| epoch   4 |   400/  700 batches | lr 0.00 | ms/batch 41.08 | loss  1.77 | ppl     5.87\n",
      "| epoch   4 |   600/  700 batches | lr 0.00 | ms/batch 41.04 | loss  1.78 | ppl     5.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 30.26s | valid loss  1.91 | valid ppl     6.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 4: The dog ran ...\n",
      "Greedy decoding:\n",
      "\tThe dog ran  electrons  in  in  the  the  problem  problem  for  for  the  the  problem  problem  for  for  the  the  problem  problem  for  for  the  the  problem  problem\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\tThe dog ran  electrons  charges  in  in  two  collisional  -  path  show  reported  that  times  a  superconductivity  two  in  -  the  coupled  conditions  atomic  for  dots  heavy  has\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\tThe dog ran  the  in  Wigner  recent  valence  dimensions  oxides  is  have  Monte  recently  Carlo  in  for  the  a  filling  short  upon  -  electron  kinetic  electron  concentric  ion\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\tThe dog ran  electrons  processes  are  in  formulated  polar  with  Metr  N  spatially  model  electron  in  current  electron  occurs  (  via  )  heating  on  in  a  -  charged\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   4 | example generation time:  1.90s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/  700 batches | lr 0.00 | ms/batch 42.22 | loss  1.69 | ppl     5.39\n",
      "| epoch   5 |   400/  700 batches | lr 0.00 | ms/batch 41.95 | loss  1.70 | ppl     5.50\n",
      "| epoch   5 |   600/  700 batches | lr 0.00 | ms/batch 41.14 | loss  1.73 | ppl     5.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 30.47s | valid loss  1.88 | valid ppl     6.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 5: The dog ran ...\n",
      "Greedy decoding:\n",
      "\tThe dog ran  -  of  electron  electrons  scattering  in  from  the  the  electron  electron  scattering  scattering  from  from  the  the  electron  electron  scattering  scattering  from  from  the  the\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\tThe dog ran  structures  charges  of  applied  two  approach  -  path  charge  bf  states  times  often  superconductivity  rough  in  (  the Sc  temperature  atomic  ordering  scales  are  in\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\tThe dog ran  spectra  in  on  recent  different  dimensions  tunneling  evaluation  in  of  the  electron  Hubbard  -  Holstein  Hartree  filling  -  upon  connection  electron  spectral  scattering  function  from\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\tThe dog ran  has  processes  tunable  in  high  polar  contrast  Metr  to  antisite  measure  electron  the  current  electron  occurs  (  the  )  heating  properties  from  of  average  charged\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   5 | example generation time:  1.82s\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 |   200/  700 batches | lr 0.00 | ms/batch 41.37 | loss  1.62 | ppl     5.05\n",
      "| epoch   6 |   400/  700 batches | lr 0.00 | ms/batch 40.30 | loss  1.64 | ppl     5.13\n",
      "| epoch   6 |   600/  700 batches | lr 0.00 | ms/batch 40.76 | loss  1.66 | ppl     5.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 29.89s | valid loss  1.87 | valid ppl     6.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 6: The dog ran ...\n",
      "Greedy decoding:\n",
      "\tThe dog ran  electron  of  scattering  electrons  of  in  electrons  -  in  short  -  -  short  short  -  -  short  short  -  -  short  short  -  -  short\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\tThe dog ran  medium  charges  is  in  proposed  collisional orhod  path  predicts  radiation  information  times  structures  superconductivity  for  in  reduced  the  temperature  coupling ,  between  superconductivity  heavy  in\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\tThe dog ran  perturbation  by  theory  the  e  tail  distributions  of  energy  electron  loss  pair  in  -  terahertz  multiconfiguration  technological  short  upon  -  electron  spectral  exchange  function  correlation\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\tThe dog ran  relies  processes  on  in  high  polar  contrast  vacuum  to  reconnection  001  acceleration  inorganic  current  occurs  occurs  in  when  situ  heating  measurements  from  of  average  charged\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   6 | example generation time:  1.83s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/  700 batches | lr 0.00 | ms/batch 41.68 | loss  1.55 | ppl     4.70\n",
      "| epoch   7 |   400/  700 batches | lr 0.00 | ms/batch 40.80 | loss  1.59 | ppl     4.90\n",
      "| epoch   7 |   600/  700 batches | lr 0.00 | ms/batch 41.15 | loss  1.61 | ppl     4.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 30.15s | valid loss  1.87 | valid ppl     6.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 7: The dog ran ...\n",
      "Greedy decoding:\n",
      "\tThe dog ran  of  of  electrons  electrons  in  in  -  -  electron  electron  scattering  scattering  of  of  electrons  electrons  in  in  -  -  electron  electron  scattering  scattering  of\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\tThe dog ran  medium  with  (  applied  GeV  collisional  virtual  path  during  but  contrast  the  to  leading  two  to  -  understand  interaction  the ,  representation  is  of  predicted\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\tThe dog ran s  in  Wigner  recent  valence  experimental  oxides  evaluation  have  of  predicted  electron  in  -  terahertz  multiconfiguration  (  short  upon  -  electron  spectral  exchange  concentric  ion\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\tThe dog ran  relies  processes  on  in  -  polar  matrix  Metr  formalism  antisite  to  anoparticles  the  are  electron  studied  (  the  )  electron  properties  and  of  average  charged\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   7 | example generation time:  1.83s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/  700 batches | lr 0.00 | ms/batch 42.30 | loss  1.50 | ppl     4.47\n",
      "| epoch   8 |   400/  700 batches | lr 0.00 | ms/batch 40.75 | loss  1.53 | ppl     4.63\n",
      "| epoch   8 |   600/  700 batches | lr 0.00 | ms/batch 40.83 | loss  1.56 | ppl     4.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 30.16s | valid loss  1.87 | valid ppl     6.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 8: The dog ran ...\n",
      "Greedy decoding:\n",
      "\tThe dog ran  of  to  electrons  the  in  electronic  the  properties  electronic  of  properties  electrons  of  in  electrons  the  in  electronic  the  properties  electronic  of  properties  electrons  of\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\tThe dog ran  of  to  quasi  the  two  collisional  -  path  charge  but  capacity  strongly  density  correlated  two  itinerant  -  conduction  interaction  electrons ,  through  and  classical  single\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\tThe dog ran  to  with  the  high  e  dimensions .  with  bremsstrahlung  Monte  by  Carlo  in  methods  plane ,  filling  and  value  connection  the  and  occupied  concentric  $\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\tThe dog ran  has  processes  tunable  in  current  two  carrying  dimensional  conductivity  spatially  and  of  inorganic  current  occurs  occurs  \"  when  electrons  in  in  quantum  a  electron  charged\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   8 | example generation time:  1.81s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/  700 batches | lr 0.00 | ms/batch 41.88 | loss  1.46 | ppl     4.30\n",
      "| epoch   9 |   400/  700 batches | lr 0.00 | ms/batch 40.94 | loss  1.49 | ppl     4.44\n",
      "| epoch   9 |   600/  700 batches | lr 0.00 | ms/batch 41.07 | loss  1.52 | ppl     4.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 30.27s | valid loss  1.88 | valid ppl     6.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 9: The dog ran ...\n",
      "Greedy decoding:\n",
      "\tThe dog ran  of  with  electrons  the  in  electron  -  -  optical  optical  properties  properties  of  of  electrons  electrons  in  in  -  -  optical  optical  properties  properties  of\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\tThe dog ran  medium  with  of  electrons  atoms  collisional  with  path  charge  bf  states  linearly  of  ultrafast  two  dynamics  -  of  53  electrons  focused  through  laser  classical  acceleration\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\tThe dog ran s  with  $  high  e  temperature  distributions  x  in  -  the  electron  in  -  plane  multiconfiguration  filling  short  value  -  the  timescale  occupied  concentric  $\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\tThe dog ran  relies  processes  on  in  -  polar  matrix  vacuum  formalism  reconnection  to  with  the  current  electron  occurs  (  when  )  in  transfer  quantum  of  entanglement  charged\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch   9 | example generation time:  1.84s\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/  700 batches | lr 0.00 | ms/batch 41.67 | loss  1.42 | ppl     4.13\n",
      "| epoch  10 |   400/  700 batches | lr 0.00 | ms/batch 41.46 | loss  1.45 | ppl     4.25\n",
      "| epoch  10 |   600/  700 batches | lr 0.00 | ms/batch 41.39 | loss  1.48 | ppl     4.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 30.26s | valid loss  1.89 | valid ppl     6.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated text at epoch 10: The dog ran ...\n",
      "Greedy decoding:\n",
      "\tThe dog ran  and  with  theoretical  strong  studies  -  on  optical  -  properties  optical  of  properties  electrons  of  in  electrons  the  in  electronic  the  properties  electronic  of  properties\n",
      "(sample_full, seed=0, beta=1.00):\n",
      "\tThe dog ran  structures  with  of  strong  such  coupling  as  path  defect  but  Ru  strongly  structures  correlated  two  itinerant  -  conduction  53  of  focused  electrons  laser  dominates  acceleration\n",
      "(sample_full, seed=1, beta=1.00):\n",
      "\tThe dog ran s  with  -  high  e  temperature  tunneling  x  in  -  the  electron  Hubbard  -  exchange  plane  filling  resistance  value  relaxation  the  (  occupied  concentric  $\n",
      "(sample_full, seed=2, beta=1.00):\n",
      "\tThe dog ran  relies  processes  on  in  -  polar  matrix  particularly  formalism  between  of  electron  electrons  current  occurs  occurs  in  when  one  driven  -  quantum  pairing  levels  in\n",
      "-----------------------------------------------------------------------------------------\n",
      "Epoch  10 | example generation time:  1.82s\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # fasttokenizer should not be used before forking. Something\n",
    "                                                # to figure out. What this does is suppress some warning messages \n",
    "                                                # https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning\n",
    "                                                # doesn't seem to affect the timing though\n",
    "if TRAIN:\n",
    "    nbr_batches = len(dataloader.train)\n",
    "    training_ppl_train_fine = np.zeros(epochs * nbr_batches)\n",
    "    training_ppl_val_coarse = np.zeros(epochs)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        loss_per_batch = train(model, dataloader.train, device, dataset.vocab_size, epoch, optimizer, scheduler, criterion, make_std_mask, max_len_sentence)\n",
    "        val_loss = evaluate(model, dataloader.valid, device, dataset.vocab_size, criterion, max_len_sentence, make_std_mask)\n",
    "        validation_ppl = math.exp(val_loss)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                         val_loss, validation_ppl))\n",
    "        print('-' * 89)\n",
    "        \n",
    "        training_ppl_val_coarse[epoch - 1] = validation_ppl\n",
    "        training_ppl_train_fine[(epoch - 1)*nbr_batches: (epoch)*nbr_batches] = np.exp(loss_per_batch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if gen_text_each_epoch:\n",
    "            decode_during_training(model, dataset.transform, device, epoch,\n",
    "                                   make_std_mask,\n",
    "                                   nongreedy_style,\n",
    "                                   max_len_sentence,\n",
    "                                   text_prompt,\n",
    "                                   decode_seeds,\n",
    "                                   decode_betas)\n",
    "\n",
    "    np.savetxt(settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}_training_ppl_train_fine.txt', training_ppl_train_fine)\n",
    "    np.savetxt(settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}_training_ppl_val_coarse.txt', training_ppl_val_coarse)\n",
    "\n",
    "    # save best model (two methods)\n",
    "    model_full = settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}.pth'\n",
    "    model_weights = settings.DIR_MODELS + os.sep + f'{dataset.name}_weights_epoch_{epochs}.pth'\n",
    "    model_full_best = settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}_best.pth'\n",
    "    model_weights_best = settings.DIR_MODELS + os.sep + f'{dataset.name}_weights_epoch_{epochs}_best.pth'\n",
    "    # approach 1: save model (class) entirely (uses pickle)\n",
    "    torch.save(model, model_full)\n",
    "    torch.save(best_model, model_full_best)\n",
    "    # approach 2: save model weights\n",
    "    torch.save(best_model.state_dict(), model_weights_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fhFjHInPiK_A",
   "metadata": {
    "id": "fhFjHInPiK_A"
   },
   "source": [
    "## Plotting Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NDDjgoQqjODa",
   "metadata": {
    "id": "NDDjgoQqjODa"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_axis = np.arange(1, epochs + 1)\n",
    "epochs_axis_fine = np.linspace(0, epochs, epochs * nbr_batches + 1)[:-1]\n",
    "\n",
    "def plot_training_timeseries(exp_name='', fname='training_performance',\n",
    "                             ext='.jpg',\n",
    "                             logy=True, \n",
    "                             xlims=None, \n",
    "                             ylims=None\n",
    "                             ):\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(epochs_axis, training_ppl_val_coarse, '--ok', label='val', zorder=2)\n",
    "    plt.plot(epochs_axis_fine, training_ppl_train_fine, 'b', label='train', zorder=1, alpha=0.5)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('ppl (validation set)')\n",
    "    plt.legend()\n",
    "    if logy:\n",
    "        plt.yscale('log')\n",
    "        fname += '_logy'\n",
    "    if xlims is not None:\n",
    "        plt.xlim(xlims[0], xlims[1])\n",
    "    if ylims is not None:\n",
    "        plt.ylim(ylims[0], ylims[1])\n",
    "    plt.savefig(settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}_{fname} + ext')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CPgCoA_uigGn",
   "metadata": {
    "id": "CPgCoA_uigGn"
   },
   "outputs": [],
   "source": [
    "plot_training_timeseries(exp_name=dataset.name, logy=True)\n",
    "plot_training_timeseries(exp_name=dataset.name, fname='training_performance_linear', logy=False, ylims=(0, np.max(training_ppl_val_coarse) * 1.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XM5G4vD0q9TZ",
   "metadata": {
    "id": "XM5G4vD0q9TZ"
   },
   "source": [
    "### Text Generation\n",
    "\n",
    "Here I've simply taken the code Matt uses to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6acee1",
   "metadata": {
    "id": "eb6acee1"
   },
   "outputs": [],
   "source": [
    "if not TRAIN:\n",
    "    custom_filename = 'arxiv_10000'\n",
    "    custom_epochs = 10\n",
    "    model_full = settings.DIR_MODELS + os.sep + f'{custom_filename}_epoch_{custom_epochs}_best.pth'\n",
    "    model_weights = settings.DIR_MODELS + os.sep + f'{custom_filename}_weights_epoch_{custom_epochs}_best.pth'\n",
    "    \n",
    "    # approach 1: load model (class) entirely (uses pickle)\n",
    "    model_full_load = torch.load(model_full, map_location=device)\n",
    "\n",
    "    # approach 2: load model weights, need to have some parameter or something \n",
    "    model_load = TransformerModel(vocab_size, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "    model_weights_load = model_load.load_state_dict( torch.load(model_weights) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SoGylu7hq7kI",
   "metadata": {
    "id": "SoGylu7hq7kI"
   },
   "outputs": [],
   "source": [
    "# inspect both models\n",
    "#print('model_A info...\\n', model_full_load)\n",
    "#print('\\nmodel_B info...\\n', model_weights_load)\n",
    "\n",
    "#print('model_A == model_B:', model_full_load == model_weights_load)\n",
    "#model = model_full_load\n",
    "# Text generation example\n",
    "\n",
    "#model = model_load\n",
    "prompt = 'The dog ran'\n",
    "ngen = 100\n",
    "decode_style = 'sample_topp' #greedy, sample_topp\n",
    "model.to('cpu')\n",
    "generated_text = gen_some_text(\n",
    "    best_model, dataset.transform, 'cpu', max_len_sentence, text_prompt=prompt, tokens_to_gen=ngen, vis=False,\n",
    "    decode_style=decode_style)\n",
    "print(\"Text prompt:\\n\", prompt)\n",
    "print(\"Number of tokens to generate:\", ngen)\n",
    "print(\"Generated_text:\\n\", generated_text)\n",
    "\n",
    "# TODO: alternative generation\n",
    "# currently 'greedy method'\n",
    "# see: https://huggingface.co/blog/how-to-generate"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "training_and_evaluation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
