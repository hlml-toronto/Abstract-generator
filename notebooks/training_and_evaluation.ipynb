{"cells":[{"cell_type":"code","execution_count":1,"id":"c18ec27a","metadata":{"id":"c18ec27a","executionInfo":{"status":"ok","timestamp":1640302909905,"user_tz":300,"elapsed":5,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}}},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":2,"id":"MZmtW9scbJ9Y","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23605,"status":"ok","timestamp":1640302933507,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"},"user_tz":300},"id":"MZmtW9scbJ9Y","outputId":"c166aa86-c3cc-4d1d-efed-2bf99f3bfbc1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Github/Abstract-generator/notebooks\n"]}],"source":["IN_COLAB = 'google.colab' in str(get_ipython())\n","\n","if IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","    %cd /content/drive/MyDrive/Github/Abstract-generator/notebooks"]},{"cell_type":"code","execution_count":3,"id":"2p8t9jjUlx_O","metadata":{"executionInfo":{"elapsed":17652,"status":"ok","timestamp":1640302951154,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"},"user_tz":300},"id":"2p8t9jjUlx_O"},"outputs":[],"source":["%%capture\n","if IN_COLAB:\n","  !pip install feedparser tokenizers transformers scipy==1.7.1;"]},{"cell_type":"code","execution_count":4,"id":"04f6ae9e","metadata":{"executionInfo":{"elapsed":15693,"status":"ok","timestamp":1640302966840,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"},"user_tz":300},"id":"04f6ae9e","colab":{"base_uri":"https://localhost:8080/"},"outputId":"623334f4-2d58-45ce-fff3-246bd8514863"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Github/Abstract-generator\n"]}],"source":["import os, torch, time, math, sys, re, csv\n","import numpy as np\n","\n","PACKAGE_ROOT = os.path.dirname(os.path.abspath(''))\n","print(PACKAGE_ROOT)\n","sys.path.append(PACKAGE_ROOT)\n","\n","from src import settings\n","import src.data.dataset_class as dsc\n","import src.data.dataloader_class as dlc\n","\n","from src.model.transformer_torch import TransformerModel\n","from src.model.generate_text import gen_some_text, decode_during_training\n","\n","from src.model.train_evaluate import train_version_jeremy as train\n","from src.model.train_evaluate import evaluate_version_jeremy as evaluate\n","\n","#from src.model.transformer import make_gpt_model # imports don't work\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","id":"904013cd","metadata":{"id":"904013cd"},"source":["### Parameters"]},{"cell_type":"code","execution_count":5,"id":"da73442b","metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1640302966843,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"},"user_tz":300},"id":"da73442b"},"outputs":[],"source":["# ARCHITECTURE\n","# TODO : Make a class that sets all this, validates it's in use\n","max_len_sentence     = 40 # maximum sentence length\n","vocab_size  = None # None if you want to let tokenizer do its thing\n","emsize     = 512 # embedding dimension\n","nhid       = 2048 # the dimension of the feedforward network model in torch.nn.TransformerEncoder\n","nlayers    = 12 # the number of torch.nn.TransformerEncoderLayer in torch.nn.TransformerEncoder\n","nhead      = 8 # the number of heads in the multiheadattention models\n","dropout    = 0.2 # the dropout value\n","batch_size = 10 #32\n","val_batch_size = 10 #32, not used right now.\n","epochs     = 10  # The number of epochs\n","\n","#train tokenizer (or use one already trained)\n","tknzr_type = 'BPE'\n","flag_tknzr_train = True\n","flag_tknzr_fast = True\n","\n","TRAIN = True"]},{"cell_type":"markdown","id":"21983a34","metadata":{"id":"21983a34"},"source":["### Format Dataset\n","\n","Uses a custom dataset class, which is an iterable and callable structure that returns a sample from our dataset. Within this custom dataset, can determine all preprocessing."]},{"cell_type":"code","execution_count":6,"id":"015ae225","metadata":{"executionInfo":{"elapsed":9626,"status":"ok","timestamp":1640302976448,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"},"user_tz":300},"id":"015ae225","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b0efe76f-1435-4a91-e931-5e8243f7136d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Warning : overwriting previously save tokenizer with                        same filename ( /content/drive/MyDrive/Github/Abstract-generator/saved_tokenizers/BPE_arxiv_all_electron_10000.json ).\n"]}],"source":["# create dataset\n","dataset = dsc.ArxivDataset()\n","#dataset = dsc.WikiTextDataset()\n","\n","_ = dataset.tokenizer(flag_tknzr_train, tknzr_type, flag_tknzr_fast=flag_tknzr_fast)"]},{"cell_type":"markdown","id":"4bba4cb9","metadata":{"id":"4bba4cb9"},"source":["### Creating DataLoaders\n","\n","Training is done on batches, so we need a way to extract groupings of the data in the appropriate format for our transformer model.\n","Note that for transformers which we are training, dataloaders outputs both src (x[:-1] and tgt ([1:]).\n","The collation of batches for different transformer models we have vary. For HuggingFace it's ( max_len x batch_size ) whereas I think that the Annotated Transformer has ( batch_size x max_len ).\n","\n","I created a custom Dataloader class that wraps splitting the dataset and also outputs different dataloaders for each.\n","\n","NOTE : Do not use the tokenizer before the training if you use num_workers>0!\n","FastTokenizer does not play nicely with forking if you use it before the forking of your data:\n","https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning"]},{"cell_type":"code","execution_count":7,"id":"c11048fc","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1640302976448,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"},"user_tz":300},"id":"c11048fc"},"outputs":[],"source":["flag_padding_mask = True\n","\n","dataloader = dlc.CustomDataloader(dataset, batch_size, max_len_sentence, flag_padding_mask=flag_padding_mask) "]},{"cell_type":"markdown","id":"e3955854","metadata":{"id":"e3955854"},"source":["### Selecting model\n","\n","Here we choose which model we shall use for training. For now, I've selected the black box Transformer from HuggingFace because the collate_fn I've written gives the correct input size force it... however this can easily be changed! "]},{"cell_type":"code","execution_count":8,"id":"6de7d252","metadata":{"executionInfo":{"elapsed":10192,"status":"ok","timestamp":1640302986636,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"},"user_tz":300},"id":"6de7d252"},"outputs":[],"source":["# transformer from huggingface\n","# TODO : Change to the Annotated Transformer if I want\n","model = TransformerModel(dataset.vocab_size, emsize, nhead, nhid, nlayers, dropout).to(device)\n","\n","# criterion\n","criterion = torch.nn.CrossEntropyLoss()#ignore_index=tknzr.get_vocab()[\"<pad>\"])\n","\n","# optimizer TODO : why these parameters?\n","paramsAdam  = [{'params' : model.parameters(), 'lr' : 1e-3, 'betas' : (0.9, 0.999), 'eps' : 1e-08, 'weight_decay' : 0.0}]\n","paramsAdamW = [{'params' : model.parameters(), 'lr' : 5e-5, 'betas' : (0.9, 0.999), 'eps' : 1e-08, 'weight_decay' : 0.0}]\n","paramsSGD   = [{'params' : model.parameters(), 'lr' : 0.5, 'momentum' : 0.0, 'dampening' : 0.0, 'weight_decay' : 0.0}]\n","\n","optimizer = torch.optim.SGD( paramsSGD )\n","#optimizer = torch.optim.Adam( paramsAdam )\n","#optimizer = torch.optim.AdamW( paramsAdamW )\n","\n","# scheduler\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95) # 1.0 to signify no decay rate"]},{"cell_type":"code","source":["gen_text_each_epoch = True\n","# default behaviour generate sentence 1 greedily, then nseed sentences with beta=1\n","if gen_text_each_epoch:\n","    text_prompt = 'The dog ran'\n","    decode_seeds = [0, 1, 2]\n","    decode_betas = [1.0, 1.0, 1.0]\n","    nongreedy_style = 'sample_full'"],"metadata":{"id":"xNEXSEoMb-9-","executionInfo":{"status":"ok","timestamp":1640302986641,"user_tz":300,"elapsed":33,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}}},"id":"xNEXSEoMb-9-","execution_count":9,"outputs":[]},{"cell_type":"markdown","id":"9780d86f","metadata":{"id":"9780d86f"},"source":["### Training\n","\n","Training loop!"]},{"cell_type":"code","execution_count":11,"id":"75775f0e","metadata":{"executionInfo":{"elapsed":116260,"status":"error","timestamp":1640303115964,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"},"user_tz":300},"id":"75775f0e","colab":{"base_uri":"https://localhost:8080/","height":550},"outputId":"12b093eb-ad4a-4391-af46-82eb585c1ef5"},"outputs":[{"output_type":"stream","name":"stdout","text":["| epoch   1 |   200/  700 batches | lr 0.50 | ms/batch 153.78 | loss  7.47 | ppl  1752.65\n","| epoch   1 |   400/  700 batches | lr 0.50 | ms/batch 159.52 | loss  7.03 | ppl  1132.50\n","| epoch   1 |   600/  700 batches | lr 0.50 | ms/batch 149.07 | loss  6.86 | ppl   955.96\n","-----------------------------------------------------------------------------------------\n","| end of epoch   1 | time: 112.14s | valid loss  6.66 | valid ppl   782.97\n","-----------------------------------------------------------------------------------------\n","Generated text at epoch 1: The dog ran ...\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-df97653711b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                                    \u001b[0mtext_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                                    \u001b[0mdecode_seeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                                    decode_betas)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDIR_MODELS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf'{dataset.name}_epoch_{epochs}_training_ppl_train_fine.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_ppl_train_fine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Github/Abstract-generator/src/model/generate_text.py\u001b[0m in \u001b[0;36mdecode_during_training\u001b[0;34m(generator_model, tokenizer, device, epoch, nongreedy_style, max_len_context, text_prompt, decode_seeds, decode_betas)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Generated text at epoch %d: %s ...'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;31m# First get greedy decoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0mgreedy_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_some_text_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'greedy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Greedy decoding:\\n\\t%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgreedy_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;31m# Now get several sampler decodings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Github/Abstract-generator/src/model/generate_text.py\u001b[0m in \u001b[0;36mgen_some_text_wrapper\u001b[0;34m(generator_model, tokenizer, device, text_prompt, decode_style, max_len_context, decode_seed, decode_beta)\u001b[0m\n\u001b[1;32m    213\u001b[0m                                    \u001b[0mdecode_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_seed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                                    \u001b[0mdecode_beta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_beta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                    vis=False)\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgenerated_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Github/Abstract-generator/src/model/generate_text.py\u001b[0m in \u001b[0;36mgen_some_text\u001b[0;34m(model, tokenizer, device, max_len_context, text_prompt, tokens_to_gen, vis, decode_style, decode_seed, decode_beta, decode_sample_topp_threshold)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# TODO : add src_key_padding_mask to the forward call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;31m# print(out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Github/Abstract-generator/src/model/transformer_torch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    127\u001b[0m                                 \u001b[0mloss\u001b[0m \u001b[0mcalculation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mninp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         output = self.transformer_encoder(src, mask=src_mask,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"]}],"source":["os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # fasttokenizer should not be used before forking. Something\n","                                                # to figure out. What this does is suppress some warning messages \n","                                                # https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning\n","                                                # doesn't seem to affect the timing though\n","\n","if TRAIN:\n","    nbr_batches = len(dataloader.train)\n","    training_ppl_train_fine = np.zeros(epochs * nbr_batches)\n","    training_ppl_val_coarse = np.zeros(epochs)\n","    best_val_loss = float(\"inf\")\n","    best_model = None\n","    for epoch in range(1, epochs + 1):\n","        epoch_start_time = time.time()\n","        loss_per_batch = train(model, dataloader.train, device, dataset.vocab_size, epoch, optimizer, scheduler, criterion, max_len_sentence)\n","        val_loss = evaluate(model, dataloader.valid, device, dataset.vocab_size, criterion, max_len_sentence)\n","        validation_ppl = math.exp(val_loss)\n","        print('-' * 89)\n","        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                         val_loss, validation_ppl))\n","        print('-' * 89)\n","        \n","        training_ppl_val_coarse[epoch - 1] = validation_ppl\n","        training_ppl_train_fine[(epoch - 1)*nbr_batches: (epoch)*nbr_batches] = np.exp(loss_per_batch)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_model = model\n","\n","        scheduler.step()\n","\n","        if gen_text_each_epoch:\n","            decode_during_training(model, dataset.transform, device, epoch,\n","                                   nongreedy_style,\n","                                   max_len_sentence,\n","                                   text_prompt,\n","                                   decode_seeds,\n","                                   decode_betas)\n","\n","    np.savetxt(settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}_training_ppl_train_fine.txt', training_ppl_train_fine)\n","    np.savetxt(settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}_training_ppl_val_coarse.txt', training_ppl_val_coarse)\n","\n","    # save best model (two methods)\n","    model_full = settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}.pth'\n","    model_weights = settings.DIR_MODELS + os.sep + f'{dataset.name}_weights_epoch_{epochs}.pth'\n","    model_full_best = settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}_best.pth'\n","    model_weights_best = settings.DIR_MODELS + os.sep + f'{dataset.name}_weights_epoch_{epochs}_best.pth'\n","    # approach 1: save model (class) entirely (uses pickle)\n","    torch.save(model, model_full)\n","    torch.save(best_model, model_full_best)\n","    # approach 2: save model weights\n","    torch.save(best_model.state_dict(), model_weights_best)"]},{"cell_type":"markdown","source":["## Plotting Loss"],"metadata":{"id":"fhFjHInPiK_A"},"id":"fhFjHInPiK_A"},{"cell_type":"code","execution_count":null,"id":"NDDjgoQqjODa","metadata":{"id":"NDDjgoQqjODa","executionInfo":{"status":"aborted","timestamp":1640303115956,"user_tz":300,"elapsed":33,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","epochs_axis = np.arange(1, epochs + 1)\n","epochs_axis_fine = np.linspace(0, epochs, epochs * nbr_batches + 1)[:-1]\n","\n","def plot_training_timeseries(exp_name='', fname='training_performance',\n","                             ext='.jpg',\n","                             logy=True, \n","                             xlims=None, \n","                             ylims=None\n","                             ):\n","\n","    plt.figure(figsize=(8,6))\n","    plt.plot(epochs_axis, training_ppl_val_coarse, '--ok', label='val', zorder=2)\n","    plt.plot(epochs_axis_fine, training_ppl_train_fine, 'b', label='train', zorder=1, alpha=0.5)\n","    plt.xlabel('epoch')\n","    plt.ylabel('ppl (validation set)')\n","    plt.legend()\n","    if logy:\n","        plt.yscale('log')\n","        fname += '_logy'\n","    if xlims is not None:\n","        plt.xlim(xlims[0], xlims[1])\n","    if ylims is not None:\n","        plt.ylim(ylims[0], ylims[1])\n","    plt.savefig(settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}_{fname} + ext')\n","    plt.show()"]},{"cell_type":"code","source":["plot_training_timeseries(exp_name=dataset.name, logy=True)\n","plot_training_timeseries(exp_name=dataset.name, fname='training_performance_linear', logy=False, ylims=(0, np.max(training_ppl_val_coarse) * 1.2))"],"metadata":{"id":"CPgCoA_uigGn","executionInfo":{"status":"aborted","timestamp":1640303115960,"user_tz":300,"elapsed":34,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}}},"id":"CPgCoA_uigGn","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"XM5G4vD0q9TZ","metadata":{"id":"XM5G4vD0q9TZ"},"source":["### Text Generation\n","\n","Here I've simply taken the code Matt uses to generate text."]},{"cell_type":"code","execution_count":null,"id":"eb6acee1","metadata":{"id":"eb6acee1","executionInfo":{"status":"aborted","timestamp":1640303115961,"user_tz":300,"elapsed":33,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"}}},"outputs":[],"source":["if not TRAIN:\n","    custom_filename = 'arxiv_10000'\n","    custom_epochs = 10\n","    model_full = settings.DIR_MODELS + os.sep + f'{custom_filename}_epoch_{custom_epochs}_best.pth'\n","    model_weights = settings.DIR_MODELS + os.sep + f'{custom_filename}_weights_epoch_{custom_epochs}_best.pth'\n","    \n","    # approach 1: load model (class) entirely (uses pickle)\n","    model_full_load = torch.load(model_full, map_location=device)\n","\n","    # approach 2: load model weights, need to have some parameter or something \n","    model_load = TransformerModel(vocab_size, emsize, nhead, nhid, nlayers, dropout).to(device)\n","    model_weights_load = model_load.load_state_dict( torch.load(model_weights) )"]},{"cell_type":"code","execution_count":null,"id":"SoGylu7hq7kI","metadata":{"executionInfo":{"elapsed":31,"status":"aborted","timestamp":1640303115962,"user":{"displayName":"Jeremy Rothschild","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj61S-ps1fSn3WHOaumL82qcXAixc33zZBJ_THQl4E=s64","userId":"12754514505135179603"},"user_tz":300},"id":"SoGylu7hq7kI"},"outputs":[],"source":["# inspect both models\n","#print('model_A info...\\n', model_full_load)\n","#print('\\nmodel_B info...\\n', model_weights_load)\n","\n","#print('model_A == model_B:', model_full_load == model_weights_load)\n","#model = model_full_load\n","# Text generation example\n","\n","#model = model_load\n","prompt = 'The dog ran'\n","ngen = 100\n","decode_style = 'sample_topp' #greedy, sample_topp\n","model.to('cpu')\n","generated_text = gen_some_text(\n","    best_model, dataset.transform, 'cpu', max_len_sentence, text_prompt=prompt, tokens_to_gen=ngen, vis=False,\n","    decode_style=decode_style)\n","print(\"Text prompt:\\n\", prompt)\n","print(\"Number of tokens to generate:\", ngen)\n","print(\"Generated_text:\\n\", generated_text)\n","\n","# TODO: alternative generation\n","# currently 'greedy method'\n","# see: https://huggingface.co/blog/how-to-generate"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"training_and_evaluation.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":5}