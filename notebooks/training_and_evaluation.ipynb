{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c18ec27a",
      "metadata": {
        "id": "c18ec27a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85939aff-ea03-4e80-c602-e8792d82f533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "MZmtW9scbJ9Y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZmtW9scbJ9Y",
        "outputId": "55e81d33-9ec1-4b5c-c898-d2811ec99fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Documents/HLML/Abstract-generator/notebooks\n"
          ]
        }
      ],
      "source": [
        "IN_COLAB = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    %cd /content/drive/MyDrive/Documents/HLML/Abstract-generator/notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2p8t9jjUlx_O",
      "metadata": {
        "id": "2p8t9jjUlx_O"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if IN_COLAB:\n",
        "  !pip install feedparser tokenizers transformers scipy==1.7.1 sickle;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "04f6ae9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04f6ae9e",
        "outputId": "b7cb5b54-e7ca-4773-a9e4-06a489605ebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Documents/HLML/Abstract-generator\n"
          ]
        }
      ],
      "source": [
        "import os, torch, time, math, sys, re, csv\n",
        "import numpy as np\n",
        "\n",
        "PACKAGE_ROOT = os.path.dirname(os.path.abspath(''))\n",
        "print(PACKAGE_ROOT)\n",
        "sys.path.append(PACKAGE_ROOT)\n",
        "\n",
        "from src import settings\n",
        "import src.data.dataset_class as dsc\n",
        "import src.data.dataloader_class as dlc\n",
        "\n",
        "from src.model.generate_text import gen_some_text, decode_during_training\n",
        "\n",
        "from src.model.train_evaluate import train_version_jeremy as train\n",
        "from src.model.train_evaluate import evaluate_version_jeremy as evaluate\n",
        "\n",
        "#from src.model.transformer import make_gpt_model # imports don't work\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "904013cd",
      "metadata": {
        "id": "904013cd"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "da73442b",
      "metadata": {
        "id": "da73442b"
      },
      "outputs": [],
      "source": [
        "# ARCHITECTURE\n",
        "# TODO : Make a class that sets all this, validates it's in use\n",
        "max_len_sentence     = 40 # maximum sentence length\n",
        "vocab_size  = None # None if you want to let tokenizer do its thing\n",
        "emsize     = 512 # embedding dimension\n",
        "nhid       = 2048 # the dimension of the feedforward network model in torch.nn.TransformerEncoder\n",
        "nlayers    = 12 # the number of torch.nn.TransformerEncoderLayer in torch.nn.TransformerEncoder\n",
        "nhead      = 8 # the number of heads in the multiheadattention models\n",
        "dropout    = 0.2 # the dropout value\n",
        "batch_size = 10 #32\n",
        "val_batch_size = 10 #32, not used right now.\n",
        "epochs     = 10  # The number of epochs\n",
        "\n",
        "#train tokenizer (or use one already trained)\n",
        "tknzr_type = 'BPE'\n",
        "flag_tknzr_train = True\n",
        "flag_tknzr_fast = True\n",
        "\n",
        "TRAIN = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21983a34",
      "metadata": {
        "id": "21983a34"
      },
      "source": [
        "### Format Dataset\n",
        "\n",
        "Uses a custom dataset class, which is an iterable and callable structure that returns a sample from our dataset. Within this custom dataset, can determine all preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "015ae225",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "015ae225",
        "outputId": "bbc963fc-265a-40c1-a4de-f95465bb8029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning : overwriting previously save tokenizer with                        same filename ( /content/drive/MyDrive/Documents/HLML/Abstract-generator/saved_tokenizers/BPE_arxiv_all_electron_10000.json ).\n"
          ]
        }
      ],
      "source": [
        "# create dataset\n",
        "dataset = dsc.ArxivDataset()\n",
        "#dataset = dsc.WikiTextDataset()\n",
        "\n",
        "_ = dataset.tokenizer(flag_tknzr_train, tknzr_type, flag_tknzr_fast=flag_tknzr_fast)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3955854",
      "metadata": {
        "id": "e3955854"
      },
      "source": [
        "### Selecting model\n",
        "\n",
        "Here we choose which model we shall use for training. For now, I've selected the black box Transformer from HuggingFace because the collate_fn I've written gives the correct input size force it... however this can easily be changed! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "WAlDNZ1FMfKz",
      "metadata": {
        "id": "WAlDNZ1FMfKz"
      },
      "outputs": [],
      "source": [
        "HF = False  # if False, uses AIAYN "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bba4cb9",
      "metadata": {
        "id": "4bba4cb9"
      },
      "source": [
        "### Creating DataLoaders\n",
        "\n",
        "Training is done on batches, so we need a way to extract groupings of the data in the appropriate format for our transformer model.\n",
        "Note that for transformers which we are training, dataloaders outputs both src (x[:-1] and tgt ([1:]).\n",
        "The collation of batches for different transformer models we have vary. For HuggingFace it's ( max_len x batch_size ) whereas I think that the Annotated Transformer has ( batch_size x max_len ).\n",
        "\n",
        "I created a custom Dataloader class that wraps splitting the dataset and also outputs different dataloaders for each.\n",
        "\n",
        "NOTE : Do not use the tokenizer before the training if you use num_workers>0!\n",
        "FastTokenizer does not play nicely with forking if you use it before the forking of your data:\n",
        "https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c11048fc",
      "metadata": {
        "id": "c11048fc"
      },
      "outputs": [],
      "source": [
        "if HF:\n",
        "    dim = 0\n",
        "else:\n",
        "    dim = 1\n",
        "dataloader = dlc.CustomDataloader(dataset, batch_size, max_len_sentence, dim=dim)  # num_workers need to be changed to something less for Windows users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6de7d252",
      "metadata": {
        "id": "6de7d252"
      },
      "outputs": [],
      "source": [
        "if HF:\n",
        "    from src.model.transformer_torch import TransformerModel, make_std_mask\n",
        "\n",
        "    # transformer from huggingface\n",
        "    # TODO : Change to the Annotated Transformer if I want\n",
        "    model = TransformerModel(dataset.vocab_size, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
        "\n",
        "    # criterion\n",
        "    criterion = torch.nn.CrossEntropyLoss()#ignore_index=tknzr.get_vocab()[\"<pad>\"])\n",
        "\n",
        "    # optimizer TODO : why these parameters?\n",
        "    paramsAdam  = [{'params' : model.parameters(), 'lr' : 1e-3, 'betas' : (0.9, 0.999), 'eps' : 1e-08, 'weight_decay' : 0.0}]\n",
        "    paramsAdamW = [{'params' : model.parameters(), 'lr' : 5e-5, 'betas' : (0.9, 0.999), 'eps' : 1e-08, 'weight_decay' : 0.0}]\n",
        "    paramsSGD   = [{'params' : model.parameters(), 'lr' : 0.5, 'momentum' : 0.0, 'dampening' : 0.0, 'weight_decay' : 0.0}]\n",
        "\n",
        "    optimizer = torch.optim.SGD( paramsSGD )\n",
        "    #optimizer = torch.optim.Adam( paramsAdam )\n",
        "    #optimizer = torch.optim.AdamW( paramsAdamW )\n",
        "    \n",
        "    # scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95) # 1.0 to signify no decay rate\n",
        "\n",
        "else:\n",
        "    from src.model.transformer_aiayn import make_model, LabelSmoothing, NoamOpt, make_std_mask\n",
        "\n",
        "    model = make_model(dataset.vocab_size, N=nlayers, d_model=emsize, d_ff=nhid, h=nhead, dropout=dropout).to(device)\n",
        "\n",
        "    criterion = LabelSmoothing(size=dataset.vocab_size, padding_idx=0, smoothing=0.0)  # should this be SimpleLossCompute???\n",
        "\n",
        "    optimizer = NoamOpt(model.embed[0].d_model, 1, 4000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "    \n",
        "    scheduler = optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "xNEXSEoMb-9-",
      "metadata": {
        "id": "xNEXSEoMb-9-"
      },
      "outputs": [],
      "source": [
        "gen_text_each_epoch = True\n",
        "# default behaviour generate sentence 1 greedily, then nseed sentences with beta=1\n",
        "if gen_text_each_epoch:\n",
        "    text_prompt = 'The dog ran'\n",
        "    decode_seeds = [0, 1, 2]\n",
        "    decode_betas = [1.0, 1.0, 1.0]\n",
        "    nongreedy_style = 'sample_full'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9780d86f",
      "metadata": {
        "id": "9780d86f"
      },
      "source": [
        "### Training\n",
        "\n",
        "Training loop!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75775f0e",
      "metadata": {
        "id": "75775f0e"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # fasttokenizer should not be used before forking. Something\n",
        "                                                # to figure out. What this does is suppress some warning messages \n",
        "                                                # https://stackoverflow.com/questions/62691279/how-to-disable-tokenizers-parallelism-true-false-warning\n",
        "                                                # doesn't seem to affect the timing though\n",
        "if TRAIN:\n",
        "    nbr_batches = len(dataloader.train)\n",
        "    training_ppl_train_fine = np.zeros(epochs * nbr_batches)\n",
        "    training_ppl_val_coarse = np.zeros(epochs)\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_model = None\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        loss_per_batch = train(model, dataloader.train, device, dataset.vocab_size, epoch, optimizer, scheduler, criterion, make_std_mask, 0, max_len_sentence)\n",
        "        val_loss = evaluate(model, dataloader.valid, device, dataset.vocab_size, criterion, max_len_sentence, make_std_mask)\n",
        "        validation_ppl = math.exp(val_loss)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                         val_loss, validation_ppl))\n",
        "        print('-' * 89)\n",
        "        \n",
        "        training_ppl_val_coarse[epoch - 1] = validation_ppl\n",
        "        training_ppl_train_fine[(epoch - 1)*nbr_batches: (epoch)*nbr_batches] = np.exp(loss_per_batch)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = model\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if gen_text_each_epoch:\n",
        "            decode_during_training(model, dataset.transform, device, epoch,\n",
        "                                   make_std_mask,\n",
        "                                   nongreedy_style,\n",
        "                                   max_len_sentence,\n",
        "                                   text_prompt,\n",
        "                                   decode_seeds,\n",
        "                                   decode_betas)\n",
        "\n",
        "    np.savetxt(settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}_training_ppl_train_fine.txt', training_ppl_train_fine)\n",
        "    np.savetxt(settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}_training_ppl_val_coarse.txt', training_ppl_val_coarse)\n",
        "\n",
        "    # save best model (two methods)\n",
        "    model_full = settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}.pth'\n",
        "    model_weights = settings.DIR_MODELS + os.sep + f'{dataset.name}_weights_epoch_{epochs}.pth'\n",
        "    model_full_best = settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}_best.pth'\n",
        "    model_weights_best = settings.DIR_MODELS + os.sep + f'{dataset.name}_weights_epoch_{epochs}_best.pth'\n",
        "    # approach 1: save model (class) entirely (uses pickle)\n",
        "    torch.save(model, model_full)\n",
        "    torch.save(best_model, model_full_best)\n",
        "    # approach 2: save model weights\n",
        "    torch.save(best_model.state_dict(), model_weights_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fhFjHInPiK_A",
      "metadata": {
        "id": "fhFjHInPiK_A"
      },
      "source": [
        "## Plotting Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NDDjgoQqjODa",
      "metadata": {
        "id": "NDDjgoQqjODa"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs_axis = np.arange(1, epochs + 1)\n",
        "epochs_axis_fine = np.linspace(0, epochs, epochs * nbr_batches + 1)[:-1]\n",
        "\n",
        "def plot_training_timeseries(exp_name='', fname='training_performance',\n",
        "                             ext='.jpg',\n",
        "                             logy=True, \n",
        "                             xlims=None, \n",
        "                             ylims=None\n",
        "                             ):\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot(epochs_axis, training_ppl_val_coarse, '--ok', label='val', zorder=2)\n",
        "    plt.plot(epochs_axis_fine, training_ppl_train_fine, 'b', label='train', zorder=1, alpha=0.5)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('ppl (validation set)')\n",
        "    plt.legend()\n",
        "    if logy:\n",
        "        plt.yscale('log')\n",
        "        fname += '_logy'\n",
        "    if xlims is not None:\n",
        "        plt.xlim(xlims[0], xlims[1])\n",
        "    if ylims is not None:\n",
        "        plt.ylim(ylims[0], ylims[1])\n",
        "    plt.savefig(settings.DIR_MODELS + os.sep + f'{dataset.name}_epoch_{epochs}_{fname} + ext')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CPgCoA_uigGn",
      "metadata": {
        "id": "CPgCoA_uigGn"
      },
      "outputs": [],
      "source": [
        "plot_training_timeseries(exp_name=dataset.name, logy=True)\n",
        "plot_training_timeseries(exp_name=dataset.name, fname='training_performance_linear', logy=False, ylims=(0, np.max(training_ppl_val_coarse) * 1.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XM5G4vD0q9TZ",
      "metadata": {
        "id": "XM5G4vD0q9TZ"
      },
      "source": [
        "### Text Generation\n",
        "\n",
        "Here I've simply taken the code Matt uses to generate text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb6acee1",
      "metadata": {
        "id": "eb6acee1"
      },
      "outputs": [],
      "source": [
        "if not TRAIN:\n",
        "    custom_filename = 'arxiv_10000'\n",
        "    custom_epochs = 10\n",
        "    model_full = settings.DIR_MODELS + os.sep + f'{custom_filename}_epoch_{custom_epochs}_best.pth'\n",
        "    model_weights = settings.DIR_MODELS + os.sep + f'{custom_filename}_weights_epoch_{custom_epochs}_best.pth'\n",
        "    \n",
        "    # approach 1: load model (class) entirely (uses pickle)\n",
        "    model_full_load = torch.load(model_full, map_location=device)\n",
        "\n",
        "    # approach 2: load model weights, need to have some parameter or something \n",
        "    model_load = TransformerModel(vocab_size, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
        "    model_weights_load = model_load.load_state_dict( torch.load(model_weights) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SoGylu7hq7kI",
      "metadata": {
        "id": "SoGylu7hq7kI"
      },
      "outputs": [],
      "source": [
        "# inspect both models\n",
        "#print('model_A info...\\n', model_full_load)\n",
        "#print('\\nmodel_B info...\\n', model_weights_load)\n",
        "\n",
        "#print('model_A == model_B:', model_full_load == model_weights_load)\n",
        "#model = model_full_load\n",
        "# Text generation example\n",
        "\n",
        "#model = model_load\n",
        "prompt = 'The dog ran'\n",
        "ngen = 100\n",
        "decode_style = 'sample_topp' #greedy, sample_topp\n",
        "model.to('cpu')\n",
        "generated_text = gen_some_text(\n",
        "    best_model, dataset.transform, 'cpu', max_len_sentence, text_prompt=prompt, tokens_to_gen=ngen, vis=False,\n",
        "    decode_style=decode_style)\n",
        "print(\"Text prompt:\\n\", prompt)\n",
        "print(\"Number of tokens to generate:\", ngen)\n",
        "print(\"Generated_text:\\n\", generated_text)\n",
        "\n",
        "# TODO: alternative generation\n",
        "# currently 'greedy method'\n",
        "# see: https://huggingface.co/blog/how-to-generate"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "training_and_evaluation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}